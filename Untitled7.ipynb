{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc93c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "# from google.colab import drive\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "\n",
    "    df.drop([\n",
    "        'compi',\n",
    "        'compi_num', \n",
    "        \"speed\", \n",
    "        'rank',\n",
    "#         'result',\n",
    "#         'course',\n",
    "        'born',\n",
    "        '1走前走破タイム', '2走前走破タイム', '3走前走破タイム',\n",
    "        '4走前走破タイム', '5走前走破タイム',\n",
    "        '1走前補正タイム', '2走前補正タイム', '3走前補正タイム',\n",
    "        '4走前補正タイム', '5走前補正タイム',\n",
    "        '1走前結果', '2走前結果', '3走前結果',\n",
    "        '4走前結果', '5走前結果',\n",
    "        '1走前オッズ', '2走前オッズ', '3走前オッズ',\n",
    "        '4走前オッズ', '5走前オッズ',\n",
    "        '1走前コンピ指数', '2走前コンピ指数', '3走前コンピ指数',\n",
    "        '4走前コンピ指数', '5走前コンピ指数',\n",
    "        'horse_race_id',  'body_weight','body_weight_in_de',\n",
    "            '騎手全体勝率','騎手全体連対率','騎手全体複勝率','騎手競馬場別騎乗回数','騎手競馬場別勝率','騎手競馬場別連対率','騎手競馬場別複勝率',\n",
    "    '騎手コース別騎乗回数','騎手コース別勝率','騎手コース別連対率','騎手コース別複勝率','騎手距離別騎乗回数','騎手距離別勝率','騎手距離別連対率',\n",
    "    '騎手距離別複勝率','騎手同コース同距離別騎乗回数','騎手同コース同距離別勝率','騎手同コース同距離別連対率','騎手同コース同距離別複勝率',\n",
    "    '調教師全体勝率','調教師全体連対率','調教師全体複勝率','調教師競馬場別騎乗回数','調教師競馬場別勝率','調教師競馬場別連対率',\n",
    "    '調教師競馬場別複勝率','調教師コース別騎乗回数','調教師コース別勝率','調教師コース別連対率','調教師コース別複勝率','調教師距離別騎乗回数',\n",
    "    '調教師距離別勝率','調教師距離別連対率','調教師距離別複勝率','調教師同コース同距離別騎乗回数','調教師同コース同距離別勝率',\n",
    "    '調教師年齢別年間勝率', '調教師年齢別年間連対率', '調教師年齢別年間複勝率','調教師年齢別勝率', '調教師年齢別連対率', '調教師年齢別複勝率',\n",
    "    '調教師同コース同距離別連対率','調教師同コース同距離別複勝率','種牡馬全体勝率','種牡馬全体連対率','種牡馬全体複勝率',\n",
    "    '種牡馬競馬場別出走頭数','種牡馬競馬場別勝率','種牡馬競馬場別連対率','種牡馬競馬場別複勝率','種牡馬コース別出走頭数','種牡馬コース別勝率',\n",
    "    '種牡馬コース別連対率','種牡馬コース別複勝率','種牡馬距離別出走頭数','種牡馬距離別勝率','種牡馬距離別連対率','種牡馬距離別複勝率',\n",
    "    '種牡馬同コース同距離別出走頭数','種牡馬同コース同距離別勝率','種牡馬同コース同距離別連対率','種牡馬同コース同距離別複勝率',\n",
    "    '種牡馬同周り勝率', '種牡馬同周り連対率', '種牡馬同周り複勝率',  '種牡馬同枠勝率', '種牡馬同枠連対率', '種牡馬同枠複勝率',\n",
    "    '父系統出走頭数','父系統全体勝率','父系統全体連対率','父系統全体複勝率','父系統競馬場別出走頭数','父系統競馬場別勝率','父系統競馬場別連対率','父系統競馬場別複勝率','父系統コース別出走頭数','父系統コース別勝率',\n",
    "    '父系統コース別連対率','父系統コース別複勝率','父系統距離別出走頭数','父系統距離別勝率','父系統距離別連対率','父系統距離別複勝率',\n",
    "    '父系統同コース同距離別出走頭数','父系統同コース同距離別勝率','父系統同コース同距離別連対率','父系統同コース同距離別複勝率',\n",
    "    '季節勝率', '季節連対率', '季節複勝率', '逃げ率','先行率','中団率','追込率','マクリ率','上がり3F平均',\n",
    "    '勝率','同競馬場勝率','同距離勝率','同競馬場同距離勝率','同騎手騎乗勝率','コースタイプ勝率','同距離同クラス勝率','同枠タイプ生涯勝率',\n",
    "    '連対率','同競馬場連対率','同距離連対率','同競馬場同距離連対率','同騎手騎乗連対率','コースタイプ連対率','同距離同クラス連対率','同枠タイプ生涯連対率',\n",
    "    '複勝率','同競馬場複勝率','同距離複勝率','同競馬場同距離複勝率','同騎手騎乗複勝率','コースタイプ複勝率','同距離同クラス複勝率','同枠タイプ生涯複勝率',\n",
    "    '生涯出遅れ率','騎乗騎手年間出遅れ率', '同周り勝率', '同周り連対率', '同周り複勝率',\n",
    "\n",
    "    ], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    df2 = pd.get_dummies(df2, sparse=True)\n",
    "#     df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds', 'time_odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds', 'time_odds']]\n",
    "        pred_table['proba'] = self.predict_proba(X)\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds', 'win_ratio', 'proba']]\n",
    "#             pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "#         std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "#         .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "        \n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # 閾値を増やす        \n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b500961",
   "metadata": {},
   "outputs": [],
   "source": [
    "haitou = pd.read_csv('./csv_new2/race_detail.csv')\n",
    "haitou = haitou.set_index('race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4161b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrace = pd.read_pickle('./pickle_new/base_race_20220813_5.pickle')\n",
    "allrace = pd.read_pickle('./pickle_new/base_race_20220813_2_vec.pickle')\n",
    "time = pd.read_csv('./csv_new2/base/race_time.csv')\n",
    "allrace = allrace.merge(time, how='left', on='id')\n",
    "\n",
    "nige_senko = pd.read_csv('./csv_new2/base/race_nige.csv')\n",
    "allrace = allrace.merge(nige_senko, on='race_id')\n",
    "\n",
    "time_odds_base = pd.read_csv('./csv_new2/time_odds.csv')\n",
    "allrace = allrace.merge(time_odds_base, how='left', on='id')\n",
    "\n",
    "df = allrace.query('(course == 2 | course == 1)')\n",
    "all_r = preprocessing(df)\n",
    "# all_r = preprocessing(allrace)\n",
    "# all_r['popular'] = all_r['popular'].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "all_r.drop([\n",
    "  '気温', '風速', '風向',\n",
    "  '1走前着差', '2走前着差', '3走前着差',\n",
    "    '4走前着差', '5走前着差',\n",
    "  '1走前スピードZI','2走前スピードZI', '3走前スピードZI',\n",
    "    '4走前スピードZI', '5走前スピードZI',\n",
    "  '1走前スピード指数','2走前スピード指数', '3走前スピード指数',\n",
    "    '4走前スピード指数', '5走前スピード指数',\n",
    "    '1走前相対着順', '2走前相対着順','3走前相対着順','4走前相対着順','5走前相対着順',\n",
    "    '1走前相対人気', '2走前相対人気','3走前相対人気','4走前相対人気','5走前相対人気',\n",
    "      '1走前スピード指数偏差','2走前スピード指数偏差', '3走前スピード指数偏差',\n",
    "    '4走前スピード指数偏差', '5走前スピード指数偏差',\n",
    "    '1走前上がり3F順', '1走前先行指数偏差', '1走前上がり指数偏差', '1走前上がり指数',\n",
    "    '1走前人気','2走前人気','3走前人気','4走前人気','5走前人気',\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "categorical = process_categorical(all_r, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id',\n",
    "])\n",
    "\n",
    "categorical = categorical.reset_index()\n",
    "vec = pd.read_pickle('./pickle_new/peds_vec.pickle')\n",
    "# categorical = categorical.merge(vec[['horse_id',\"peds_2\",\"peds_3\",\"peds_4\",\"peds_5\",\"peds_6\",\"peds_7\",\"peds_8\",\"peds_9\",\"peds_10\",\n",
    "#     \"peds_11\",\"peds_12\",\"peds_13\",\"peds_14\",]], on='horse_id')\n",
    "categorical = categorical.merge(vec, on='horse_id')\n",
    "categorical = categorical.set_index('race_id')\n",
    "\n",
    "# target = pd.read_pickle('./pickle_new/new_race_20220904.pickle')\n",
    "# time_odds = pd.read_csv('./csv_new2/20220904/time_odds.csv')\n",
    "# target = target.merge(time_odds, how='left', on='id')\n",
    "# target = target[target['date'].notnull()]\n",
    "\n",
    "# target = target.query('(course == 2 | course == 1)')\n",
    "# target = preprocessing(target)\n",
    "# target['result'] = target['result'].map(lambda x: 1 if x == 1 else 0)\n",
    "# target.drop([\n",
    "#   '気温', '風速', '風向',\n",
    "#   '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI','4走前スピードZI', '5走前スピードZI',\n",
    "#   '1走前スピード指数','2走前スピード指数', '3走前スピード指数','4走前スピード指数', '5走前スピード指数',\n",
    "# #   '先行指数', 'ペース指数', '上がり指数', 'スピード指数'\n",
    "# ], axis=1, inplace=True)\n",
    "# for i in range(1, 63):\n",
    "#     target.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "# test1 = process_categorical(target,  [\n",
    "#     'producer', 'owner', 'training_course', \n",
    "#     'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "#     '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "#     'color_id', 'stallion_id', 'affiliation_id'\n",
    "# ])\n",
    "categorical.drop(['index'], axis=1, inplace=True)\n",
    "c = categorical.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4c40c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = c.copy()\n",
    "c['result'] = c['result'].map(lambda x: 1 if x <= 1 else 0)\n",
    "result_d = c.fillna(0)\n",
    "\n",
    "train1, valid1  = split_data(result_d)\n",
    "valid1, test2  = train_valid_split_data(valid1)\n",
    "\n",
    "X_train1_d  = train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_train1_d  = train1['result']\n",
    "X_valid1_d  = valid1.drop(['id', 'date', 'result',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_valid1_d  = valid1['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f6f45416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d = torch.Tensor(X_train1_d.values)\n",
    "t_train_d = torch.Tensor(t_train1_d.values)\n",
    "X_valid_d = torch.Tensor(X_valid1_d.drop(['odds', 'popular', 'time_odds'], axis=1).values)\n",
    "t_valid_d = torch.Tensor(t_valid1_d.values)\n",
    "\n",
    "t_train_d = t_train_d.reshape([-1, 1])\n",
    "t_valid_d = t_valid_d.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc5b4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1_d = test2.drop(['id', 'date', 'result',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_test1_d = test2['result']\n",
    "X_test_d = torch.Tensor(X_test1_d.drop(['odds', 'popular', 'time_odds'], axis=1).values)\n",
    "t_test_d = torch.Tensor(t_test1_d.values)\n",
    "\n",
    "t_test_d = t_test_d.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4fee4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_train_d, t_train_d)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ce6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(567, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e4714335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train:[loss=0.062, AUC=0.748], test:[loss=0.064, AUC=0.744]\n",
      "epoch: 1, train:[loss=0.062, AUC=0.734], test:[loss=0.065, AUC=0.724]\n",
      "epoch: 2, train:[loss=0.061, AUC=0.753], test:[loss=0.063, AUC=0.747]\n",
      "epoch: 3, train:[loss=0.061, AUC=0.751], test:[loss=0.064, AUC=0.749]\n",
      "epoch: 4, train:[loss=0.062, AUC=0.747], test:[loss=0.065, AUC=0.736]\n",
      "epoch: 5, train:[loss=0.061, AUC=0.762], test:[loss=0.063, AUC=0.760]\n",
      "epoch: 6, train:[loss=0.061, AUC=0.758], test:[loss=0.064, AUC=0.750]\n",
      "epoch: 7, train:[loss=0.061, AUC=0.759], test:[loss=0.063, AUC=0.750]\n",
      "epoch: 8, train:[loss=0.060, AUC=0.770], test:[loss=0.062, AUC=0.766]\n",
      "epoch: 9, train:[loss=0.060, AUC=0.768], test:[loss=0.063, AUC=0.756]\n",
      "epoch: 10, train:[loss=0.060, AUC=0.768], test:[loss=0.063, AUC=0.763]\n",
      "epoch: 11, train:[loss=0.061, AUC=0.766], test:[loss=0.064, AUC=0.760]\n",
      "epoch: 12, train:[loss=0.061, AUC=0.768], test:[loss=0.064, AUC=0.761]\n",
      "epoch: 13, train:[loss=0.061, AUC=0.769], test:[loss=0.063, AUC=0.758]\n",
      "epoch: 14, train:[loss=0.061, AUC=0.756], test:[loss=0.064, AUC=0.740]\n",
      "epoch: 15, train:[loss=0.060, AUC=0.770], test:[loss=0.063, AUC=0.762]\n",
      "epoch: 16, train:[loss=0.060, AUC=0.768], test:[loss=0.063, AUC=0.759]\n",
      "epoch: 17, train:[loss=0.060, AUC=0.769], test:[loss=0.063, AUC=0.764]\n",
      "epoch: 18, train:[loss=0.060, AUC=0.773], test:[loss=0.062, AUC=0.769]\n",
      "epoch: 19, train:[loss=0.060, AUC=0.771], test:[loss=0.062, AUC=0.765]\n",
      "epoch: 20, train:[loss=0.060, AUC=0.772], test:[loss=0.062, AUC=0.768]\n",
      "epoch: 21, train:[loss=0.060, AUC=0.770], test:[loss=0.062, AUC=0.763]\n",
      "epoch: 22, train:[loss=0.060, AUC=0.770], test:[loss=0.062, AUC=0.765]\n",
      "epoch: 23, train:[loss=0.061, AUC=0.771], test:[loss=0.063, AUC=0.764]\n",
      "epoch: 24, train:[loss=0.060, AUC=0.776], test:[loss=0.063, AUC=0.769]\n",
      "epoch: 25, train:[loss=0.060, AUC=0.767], test:[loss=0.063, AUC=0.766]\n",
      "epoch: 26, train:[loss=0.060, AUC=0.777], test:[loss=0.062, AUC=0.773]\n",
      "epoch: 27, train:[loss=0.060, AUC=0.774], test:[loss=0.062, AUC=0.769]\n",
      "epoch: 28, train:[loss=0.060, AUC=0.771], test:[loss=0.062, AUC=0.767]\n",
      "epoch: 29, train:[loss=0.061, AUC=0.770], test:[loss=0.063, AUC=0.763]\n",
      "epoch: 30, train:[loss=0.060, AUC=0.774], test:[loss=0.062, AUC=0.767]\n",
      "epoch: 31, train:[loss=0.060, AUC=0.770], test:[loss=0.062, AUC=0.765]\n",
      "epoch: 32, train:[loss=0.060, AUC=0.775], test:[loss=0.062, AUC=0.766]\n",
      "epoch: 33, train:[loss=0.060, AUC=0.772], test:[loss=0.062, AUC=0.766]\n",
      "epoch: 34, train:[loss=0.060, AUC=0.773], test:[loss=0.062, AUC=0.769]\n",
      "epoch: 35, train:[loss=0.060, AUC=0.776], test:[loss=0.062, AUC=0.770]\n",
      "epoch: 36, train:[loss=0.060, AUC=0.776], test:[loss=0.063, AUC=0.770]\n",
      "epoch: 37, train:[loss=0.060, AUC=0.773], test:[loss=0.063, AUC=0.764]\n",
      "epoch: 38, train:[loss=0.060, AUC=0.773], test:[loss=0.063, AUC=0.760]\n",
      "epoch: 39, train:[loss=0.060, AUC=0.775], test:[loss=0.063, AUC=0.768]\n",
      "epoch: 40, train:[loss=0.060, AUC=0.775], test:[loss=0.063, AUC=0.770]\n",
      "epoch: 41, train:[loss=0.060, AUC=0.775], test:[loss=0.062, AUC=0.767]\n",
      "epoch: 42, train:[loss=0.060, AUC=0.772], test:[loss=0.063, AUC=0.756]\n",
      "epoch: 43, train:[loss=0.060, AUC=0.777], test:[loss=0.062, AUC=0.773]\n",
      "epoch: 44, train:[loss=0.060, AUC=0.777], test:[loss=0.062, AUC=0.774]\n",
      "epoch: 45, train:[loss=0.060, AUC=0.776], test:[loss=0.062, AUC=0.771]\n",
      "epoch: 46, train:[loss=0.060, AUC=0.775], test:[loss=0.062, AUC=0.770]\n",
      "epoch: 47, train:[loss=0.060, AUC=0.777], test:[loss=0.062, AUC=0.772]\n",
      "epoch: 48, train:[loss=0.060, AUC=0.770], test:[loss=0.063, AUC=0.753]\n",
      "epoch: 49, train:[loss=0.060, AUC=0.779], test:[loss=0.062, AUC=0.772]\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "\n",
    "    for X, t in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(X)\n",
    "        loss = loss_fn(y, t)\n",
    "        # 傾きの計算\n",
    "        loss.backward()\n",
    "        # optimizerの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_train_d = model(X_train_d)\n",
    "    y_valid_d = model(X_valid_d)\n",
    "   #  平均二乗誤差 予測値と正解値の誤差の計算\n",
    "    loss_train = loss_fn(y_train_d, t_train_d)\n",
    "    loss_valid = loss_fn(y_valid_d, t_valid_d)\n",
    "    auc_train = roc_auc_score(t_train_d.detach().numpy(), y_train_d.detach().numpy())\n",
    "    auc_valid = roc_auc_score(t_valid_d.detach().numpy(), y_valid_d.detach().numpy())\n",
    "    \n",
    "    print('epoch: {}, train:[loss={:.3f}, AUC={:.3f}], test:[loss={:.3f}, AUC={:.3f}]'.  format(epoch, loss_train, auc_train, loss_valid, auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10017a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'deep.pth')\n",
    "# model.state_dict()\n",
    "# model.load_state_dict(torch.load('deep.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "652de002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768945187891358"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_d = model(X_test_d)\n",
    "roc_auc_score(t_test_d.detach().numpy(), y_test_d.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c212b620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = t_pred_d\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = proba\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['pred'] = x['proba'].map(lambda x: 0 if x < 0.5 else 1)\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['pred_rank'] = x[['proba']].groupby(level=0).rank(ascending=False)\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['win_ratio'] = [(p / sum1.loc[i])[0] for i, p in t_pred_d.items()]\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['expected'] = x['win_ratio'] * x['time_odds']\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['d_odds'] = 0.8 / x['win_ratio']\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['d_odds'] = x['d_odds'].map(lambda x: format(x, '.1f'))\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_80878/3592491816.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['d_odds'] = x['d_odds'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "x = valid1[['h_num', 'odds', 'time_odds', 'popular']]\n",
    "t_pred_d = pd.Series(np.around(torch.flatten(y_valid_d).detach().numpy(), decimals=5), index=x.index)\n",
    "sum1 = pd.DataFrame(t_pred_d.groupby(level=0).sum())\n",
    "\n",
    "x['proba'] = t_pred_d\n",
    "proba = x[['proba']]\n",
    "standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "\n",
    "x['proba'] = proba\n",
    "x['pred'] = x['proba'].map(lambda x: 0 if x < 0.5 else 1)\n",
    "x['pred_rank'] = x[['proba']].groupby(level=0).rank(ascending=False)\n",
    "x['win_ratio'] = [(p / sum1.loc[i])[0] for i, p in t_pred_d.items()]\n",
    "x['expected'] = x['win_ratio'] * x['time_odds']\n",
    "\n",
    "x['d_odds'] = 0.8 / x['win_ratio']\n",
    "x['d_odds'] = x['d_odds'].map(lambda x: format(x, '.1f'))\n",
    "x['d_odds'] = x['d_odds'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7d83f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "shisuu = pd.read_csv('./shisuu_new.csv')\n",
    "v = valid1.reset_index()[['race_id', 'h_num', 'id']]\n",
    "v['horse_race_id'] = v['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82672b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = v.drop(['id'], axis=1).merge(shisuu, on='horse_race_id')\n",
    "sv = sv.merge(x, on=['race_id', 'h_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c9251469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>pred</th>\n",
       "      <th>proba</th>\n",
       "      <th>expected</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>popular</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>d_odds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851311</td>\n",
       "      <td>1.232079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018041503010412</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566258</td>\n",
       "      <td>0.486076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018041503010412</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.584404</td>\n",
       "      <td>0.906133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.210729</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.931843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258845</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>11</td>\n",
       "      <td>8.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718757</td>\n",
       "      <td>2.264664</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.226466</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137463</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865565</td>\n",
       "      <td>0.769499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265345</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137473</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>22.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507563</td>\n",
       "      <td>2.665220</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.117930</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137474</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>16</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680255</td>\n",
       "      <td>1.026541</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.263216</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137475</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648918</td>\n",
       "      <td>0.605434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242174</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137479</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>12</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702749</td>\n",
       "      <td>1.252437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.278319</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25696 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 race_id  h_num  odds  time_odds  pred     proba  expected  \\\n",
       "5       2018041509020810      8   4.5        4.2     1  0.851311  1.232079   \n",
       "13      2018041503010412      4   2.5        2.4     1  0.566258  0.486076   \n",
       "20      2018041503010412      2   5.2        4.3     1  0.584404  0.906133   \n",
       "22      2018041506030807      2   4.0        3.6     1  0.784431  0.931843   \n",
       "33      2018041506030807     11   8.9       10.0     1  0.718757  2.264664   \n",
       "...                  ...    ...   ...        ...   ...       ...       ...   \n",
       "137463  2021042405020111      2   3.1        2.9     1  0.865565  0.769499   \n",
       "137473  2021042405020111      4  17.8       22.6     1  0.507563  2.665220   \n",
       "137474  2021042404010505     16   3.8        3.9     1  0.680255  1.026541   \n",
       "137475  2021042404010505      9   2.5        2.5     1  0.648918  0.605434   \n",
       "137479  2021042404010505     12   4.4        4.5     1  0.702749  1.252437   \n",
       "\n",
       "        pred_rank  popular  win_ratio  d_odds  score  \n",
       "5             1.0      1.0   0.293352     2.7      4  \n",
       "13            2.0      1.0   0.202532     4.0      5  \n",
       "20            1.0      3.0   0.210729     3.8      5  \n",
       "22            1.0      1.0   0.258845     3.1     -6  \n",
       "33            2.0      5.0   0.226466     3.5     -2  \n",
       "...           ...      ...        ...     ...    ...  \n",
       "137463        1.0      1.0   0.265345     3.0     17  \n",
       "137473        2.0      9.0   0.117930     6.8     -2  \n",
       "137474        2.0      2.0   0.263216     3.0     -9  \n",
       "137475        3.0      1.0   0.242174     3.3     -6  \n",
       "137479        1.0      3.0   0.278319     2.9      0  \n",
       "\n",
       "[25696 rows x 12 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = sv[['race_id', 'h_num', 'odds', 'time_odds', 'pred', 'proba', 'expected', 'pred_rank', 'popular', 'win_ratio', 'd_odds', 'score']]\n",
    "bt1 = x1[\n",
    "    (x1['pred'] == 1)\n",
    "#     &\n",
    "#     ((x1['pred_rank'] == 1))\n",
    "#     &\n",
    "#      (x1['time_odds'] / x1['d_odds'] >= 1)\n",
    "#     &\n",
    "#     (x1['expected'] >= 1)\n",
    "#     &\n",
    "#     (x1['score'] > 0)\n",
    "]\n",
    "bt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b1807862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     8658\n",
       "2.0     6122\n",
       "3.0     4028\n",
       "4.0     2503\n",
       "5.0     1569\n",
       "6.0     1012\n",
       "7.0      671\n",
       "8.0      424\n",
       "9.0      279\n",
       "10.0     162\n",
       "11.0     110\n",
       "12.0      73\n",
       "13.0      38\n",
       "14.0      23\n",
       "15.0      16\n",
       "16.0       7\n",
       "17.0       1\n",
       "Name: popular, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt1['popular'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "25815fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：25696 レース数:10060 出現頻度:100.0% 的中数:5145 的中率:20.0% 賭金:2,569,600円 配当合計:2,011,850円 最高配当:8,070円 回収率:78.3%\n"
     ]
    }
   ],
   "source": [
    "bh = bt1.merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} レース数:{} 出現頻度:{:.1%} 的中数:{} 的中率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(bt1),\\\n",
    "    len(bt1.groupby('race_id')),\\\n",
    "    len(bt1.groupby('race_id')) / len(valid1.groupby('race_id')),\n",
    "    len(bh[bh['h_num'] == bh['1着馬番']]),\\\n",
    "    len(bh[bh['h_num'] == bh['1着馬番']]) / (len(bt1)), \\\n",
    "    len(bt1) * 100,\\\n",
    "    bh[bh['h_num'] == bh['1着馬番']]['単勝'].sum() ,\\\n",
    "    bh[bh['h_num'] == bh['1着馬番']]['単勝'].max(), \\\n",
    "    (bh[bh['h_num'] == bh['1着馬番']]['単勝'].sum() / (len(bt1) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "369337ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "X_train_t = train1.drop(['id', 'horse_id', 'date','correct_time', 'time_odds',\n",
    "                         'time_popular', 'result', 'odds', 'popular', 'producer', 'owner'], axis=1)\n",
    "# X_train = train.drop(['date'], axis=1)\n",
    "y_train_t = train1['correct_time']\n",
    "X_valid_t = valid1.drop(['date', 'horse_id', 'correct_time', 'time_odds', 'time_popular', 'result', 'odds',  'producer', 'owner'], axis=1)\n",
    "y_valid_t = valid1['correct_time']\n",
    "X_test_t = test2.drop(['date', 'horse_id', 'correct_time', 'time_odds', 'time_popular', 'result', 'odds',  'producer', 'owner'], axis=1)\n",
    "y_test_t = test2['correct_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f80ddd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 115062\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 88.240528\n",
      "[1]\tTrain's rmse: 11.6518\tTest's rmse: 12.3318\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tTrain's rmse: 11.4559\tTest's rmse: 12.1465\n",
      "[3]\tTrain's rmse: 11.2786\tTest's rmse: 11.9799\n",
      "[4]\tTrain's rmse: 11.1306\tTest's rmse: 11.846\n",
      "[5]\tTrain's rmse: 10.9582\tTest's rmse: 11.6929\n",
      "[6]\tTrain's rmse: 10.815\tTest's rmse: 11.5714\n",
      "[7]\tTrain's rmse: 10.6936\tTest's rmse: 11.4649\n",
      "[8]\tTrain's rmse: 10.6048\tTest's rmse: 11.3931\n",
      "[9]\tTrain's rmse: 10.5117\tTest's rmse: 11.3157\n",
      "[10]\tTrain's rmse: 10.4267\tTest's rmse: 11.2492\n",
      "[11]\tTrain's rmse: 10.3577\tTest's rmse: 11.1979\n",
      "[12]\tTrain's rmse: 10.293\tTest's rmse: 11.1515\n",
      "[13]\tTrain's rmse: 10.2149\tTest's rmse: 11.0953\n",
      "[14]\tTrain's rmse: 10.1513\tTest's rmse: 11.0471\n",
      "[15]\tTrain's rmse: 10.095\tTest's rmse: 11.0119\n",
      "[16]\tTrain's rmse: 10.0478\tTest's rmse: 10.9797\n",
      "[17]\tTrain's rmse: 9.99768\tTest's rmse: 10.9483\n",
      "[18]\tTrain's rmse: 9.95133\tTest's rmse: 10.9239\n",
      "[19]\tTrain's rmse: 9.91214\tTest's rmse: 10.9032\n",
      "[20]\tTrain's rmse: 9.8741\tTest's rmse: 10.8844\n",
      "[21]\tTrain's rmse: 9.82641\tTest's rmse: 10.8547\n",
      "[22]\tTrain's rmse: 9.78401\tTest's rmse: 10.8319\n",
      "[23]\tTrain's rmse: 9.74593\tTest's rmse: 10.8152\n",
      "[24]\tTrain's rmse: 9.71099\tTest's rmse: 10.8043\n",
      "[25]\tTrain's rmse: 9.67419\tTest's rmse: 10.7914\n",
      "[26]\tTrain's rmse: 9.63805\tTest's rmse: 10.7807\n",
      "[27]\tTrain's rmse: 9.60213\tTest's rmse: 10.7648\n",
      "[28]\tTrain's rmse: 9.56969\tTest's rmse: 10.7474\n",
      "[29]\tTrain's rmse: 9.5393\tTest's rmse: 10.7424\n",
      "[30]\tTrain's rmse: 9.50539\tTest's rmse: 10.729\n",
      "[31]\tTrain's rmse: 9.47705\tTest's rmse: 10.7206\n",
      "[32]\tTrain's rmse: 9.44793\tTest's rmse: 10.7112\n",
      "[33]\tTrain's rmse: 9.41713\tTest's rmse: 10.6995\n",
      "[34]\tTrain's rmse: 9.38902\tTest's rmse: 10.6954\n",
      "[35]\tTrain's rmse: 9.36405\tTest's rmse: 10.691\n",
      "[36]\tTrain's rmse: 9.33527\tTest's rmse: 10.6822\n",
      "[37]\tTrain's rmse: 9.30849\tTest's rmse: 10.6792\n",
      "[38]\tTrain's rmse: 9.28239\tTest's rmse: 10.6729\n",
      "[39]\tTrain's rmse: 9.25498\tTest's rmse: 10.6619\n",
      "[40]\tTrain's rmse: 9.22729\tTest's rmse: 10.6553\n",
      "[41]\tTrain's rmse: 9.20437\tTest's rmse: 10.6523\n",
      "[42]\tTrain's rmse: 9.1803\tTest's rmse: 10.6466\n",
      "[43]\tTrain's rmse: 9.15763\tTest's rmse: 10.643\n",
      "[44]\tTrain's rmse: 9.13645\tTest's rmse: 10.6422\n",
      "[45]\tTrain's rmse: 9.11438\tTest's rmse: 10.6377\n",
      "[46]\tTrain's rmse: 9.09133\tTest's rmse: 10.6356\n",
      "[47]\tTrain's rmse: 9.07115\tTest's rmse: 10.6316\n",
      "[48]\tTrain's rmse: 9.04804\tTest's rmse: 10.6272\n",
      "[49]\tTrain's rmse: 9.02754\tTest's rmse: 10.6255\n",
      "[50]\tTrain's rmse: 9.00517\tTest's rmse: 10.6223\n",
      "[51]\tTrain's rmse: 8.98363\tTest's rmse: 10.6193\n",
      "[52]\tTrain's rmse: 8.96089\tTest's rmse: 10.6155\n",
      "[53]\tTrain's rmse: 8.93968\tTest's rmse: 10.6146\n",
      "[54]\tTrain's rmse: 8.91685\tTest's rmse: 10.6124\n",
      "[55]\tTrain's rmse: 8.89706\tTest's rmse: 10.6108\n",
      "[56]\tTrain's rmse: 8.87718\tTest's rmse: 10.6096\n",
      "[57]\tTrain's rmse: 8.85695\tTest's rmse: 10.6053\n",
      "[58]\tTrain's rmse: 8.83609\tTest's rmse: 10.5996\n",
      "[59]\tTrain's rmse: 8.81667\tTest's rmse: 10.5972\n",
      "[60]\tTrain's rmse: 8.79757\tTest's rmse: 10.5931\n",
      "[61]\tTrain's rmse: 8.78157\tTest's rmse: 10.5932\n",
      "[62]\tTrain's rmse: 8.76239\tTest's rmse: 10.5918\n",
      "[63]\tTrain's rmse: 8.74533\tTest's rmse: 10.5929\n",
      "[64]\tTrain's rmse: 8.72935\tTest's rmse: 10.5913\n",
      "[65]\tTrain's rmse: 8.7122\tTest's rmse: 10.5903\n",
      "[66]\tTrain's rmse: 8.69349\tTest's rmse: 10.588\n",
      "[67]\tTrain's rmse: 8.67633\tTest's rmse: 10.5868\n",
      "[68]\tTrain's rmse: 8.65904\tTest's rmse: 10.5847\n",
      "[69]\tTrain's rmse: 8.6426\tTest's rmse: 10.5839\n",
      "[70]\tTrain's rmse: 8.62602\tTest's rmse: 10.5821\n",
      "[71]\tTrain's rmse: 8.6083\tTest's rmse: 10.5815\n",
      "[72]\tTrain's rmse: 8.5921\tTest's rmse: 10.5821\n",
      "[73]\tTrain's rmse: 8.57581\tTest's rmse: 10.5795\n",
      "[74]\tTrain's rmse: 8.55786\tTest's rmse: 10.5741\n",
      "[75]\tTrain's rmse: 8.53988\tTest's rmse: 10.5701\n",
      "[76]\tTrain's rmse: 8.52571\tTest's rmse: 10.5703\n",
      "[77]\tTrain's rmse: 8.50932\tTest's rmse: 10.569\n",
      "[78]\tTrain's rmse: 8.49411\tTest's rmse: 10.5684\n",
      "[79]\tTrain's rmse: 8.47927\tTest's rmse: 10.5676\n",
      "[80]\tTrain's rmse: 8.46516\tTest's rmse: 10.5664\n",
      "[81]\tTrain's rmse: 8.45094\tTest's rmse: 10.5654\n",
      "[82]\tTrain's rmse: 8.43384\tTest's rmse: 10.5641\n",
      "[83]\tTrain's rmse: 8.41962\tTest's rmse: 10.5638\n",
      "[84]\tTrain's rmse: 8.40404\tTest's rmse: 10.5645\n",
      "[85]\tTrain's rmse: 8.38984\tTest's rmse: 10.5652\n",
      "[86]\tTrain's rmse: 8.37535\tTest's rmse: 10.5662\n",
      "[87]\tTrain's rmse: 8.36242\tTest's rmse: 10.5655\n",
      "[88]\tTrain's rmse: 8.34872\tTest's rmse: 10.5657\n",
      "[89]\tTrain's rmse: 8.33369\tTest's rmse: 10.5645\n",
      "[90]\tTrain's rmse: 8.31996\tTest's rmse: 10.5649\n",
      "[91]\tTrain's rmse: 8.30848\tTest's rmse: 10.5656\n",
      "[92]\tTrain's rmse: 8.29505\tTest's rmse: 10.5662\n",
      "[93]\tTrain's rmse: 8.2819\tTest's rmse: 10.5669\n",
      "[94]\tTrain's rmse: 8.26771\tTest's rmse: 10.5662\n",
      "[95]\tTrain's rmse: 8.25582\tTest's rmse: 10.5672\n",
      "[96]\tTrain's rmse: 8.24238\tTest's rmse: 10.5684\n",
      "[97]\tTrain's rmse: 8.22869\tTest's rmse: 10.5674\n",
      "[98]\tTrain's rmse: 8.2156\tTest's rmse: 10.5686\n",
      "[99]\tTrain's rmse: 8.20449\tTest's rmse: 10.568\n",
      "[100]\tTrain's rmse: 8.19099\tTest's rmse: 10.5694\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's rmse: 8.19099\tTest's rmse: 10.5694\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "params = {\n",
    "#   'task': 'train',\n",
    "#   'boosting_type': 'gbdt',\n",
    "  'objective': 'regression',\n",
    "  'metric': 'rmse', \n",
    "#   'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "# model = lgb.LGBMRegressor()\n",
    "# model.fit(\n",
    "#     X_train, \n",
    "#     y_train,\n",
    "#     eval_set=[(X_test.drop(['id'], axis=1), y_test), (X_train, y_train)],\n",
    "#     verbose=-1\n",
    "# )\n",
    "\n",
    "# lgb.plot_metric(model)\n",
    "lgb_train = lgb.Dataset(X_train_t, y_train_t)\n",
    "lgb_valid = lgb.Dataset(X_valid_t.drop(['id', 'popular'], axis=1), y_valid_t, reference=lgb_train)\n",
    "# lgb_test = lgb.Dataset(X_test.drop(['id'], axis=1), y_test, reference=lgb_train)\n",
    "\n",
    "lgb_results = {} \n",
    "\n",
    "params = {\n",
    " 'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 4.6593794482838764e-07,\n",
    " 'lambda_l2': 7.9635811807277594,\n",
    " 'num_leaves': 84,\n",
    " 'feature_fraction': 0.484,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 100,\n",
    " 'num_iterations': 100,\n",
    " 'early_stopping_round': 50,\n",
    "#  'categorical_column': [0, 3, 5, 6, 7, 8, 359, 363, 364, 372, 373, 375, 376]\n",
    "}\n",
    "\n",
    "model_t = lgb.train(\n",
    "  params=params,\n",
    "  train_set=lgb_train,\n",
    "  valid_sets=[lgb_train, lgb_valid],\n",
    "#   valid_sets=[lgb_train, lgb_test],\n",
    "  valid_names=['Train', 'Test'],\n",
    "  num_boost_round=100,\n",
    ") \n",
    "\n",
    "# model = lgb_o.train(\n",
    "#     params,\n",
    "#     lgb_train,\n",
    "#     valid_sets=lgb_valid,\n",
    "#     verbose_eval=False,\n",
    "#     show_progress_bar=False,\n",
    "#     num_boost_round=100,\n",
    "#     early_stopping_rounds=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8ae757ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2874086922656477"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = X_valid_t.drop(['id', 'popular'], axis=1)\n",
    "y_pred_t = model_t.predict(X)\n",
    "r2_score(y_valid_t, y_pred_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8c8c2688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>peds_6</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>peds_62</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>peds_14</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>peds_30</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>peds_2</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trainer_id</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>peds_61</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>peds_29</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>peds_60</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>stallion_id</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>peds_28</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>peds_13</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distance</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>peds_5</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>peds_58</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>peds_26</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>grade</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>peds_12</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>peds_54</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>peds_1</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>peds_4</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1走前上がり3F</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>間隔</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>peds_10</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1走前上がり3F速度</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>peds_46</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>peds_22</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>course</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1走前コンピ順位</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>獲得賞金合計</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>furlong_1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>training_course</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>peds_53</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>コースタイプ複勝率偏差</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>peds_57</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1走前距離</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>furlong_2_1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>馬場状態</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2走前上がり3F順</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            features  importance\n",
       "508           peds_6         770\n",
       "564          peds_62         660\n",
       "516          peds_14         610\n",
       "532          peds_30         574\n",
       "504           peds_2         560\n",
       "6         trainer_id         451\n",
       "563          peds_61         418\n",
       "3          jockey_id         335\n",
       "531          peds_29         254\n",
       "562          peds_60         253\n",
       "341      stallion_id         199\n",
       "530          peds_28         192\n",
       "515          peds_13         187\n",
       "2           distance         165\n",
       "507           peds_5         154\n",
       "560          peds_58         137\n",
       "528          peds_26         136\n",
       "353            grade         128\n",
       "514          peds_12         122\n",
       "556          peds_54         113\n",
       "503           peds_1         105\n",
       "506           peds_4          87\n",
       "27          1走前上がり3F          76\n",
       "12                間隔          71\n",
       "512          peds_10          66\n",
       "28        1走前上がり3F速度          64\n",
       "548          peds_46          62\n",
       "524          peds_22          56\n",
       "1             course          56\n",
       "18          1走前コンピ順位          49\n",
       "359           獲得賞金合計          38\n",
       "331        furlong_1          34\n",
       "337  training_course          33\n",
       "555          peds_53          33\n",
       "481      コースタイプ複勝率偏差          31\n",
       "559          peds_57          29\n",
       "39             1走前距離          29\n",
       "336      furlong_2_1          29\n",
       "351             馬場状態          29\n",
       "89         2走前上がり3F順          27"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'features': X.columns, 'importance':model_t.feature_importance()})\n",
    "importances.sort_values('importance', ascending=False)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "710b164a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = X_valid_t.drop(['id'], axis=1)\n",
    "actual = actual.reset_index()\n",
    "actual['pred_time'] = model_t.predict(X_valid_t.drop(['id', 'popular'], axis=1))\n",
    "actual['pred_rank'] = actual[['race_id', 'pred_time']].groupby('race_id').rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "43290933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>pred_time</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>popular</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>3</td>\n",
       "      <td>89.127693</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>2</td>\n",
       "      <td>94.199679</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>12</td>\n",
       "      <td>92.770940</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>13</td>\n",
       "      <td>94.869807</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>9</td>\n",
       "      <td>94.767549</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137483</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>3</td>\n",
       "      <td>91.257537</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137484</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>2</td>\n",
       "      <td>79.548596</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137485</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>6</td>\n",
       "      <td>90.968591</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137486</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>7</td>\n",
       "      <td>79.894412</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137487</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>15</td>\n",
       "      <td>86.897401</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137488 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 race_id  h_num  pred_time  pred_rank  popular  score\n",
       "0       2018041509020810      3  89.127693       11.0      2.0    -10\n",
       "1       2018041509020810      2  94.199679        5.0      5.0      1\n",
       "2       2018041509020810     12  92.770940        7.0      6.0     -4\n",
       "3       2018041509020810     13  94.869807        3.0      3.0     11\n",
       "4       2018041509020810      9  94.767549        4.0     10.0     -7\n",
       "...                  ...    ...        ...        ...      ...    ...\n",
       "137483  2021042404010505      3  91.257537        4.0      5.0     -2\n",
       "137484  2021042404010505      2  79.548596       14.0     16.0    -20\n",
       "137485  2021042404010505      6  90.968591        5.0     11.0     -8\n",
       "137486  2021042404010505      7  79.894412       13.0     14.0    -13\n",
       "137487  2021042404010505     15  86.897401       10.0     10.0      0\n",
       "\n",
       "[137488 rows x 6 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sva = v.drop(['id'], axis=1).merge(shisuu, on='horse_race_id')\n",
    "sva = sva.merge(actual, on=['race_id', 'h_num'])\n",
    "sva[['race_id', 'h_num', 'pred_time', 'pred_rank', 'popular', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cbf78d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：10060 出現確率：100.0% 的中率:23.55% 的中数:2369 賭金:1,006,000円 配当合計:777,280円 最高配当:4,430円 回収率:77.26%\n"
     ]
    }
   ],
   "source": [
    "b_actual = sva.merge(haitou, on='race_id')\n",
    "t_bt = b_actual[\n",
    "#     ((b_actual['pred_time'] >= 85))\n",
    "#     &\n",
    "#     (b_actual['score'] > 5)\n",
    "    b_actual['pred_rank'] == 1\n",
    "]\n",
    "\n",
    "print(\"点数：{} 出現確率：{:.1%} 的中率:{:.2%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.2%}\". format(\n",
    "    len(t_bt),\\\n",
    "    len(t_bt.groupby('race_id')) / len(X_valid_t.groupby('race_id')),\n",
    "    len(t_bt[t_bt['h_num'] == t_bt['1着馬番']]) / (len(t_bt)),\\\n",
    "    len(t_bt[t_bt['h_num'] == t_bt['1着馬番']]),\\\n",
    "    len(t_bt) * 100,\\\n",
    "    t_bt[t_bt['h_num'] == t_bt['1着馬番']]['単勝'].sum(),\\\n",
    "    t_bt[t_bt['h_num'] == t_bt['1着馬番']]['単勝'].max(),\\\n",
    "    (t_bt[t_bt['h_num'] == t_bt['1着馬番']]['単勝'].sum() / (len(t_bt) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8de01c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'correct_time', 'popular', 'horse_id'], axis=1)\n",
    "y_train1 = train1['result']\n",
    "X_valid1 = valid1.drop(['date', 'result', 'popular',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "y_valid1 = valid1['result']\n",
    "X_test1 = test2.drop(['date', 'result', 'popular',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "y_test1 = test2['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4c4e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-03 12:41:44,354]\u001b[0m A new study created in memory with name: no-name-460c1b4c-f677-4d1c-afb6-349a9d672353\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                   | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.601233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.243580:  14%|8     | 1/7 [00:33<03:20, 33.41s/it]\u001b[32m[I 2022-10-03 12:42:17,872]\u001b[0m Trial 0 finished with value: 0.243580150428037 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.243580150428037.\u001b[0m\n",
      "feature_fraction, val_score: 0.243580:  14%|8     | 1/7 [00:33<03:20, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.217863\tvalid_1's binary_logloss: 0.24358\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241950:  29%|#7    | 2/7 [01:12<03:02, 36.49s/it]\u001b[32m[I 2022-10-03 12:42:56,551]\u001b[0m Trial 1 finished with value: 0.2419499125744315 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.2419499125744315.\u001b[0m\n",
      "feature_fraction, val_score: 0.241950:  29%|#7    | 2/7 [01:12<03:02, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.215706\tvalid_1's binary_logloss: 0.24195\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241821:  43%|##5   | 3/7 [01:46<02:22, 35.70s/it]\u001b[32m[I 2022-10-03 12:43:31,303]\u001b[0m Trial 2 finished with value: 0.2418206045241696 and parameters: {'feature_fraction': 0.7}. Best is trial 2 with value: 0.2418206045241696.\u001b[0m\n",
      "feature_fraction, val_score: 0.241821:  43%|##5   | 3/7 [01:46<02:22, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.217226\tvalid_1's binary_logloss: 0.241821\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241801:  57%|###4  | 4/7 [02:21<01:46, 35.45s/it]\u001b[32m[I 2022-10-03 12:44:06,368]\u001b[0m Trial 3 finished with value: 0.24180148551691705 and parameters: {'feature_fraction': 0.4}. Best is trial 3 with value: 0.24180148551691705.\u001b[0m\n",
      "feature_fraction, val_score: 0.241801:  57%|###4  | 4/7 [02:21<01:46, 35.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.215814\tvalid_1's binary_logloss: 0.241801\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.342982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241801:  71%|####2 | 5/7 [02:52<01:07, 33.74s/it]\u001b[32m[I 2022-10-03 12:44:37,070]\u001b[0m Trial 4 finished with value: 0.2427951610738419 and parameters: {'feature_fraction': 1.0}. Best is trial 3 with value: 0.24180148551691705.\u001b[0m\n",
      "feature_fraction, val_score: 0.241801:  71%|####2 | 5/7 [02:52<01:07, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.220053\tvalid_1's binary_logloss: 0.242795\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241801:  86%|#####1| 6/7 [03:27<00:34, 34.24s/it]\u001b[32m[I 2022-10-03 12:45:12,280]\u001b[0m Trial 5 finished with value: 0.24261669839271857 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 3 with value: 0.24180148551691705.\u001b[0m\n",
      "feature_fraction, val_score: 0.241801:  86%|#####1| 6/7 [03:27<00:34, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.216073\tvalid_1's binary_logloss: 0.242617\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.317167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.241283: 100%|######| 7/7 [04:00<00:00, 33.81s/it]\u001b[32m[I 2022-10-03 12:45:45,207]\u001b[0m Trial 6 finished with value: 0.24128271122102019 and parameters: {'feature_fraction': 0.8}. Best is trial 6 with value: 0.24128271122102019.\u001b[0m\n",
      "feature_fraction, val_score: 0.241283: 100%|######| 7/7 [04:00<00:00, 34.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.217972\tvalid_1's binary_logloss: 0.241283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:   0%|                   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:   5%|5          | 1/20 [00:28<08:59, 28.42s/it]\u001b[32m[I 2022-10-03 12:46:13,712]\u001b[0m Trial 7 finished with value: 0.24963439465502213 and parameters: {'num_leaves': 251}. Best is trial 7 with value: 0.24963439465502213.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:   5%|5          | 1/20 [00:28<08:59, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.224695\tvalid_1's binary_logloss: 0.249634\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  10%|#1         | 2/20 [01:01<09:25, 31.44s/it]\u001b[32m[I 2022-10-03 12:46:47,275]\u001b[0m Trial 8 finished with value: 0.24442352123546088 and parameters: {'num_leaves': 73}. Best is trial 8 with value: 0.24442352123546088.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  10%|#1         | 2/20 [01:02<09:25, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.213243\tvalid_1's binary_logloss: 0.244424\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.294721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  15%|#6         | 3/20 [01:29<08:20, 29.45s/it]\u001b[32m[I 2022-10-03 12:47:14,343]\u001b[0m Trial 9 finished with value: 0.24844550167012094 and parameters: {'num_leaves': 213}. Best is trial 8 with value: 0.24442352123546088.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  15%|#6         | 3/20 [01:29<08:20, 29.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.228752\tvalid_1's binary_logloss: 0.248446\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.310086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  20%|##2        | 4/20 [02:02<08:16, 31.01s/it]\u001b[32m[I 2022-10-03 12:47:47,747]\u001b[0m Trial 10 finished with value: 0.2422019584384819 and parameters: {'num_leaves': 28}. Best is trial 10 with value: 0.2422019584384819.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  20%|##2        | 4/20 [02:02<08:16, 31.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.217482\tvalid_1's binary_logloss: 0.242202\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  25%|##7        | 5/20 [02:30<07:31, 30.09s/it]\u001b[32m[I 2022-10-03 12:48:16,220]\u001b[0m Trial 11 finished with value: 0.2484867306117287 and parameters: {'num_leaves': 206}. Best is trial 10 with value: 0.2422019584384819.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  25%|##7        | 5/20 [02:31<07:31, 30.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.223693\tvalid_1's binary_logloss: 0.248487\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.368557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  30%|###3       | 6/20 [03:01<07:03, 30.24s/it]\u001b[32m[I 2022-10-03 12:48:46,754]\u001b[0m Trial 12 finished with value: 0.2421243990020739 and parameters: {'num_leaves': 10}. Best is trial 12 with value: 0.2421243990020739.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  30%|###3       | 6/20 [03:01<07:03, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.22126\tvalid_1's binary_logloss: 0.242124\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.314399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  35%|###8       | 7/20 [03:38<07:01, 32.43s/it]\u001b[32m[I 2022-10-03 12:49:23,668]\u001b[0m Trial 13 finished with value: 0.24286343053292084 and parameters: {'num_leaves': 74}. Best is trial 12 with value: 0.2421243990020739.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  35%|###8       | 7/20 [03:38<07:01, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.210598\tvalid_1's binary_logloss: 0.242863\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.297976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.223873\tvalid_1's binary_logloss: 0.244501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  40%|####4      | 8/20 [04:14<06:42, 33.58s/it]\u001b[32m[I 2022-10-03 12:49:59,719]\u001b[0m Trial 14 finished with value: 0.24349412056830225 and parameters: {'num_leaves': 2}. Best is trial 12 with value: 0.2421243990020739.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  40%|####4      | 8/20 [04:14<06:42, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's binary_logloss: 0.222601\tvalid_1's binary_logloss: 0.243494\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.298953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  45%|####9      | 9/20 [04:43<05:54, 32.23s/it]\u001b[32m[I 2022-10-03 12:50:28,988]\u001b[0m Trial 15 finished with value: 0.24771244609380222 and parameters: {'num_leaves': 122}. Best is trial 12 with value: 0.2421243990020739.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  45%|####9      | 9/20 [04:43<05:54, 32.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.225295\tvalid_1's binary_logloss: 0.247712\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.298291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  50%|#####     | 10/20 [05:17<05:26, 32.64s/it]\u001b[32m[I 2022-10-03 12:51:02,555]\u001b[0m Trial 16 finished with value: 0.24208472681569196 and parameters: {'num_leaves': 50}. Best is trial 16 with value: 0.24208472681569196.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  50%|#####     | 10/20 [05:17<05:26, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.216593\tvalid_1's binary_logloss: 0.242085\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  55%|#####5    | 11/20 [05:48<04:50, 32.32s/it]\u001b[32m[I 2022-10-03 12:51:34,142]\u001b[0m Trial 17 finished with value: 0.24738615706209854 and parameters: {'num_leaves': 126}. Best is trial 16 with value: 0.24208472681569196.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  55%|#####5    | 11/20 [05:48<04:50, 32.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.218064\tvalid_1's binary_logloss: 0.247386\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.320491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  60%|######    | 12/20 [06:18<04:12, 31.57s/it]\u001b[32m[I 2022-10-03 12:52:04,002]\u001b[0m Trial 18 finished with value: 0.2441009053784163 and parameters: {'num_leaves': 55}. Best is trial 16 with value: 0.24208472681569196.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  60%|######    | 12/20 [06:18<04:12, 31.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.223109\tvalid_1's binary_logloss: 0.244101\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.317957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  65%|######5   | 13/20 [06:58<03:57, 33.91s/it]\u001b[32m[I 2022-10-03 12:52:43,314]\u001b[0m Trial 19 finished with value: 0.24133826875217473 and parameters: {'num_leaves': 7}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  65%|######5   | 13/20 [06:58<03:57, 33.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.21859\tvalid_1's binary_logloss: 0.241338\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.330122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  70%|#######   | 14/20 [07:36<03:31, 35.28s/it]\u001b[32m[I 2022-10-03 12:53:21,729]\u001b[0m Trial 20 finished with value: 0.2429680441969706 and parameters: {'num_leaves': 52}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  70%|#######   | 14/20 [07:36<03:31, 35.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.212455\tvalid_1's binary_logloss: 0.242968\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.323441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  75%|#######5  | 15/20 [08:09<02:53, 34.61s/it]\u001b[32m[I 2022-10-03 12:53:54,795]\u001b[0m Trial 21 finished with value: 0.24736995993824928 and parameters: {'num_leaves': 99}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  75%|#######5  | 15/20 [08:09<02:53, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.223488\tvalid_1's binary_logloss: 0.24737\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.332533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  80%|########  | 16/20 [08:43<02:17, 34.32s/it]\u001b[32m[I 2022-10-03 12:54:28,436]\u001b[0m Trial 22 finished with value: 0.24732900056147172 and parameters: {'num_leaves': 163}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  80%|########  | 16/20 [08:43<02:17, 34.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.21985\tvalid_1's binary_logloss: 0.247329\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.317697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  85%|########5 | 17/20 [09:23<01:48, 36.26s/it]\u001b[32m[I 2022-10-03 12:55:09,211]\u001b[0m Trial 23 finished with value: 0.24186666411186025 and parameters: {'num_leaves': 38}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  85%|########5 | 17/20 [09:24<01:48, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.214835\tvalid_1's binary_logloss: 0.241867\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.317556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  90%|######### | 18/20 [10:02<01:13, 36.96s/it]\u001b[32m[I 2022-10-03 12:55:47,802]\u001b[0m Trial 24 finished with value: 0.2422019584384819 and parameters: {'num_leaves': 28}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  90%|######### | 18/20 [10:02<01:13, 36.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.217482\tvalid_1's binary_logloss: 0.242202\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.318041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283:  95%|#########5| 19/20 [10:36<00:35, 35.94s/it]\u001b[32m[I 2022-10-03 12:56:21,374]\u001b[0m Trial 25 finished with value: 0.24664156761513592 and parameters: {'num_leaves': 84}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283:  95%|#########5| 19/20 [10:36<00:35, 35.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.225595\tvalid_1's binary_logloss: 0.246642\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.293534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.241283: 100%|##########| 20/20 [11:12<00:00, 36.07s/it]\u001b[32m[I 2022-10-03 12:56:57,729]\u001b[0m Trial 26 finished with value: 0.24727459750722317 and parameters: {'num_leaves': 156}. Best is trial 19 with value: 0.24133826875217473.\u001b[0m\n",
      "num_leaves, val_score: 0.241283: 100%|##########| 20/20 [11:12<00:00, 33.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.212771\tvalid_1's binary_logloss: 0.247275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:   0%|                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.433133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  10%|#4            | 1/10 [00:37<05:38, 37.65s/it]\u001b[32m[I 2022-10-03 12:57:35,460]\u001b[0m Trial 27 finished with value: 0.24248947679736968 and parameters: {'bagging_fraction': 0.9305445084664057, 'bagging_freq': 7}. Best is trial 27 with value: 0.24248947679736968.\u001b[0m\n",
      "bagging, val_score: 0.241283:  10%|#4            | 1/10 [00:37<05:38, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.21713\tvalid_1's binary_logloss: 0.242489\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.955796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  20%|##8           | 2/10 [01:05<04:16, 32.12s/it]\u001b[32m[I 2022-10-03 12:58:03,725]\u001b[0m Trial 28 finished with value: 0.24440981352876173 and parameters: {'bagging_fraction': 0.5124597940150701, 'bagging_freq': 5}. Best is trial 27 with value: 0.24248947679736968.\u001b[0m\n",
      "bagging, val_score: 0.241283:  20%|##8           | 2/10 [01:05<04:16, 32.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.221626\tvalid_1's binary_logloss: 0.24441\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.311390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  30%|####2         | 3/10 [01:36<03:39, 31.35s/it]\u001b[32m[I 2022-10-03 12:58:34,162]\u001b[0m Trial 29 finished with value: 0.2425339701215952 and parameters: {'bagging_fraction': 0.6147100648959596, 'bagging_freq': 2}. Best is trial 27 with value: 0.24248947679736968.\u001b[0m\n",
      "bagging, val_score: 0.241283:  30%|####2         | 3/10 [01:36<03:39, 31.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.217969\tvalid_1's binary_logloss: 0.242534\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.332344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  40%|#####6        | 4/10 [02:07<03:08, 31.36s/it]\u001b[32m[I 2022-10-03 12:59:05,524]\u001b[0m Trial 30 finished with value: 0.24181104881479648 and parameters: {'bagging_fraction': 0.8050776410912202, 'bagging_freq': 1}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  40%|#####6        | 4/10 [02:07<03:08, 31.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.21766\tvalid_1's binary_logloss: 0.241811\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.313685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  50%|#######       | 5/10 [02:40<02:38, 31.70s/it]\u001b[32m[I 2022-10-03 12:59:37,837]\u001b[0m Trial 31 finished with value: 0.24309944202135564 and parameters: {'bagging_fraction': 0.7959588770050725, 'bagging_freq': 3}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  50%|#######       | 5/10 [02:40<02:38, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.217196\tvalid_1's binary_logloss: 0.243099\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.290463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  60%|########4     | 6/10 [03:15<02:12, 33.06s/it]\u001b[32m[I 2022-10-03 13:00:13,544]\u001b[0m Trial 32 finished with value: 0.24264657648566604 and parameters: {'bagging_fraction': 0.9570392711182649, 'bagging_freq': 7}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  60%|########4     | 6/10 [03:15<02:12, 33.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.216091\tvalid_1's binary_logloss: 0.242647\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.332605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  70%|#########7    | 7/10 [03:43<01:34, 31.38s/it]\u001b[32m[I 2022-10-03 13:00:41,459]\u001b[0m Trial 33 finished with value: 0.24448993910248276 and parameters: {'bagging_fraction': 0.6854448389466471, 'bagging_freq': 4}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  70%|#########7    | 7/10 [03:43<01:34, 31.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.221806\tvalid_1's binary_logloss: 0.24449\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.313854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  80%|###########2  | 8/10 [04:10<00:59, 30.00s/it]\u001b[32m[I 2022-10-03 13:01:08,505]\u001b[0m Trial 34 finished with value: 0.2435499850019875 and parameters: {'bagging_fraction': 0.6965105239264223, 'bagging_freq': 5}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  80%|###########2  | 8/10 [04:10<00:59, 30.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.222204\tvalid_1's binary_logloss: 0.24355\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.297230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283:  90%|############6 | 9/10 [04:38<00:29, 29.36s/it]\u001b[32m[I 2022-10-03 13:01:36,446]\u001b[0m Trial 35 finished with value: 0.242924171102441 and parameters: {'bagging_fraction': 0.7017341338633492, 'bagging_freq': 2}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283:  90%|############6 | 9/10 [04:38<00:29, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.221169\tvalid_1's binary_logloss: 0.242924\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.241283: 100%|#############| 10/10 [05:10<00:00, 30.01s/it]\u001b[32m[I 2022-10-03 13:02:07,917]\u001b[0m Trial 36 finished with value: 0.2424101875480686 and parameters: {'bagging_fraction': 0.780726680824106, 'bagging_freq': 4}. Best is trial 30 with value: 0.24181104881479648.\u001b[0m\n",
      "bagging, val_score: 0.241283: 100%|#############| 10/10 [05:10<00:00, 31.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.217669\tvalid_1's binary_logloss: 0.24241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.241283:   0%|       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.241136:  17%|1| 1/6 [00:34<02:53, 34.63s/i\u001b[32m[I 2022-10-03 13:02:42,631]\u001b[0m Trial 37 finished with value: 0.24113611997432116 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.24113611997432116.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.241136:  17%|1| 1/6 [00:34<02:53, 34.63s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.217639\tvalid_1's binary_logloss: 0.241136\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.289986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.239095:  33%|3| 2/6 [01:15<02:33, 38.28s/i\u001b[32m[I 2022-10-03 13:03:23,459]\u001b[0m Trial 38 finished with value: 0.2390954559710934 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 38 with value: 0.2390954559710934.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.239095:  33%|3| 2/6 [01:15<02:33, 38.28s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.311048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.239095:  50%|5| 3/6 [01:48<01:47, 35.81s/i\u001b[32m[I 2022-10-03 13:03:56,335]\u001b[0m Trial 39 finished with value: 0.24352484741473737 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 38 with value: 0.2390954559710934.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.239095:  50%|5| 3/6 [01:48<01:47, 35.81s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.218727\tvalid_1's binary_logloss: 0.243525\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.323407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.239095:  67%|6| 4/6 [02:22<01:10, 35.14s/i\u001b[32m[I 2022-10-03 13:04:30,427]\u001b[0m Trial 40 finished with value: 0.2419554984782035 and parameters: {'feature_fraction': 0.88}. Best is trial 38 with value: 0.2390954559710934.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.239095:  67%|6| 4/6 [02:22<01:10, 35.14s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.217714\tvalid_1's binary_logloss: 0.241955\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.306891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.239095:  83%|8| 5/6 [02:56<00:34, 34.85s/i\u001b[32m[I 2022-10-03 13:05:04,781]\u001b[0m Trial 41 finished with value: 0.24131834709190034 and parameters: {'feature_fraction': 0.784}. Best is trial 38 with value: 0.2390954559710934.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.239095:  83%|8| 5/6 [02:56<00:34, 34.85s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.217419\tvalid_1's binary_logloss: 0.241318\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.307774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.239095: 100%|#| 6/6 [03:29<00:00, 34.18s/i\u001b[32m[I 2022-10-03 13:05:37,681]\u001b[0m Trial 42 finished with value: 0.24224959511842706 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 38 with value: 0.2390954559710934.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.239095: 100%|#| 6/6 [03:29<00:00, 34.96s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.218975\tvalid_1's binary_logloss: 0.24225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:   0%|       | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:   5%| | 1/20 [00:41<13:05, 41.34s/i\u001b[32m[I 2022-10-03 13:06:19,108]\u001b[0m Trial 43 finished with value: 0.23909545049569042 and parameters: {'lambda_l1': 6.52964835240036e-06, 'lambda_l2': 1.9828217135113662e-05}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:   5%| | 1/20 [00:41<13:05, 41.34s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.288456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  10%|1| 2/20 [01:13<10:48, 36.02s/i\u001b[32m[I 2022-10-03 13:06:51,417]\u001b[0m Trial 44 finished with value: 0.24177019634117872 and parameters: {'lambda_l1': 0.02814474508928704, 'lambda_l2': 8.021247043904097e-07}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  10%|1| 2/20 [01:13<10:48, 36.02s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.22008\tvalid_1's binary_logloss: 0.24177\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.285164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  15%|1| 3/20 [01:54<10:48, 38.13s/i\u001b[32m[I 2022-10-03 13:07:32,044]\u001b[0m Trial 45 finished with value: 0.23909545579863684 and parameters: {'lambda_l1': 2.3289313763840917e-07, 'lambda_l2': 6.163398307364363e-07}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  15%|1| 3/20 [01:54<10:48, 38.13s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.294511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  20%|2| 4/20 [02:35<10:28, 39.28s/i\u001b[32m[I 2022-10-03 13:08:13,098]\u001b[0m Trial 46 finished with value: 0.2390954517123219 and parameters: {'lambda_l1': 0.0005198820459225064, 'lambda_l2': 3.078931627289681e-08}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  20%|2| 4/20 [02:35<10:28, 39.28s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.307877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  25%|2| 5/20 [03:11<09:33, 38.25s/i\u001b[32m[I 2022-10-03 13:08:49,524]\u001b[0m Trial 47 finished with value: 0.24128467385058366 and parameters: {'lambda_l1': 0.18032992366793837, 'lambda_l2': 3.4573962221045435e-06}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  25%|2| 5/20 [03:11<09:33, 38.25s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.216358\tvalid_1's binary_logloss: 0.241285\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.303503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  30%|3| 6/20 [03:46<08:40, 37.15s/i\u001b[32m[I 2022-10-03 13:09:24,546]\u001b[0m Trial 48 finished with value: 0.2412480700514491 and parameters: {'lambda_l1': 2.5455656815155745e-07, 'lambda_l2': 4.683435656193242}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  30%|3| 6/20 [03:46<08:40, 37.15s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.217443\tvalid_1's binary_logloss: 0.241248\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.283834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  35%|3| 7/20 [04:20<07:50, 36.20s/i\u001b[32m[I 2022-10-03 13:09:58,763]\u001b[0m Trial 49 finished with value: 0.24086283259621402 and parameters: {'lambda_l1': 3.152957003839893e-08, 'lambda_l2': 0.2558413382748469}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  35%|3| 7/20 [04:21<07:50, 36.20s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.217753\tvalid_1's binary_logloss: 0.240863\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.509436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  40%|4| 8/20 [04:53<07:00, 35.07s/i\u001b[32m[I 2022-10-03 13:10:31,408]\u001b[0m Trial 50 finished with value: 0.24177018785314985 and parameters: {'lambda_l1': 0.026624987882609218, 'lambda_l2': 0.0011049894278893993}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  40%|4| 8/20 [04:53<07:00, 35.07s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.22008\tvalid_1's binary_logloss: 0.24177\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.659674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  45%|4| 9/20 [05:28<06:23, 34.90s/i\u001b[32m[I 2022-10-03 13:11:05,946]\u001b[0m Trial 51 finished with value: 0.24116344489806574 and parameters: {'lambda_l1': 0.12677703208955826, 'lambda_l2': 1.9377305889494687e-05}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  45%|4| 9/20 [05:28<06:23, 34.90s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.216319\tvalid_1's binary_logloss: 0.241163\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.302984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  50%|5| 10/20 [06:03<05:51, 35.11s/\u001b[32m[I 2022-10-03 13:11:41,557]\u001b[0m Trial 52 finished with value: 0.24115348691293087 and parameters: {'lambda_l1': 1.2055292343103957e-06, 'lambda_l2': 2.1063351019943535}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  50%|5| 10/20 [06:03<05:51, 35.11s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.217182\tvalid_1's binary_logloss: 0.241153\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.302346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  55%|5| 11/20 [06:45<05:33, 37.07s/\u001b[32m[I 2022-10-03 13:12:23,069]\u001b[0m Trial 53 finished with value: 0.23923227806693287 and parameters: {'lambda_l1': 6.279628869187412e-05, 'lambda_l2': 0.0009223115931631389}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  55%|5| 11/20 [06:45<05:33, 37.07s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.21355\tvalid_1's binary_logloss: 0.239232\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.301272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  60%|6| 12/20 [07:29<05:14, 39.35s/\u001b[32m[I 2022-10-03 13:13:07,606]\u001b[0m Trial 54 finished with value: 0.23909545453037076 and parameters: {'lambda_l1': 0.00017944269686121878, 'lambda_l2': 1.0109048190061546e-08}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  60%|6| 12/20 [07:29<05:14, 39.35s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.310808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  65%|6| 13/20 [08:11<04:40, 40.11s/\u001b[32m[I 2022-10-03 13:13:49,475]\u001b[0m Trial 55 finished with value: 0.23909545580587888 and parameters: {'lambda_l1': 1.9983674486835492e-05, 'lambda_l2': 1.5178227399662568e-08}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  65%|6| 13/20 [08:11<04:40, 40.11s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.302226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  70%|7| 14/20 [08:49<03:56, 39.49s/\u001b[32m[I 2022-10-03 13:14:27,542]\u001b[0m Trial 56 finished with value: 0.23990401915900517 and parameters: {'lambda_l1': 0.0019416778236099207, 'lambda_l2': 0.018174254689947374}. Best is trial 43 with value: 0.23909545049569042.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  70%|7| 14/20 [08:49<03:56, 39.49s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.215768\tvalid_1's binary_logloss: 0.239904\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.294884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  75%|7| 15/20 [09:30<03:19, 39.93s/\u001b[32m[I 2022-10-03 13:15:08,496]\u001b[0m Trial 57 finished with value: 0.2390954383535527 and parameters: {'lambda_l1': 5.936909174721905e-06, 'lambda_l2': 6.43282115793589e-05}. Best is trial 57 with value: 0.2390954383535527.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  75%|7| 15/20 [09:30<03:19, 39.93s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.293982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  80%|8| 16/20 [10:11<02:41, 40.32s/\u001b[32m[I 2022-10-03 13:15:49,717]\u001b[0m Trial 58 finished with value: 0.2390954392271973 and parameters: {'lambda_l1': 3.7383179278020777e-06, 'lambda_l2': 6.118411526967564e-05}. Best is trial 57 with value: 0.2390954383535527.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  80%|8| 16/20 [10:12<02:41, 40.32s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.284124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  85%|8| 17/20 [10:49<01:58, 39.61s/\u001b[32m[I 2022-10-03 13:16:27,684]\u001b[0m Trial 59 finished with value: 0.2407562453715877 and parameters: {'lambda_l1': 2.1912348506591215, 'lambda_l2': 0.00011798361274627662}. Best is trial 57 with value: 0.2390954383535527.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  85%|8| 17/20 [10:49<01:58, 39.61s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.216332\tvalid_1's binary_logloss: 0.240756\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.304796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  90%|9| 18/20 [11:31<01:20, 40.20s/\u001b[32m[I 2022-10-03 13:17:09,230]\u001b[0m Trial 60 finished with value: 0.23929351071101884 and parameters: {'lambda_l1': 1.3610405949149177e-08, 'lambda_l2': 0.013582764481356865}. Best is trial 57 with value: 0.2390954383535527.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  90%|9| 18/20 [11:31<01:20, 40.20s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.213056\tvalid_1's binary_logloss: 0.239294\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.286833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095:  95%|9| 19/20 [12:12<00:40, 40.30s/\u001b[32m[I 2022-10-03 13:17:49,764]\u001b[0m Trial 61 finished with value: 0.23909542187107605 and parameters: {'lambda_l1': 2.9988448043590512e-06, 'lambda_l2': 0.0001247929741397996}. Best is trial 61 with value: 0.23909542187107605.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095:  95%|9| 19/20 [12:12<00:40, 40.30s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213556\tvalid_1's binary_logloss: 0.239095\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.288904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.239095: 100%|#| 20/20 [12:53<00:00, 40.51s/\u001b[32m[I 2022-10-03 13:18:30,797]\u001b[0m Trial 62 finished with value: 0.23934917242487996 and parameters: {'lambda_l1': 4.738993080968035e-07, 'lambda_l2': 0.007210338402319658}. Best is trial 61 with value: 0.23909542187107605.\u001b[0m\n",
      "regularization_factors, val_score: 0.239095: 100%|#| 20/20 [12:53<00:00, 38.66s/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.213521\tvalid_1's binary_logloss: 0.239349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095:   0%|              | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.292973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095:  20%|#2    | 1/5 [00:32<02:09, 32.34s/it]\u001b[32m[I 2022-10-03 13:19:03,223]\u001b[0m Trial 63 finished with value: 0.2412701011721286 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.2412701011721286.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.239095:  20%|#2    | 1/5 [00:32<02:09, 32.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.218751\tvalid_1's binary_logloss: 0.24127\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.296898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095:  40%|##4   | 2/5 [01:04<01:37, 32.43s/it]\u001b[32m[I 2022-10-03 13:19:35,706]\u001b[0m Trial 64 finished with value: 0.24157857471172356 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.2412701011721286.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.239095:  40%|##4   | 2/5 [01:04<01:37, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.219444\tvalid_1's binary_logloss: 0.241579\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.281763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095:  60%|###6  | 3/5 [01:40<01:07, 33.99s/it]\u001b[32m[I 2022-10-03 13:20:11,559]\u001b[0m Trial 65 finished with value: 0.240460081038348 and parameters: {'min_child_samples': 50}. Best is trial 65 with value: 0.240460081038348.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.239095:  60%|###6  | 3/5 [01:40<01:07, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.217305\tvalid_1's binary_logloss: 0.24046\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.302849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095:  80%|####8 | 4/5 [02:16<00:34, 34.64s/it]\u001b[32m[I 2022-10-03 13:20:47,188]\u001b[0m Trial 66 finished with value: 0.24087776350357226 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.240460081038348.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.239095:  80%|####8 | 4/5 [02:16<00:34, 34.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.21776\tvalid_1's binary_logloss: 0.240878\n",
      "[LightGBM] [Info] Number of positive: 33396, number of negative: 443223\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.304398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 89928\n",
      "[LightGBM] [Info] Number of data points in the train set: 476619, number of used features: 622\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.070069 -> initscore=-2.585637\n",
      "[LightGBM] [Info] Start training from score -2.585637\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.239095: 100%|######| 5/5 [02:50<00:00, 34.59s/it]\u001b[32m[I 2022-10-03 13:21:21,694]\u001b[0m Trial 67 finished with value: 0.2412333069216677 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.240460081038348.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.239095: 100%|######| 5/5 [02:50<00:00, 34.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.21852\tvalid_1's binary_logloss: 0.241233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "lgb_train = lgb_o.Dataset(X_train1.values, y_train1.values)\n",
    "lgb_valid = lgb_o.Dataset(X_valid1.values, y_valid1.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304bec92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 2.9988448043590512e-06,\n",
       " 'lambda_l2': 0.0001247929741397996,\n",
       " 'num_leaves': 31,\n",
       " 'feature_fraction': 0.7200000000000001,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6e9b5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7200000000000001, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7200000000000001\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0001247929741397996, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001247929741397996\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.9988448043590512e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.9988448043590512e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=1.0, bagging_freq=0,\n",
       "               feature_fraction=0.7200000000000001, feature_pre_filter=False,\n",
       "               lambda_l1=2.9988448043590512e-06,\n",
       "               lambda_l2=0.0001247929741397996, num_iterations=1000,\n",
       "               objective='binary', random_state=100)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 2.9988448043590512e-06,\n",
    " 'lambda_l2': 0.0001247929741397996,\n",
    " 'num_leaves': 31,\n",
    " 'feature_fraction': 0.7200000000000001,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "lgb_clf1 = lgb.LGBMClassifier(**params)\n",
    "lgb_clf1.fit(X_train1.values, y_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7422d002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7775923786495271"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf1, haitou, std=True)\n",
    "me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f757c035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyterthemes import jtplot\n",
    "\n",
    "y_pred = me_valid.predict_proba(X_valid1).values\n",
    "\n",
    "jtplot.style(theme='monokai')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid1, y_pred)\n",
    "pit.plot(fpr, tpr, marker='o')\n",
    "pit.xlabel(\"False positive rate\")\n",
    "pit.ylabel(\"True positive rate\")\n",
    "pit.grid()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f78d8abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1走前上がり3F速度</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>複勝率偏差</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1走前先行指数</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>上がり3F平均偏差</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>grade</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>獲得賞金合計</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>騎手騎乗回数</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>頭数</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>間隔</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>peds_62</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1走前上がり3F</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>騎乗騎手年間出遅れ率偏差</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>調教師同コース同距離別騎乗回数偏差</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2走前上がり指数</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>peds_2</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1走前コンピ順位</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1走前馬場指数</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>調教師距離別騎乗回数偏差</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>先行率偏差</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>逃げ率偏差</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>調教師コース別騎乗回数偏差</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>調教師出走回数</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trainer_id</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>調教師年齢別複勝率偏差</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>peds_14</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>騎手コース別連対率偏差</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>追込率偏差</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>コースタイプ複勝率偏差</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>producer</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>調教師年齢別年間複勝率偏差</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1走前ペース指数</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>騎手全体複勝率偏差</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>騎手競馬場別勝率偏差</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>騎手距離別騎乗回数偏差</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>調教師年齢別勝率偏差</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>連対率偏差</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>騎手同コース同距離別勝率偏差</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>生涯出遅れ率偏差</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>peds_59</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>勝率偏差</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>騎手同コース同距離別連対率偏差</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>peds_30</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>騎手競馬場別複勝率偏差</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>騎手コース別複勝率偏差</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>同距離同クラス複勝率偏差</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>furlong_2_1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>種牡馬同コース同距離別複勝率偏差</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>調教師競馬場別騎乗回数偏差</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>騎手同コース同距離別複勝率偏差</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>父系統同コース同距離別勝率偏差</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>調教師コース別複勝率偏差</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>peds_26</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>調教師年齢別年間連対率偏差</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>peds_6</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>peds_61</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>peds_58</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>調教師距離別勝率偏差</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>owner</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>peds_60</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "30          1走前上がり3F速度         214\n",
       "478              複勝率偏差         206\n",
       "36             1走前先行指数         193\n",
       "461          上がり3F平均偏差         183\n",
       "355              grade         178\n",
       "361             獲得賞金合計         165\n",
       "345             騎手騎乗回数         155\n",
       "354                 頭数         152\n",
       "12                  間隔         149\n",
       "566            peds_62         149\n",
       "29            1走前上がり3F         146\n",
       "487       騎乗騎手年間出遅れ率偏差         145\n",
       "398  調教師同コース同距離別騎乗回数偏差         144\n",
       "100           2走前上がり指数         143\n",
       "506             peds_2         140\n",
       "20            1走前コンピ順位         137\n",
       "39             1走前馬場指数         137\n",
       "394       調教師距離別騎乗回数偏差         137\n",
       "457              先行率偏差         135\n",
       "456              逃げ率偏差         135\n",
       "390      調教師コース別騎乗回数偏差         134\n",
       "346            調教師出走回数         134\n",
       "6           trainer_id         134\n",
       "405        調教師年齢別複勝率偏差         130\n",
       "518            peds_14         128\n",
       "373        騎手コース別連対率偏差         127\n",
       "459              追込率偏差         127\n",
       "483        コースタイプ複勝率偏差         127\n",
       "14            producer         127\n",
       "3            jockey_id         126\n",
       "402      調教師年齢別年間複勝率偏差         124\n",
       "37            1走前ペース指数         124\n",
       "366          騎手全体複勝率偏差         123\n",
       "368         騎手競馬場別勝率偏差         123\n",
       "375        騎手距離別騎乗回数偏差         123\n",
       "403         調教師年齢別勝率偏差         122\n",
       "470              連対率偏差         121\n",
       "380     騎手同コース同距離別勝率偏差         120\n",
       "486           生涯出遅れ率偏差         120\n",
       "563            peds_59         119\n",
       "462               勝率偏差         119\n",
       "381    騎手同コース同距離別連対率偏差         118\n",
       "534            peds_30         118\n",
       "370        騎手競馬場別複勝率偏差         117\n",
       "374        騎手コース別複勝率偏差         117\n",
       "484       同距離同クラス複勝率偏差         117\n",
       "338        furlong_2_1         117\n",
       "426   種牡馬同コース同距離別複勝率偏差         116\n",
       "386      調教師競馬場別騎乗回数偏差         115\n",
       "382    騎手同コース同距離別複勝率偏差         115\n",
       "450    父系統同コース同距離別勝率偏差         115\n",
       "393       調教師コース別複勝率偏差         114\n",
       "530            peds_26         114\n",
       "401      調教師年齢別年間連対率偏差         113\n",
       "510             peds_6         113\n",
       "565            peds_61         113\n",
       "562            peds_58         112\n",
       "395         調教師距離別勝率偏差         112\n",
       "15               owner         111\n",
       "564            peds_60         111"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid.feature_importance(X_train1, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1a9cbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = me_valid.pred_table(X_valid1, 0.3, True)\n",
    "wr['d_odds'] = 0.8 / wr['win_ratio']\n",
    "wr['d_odds'] = wr['d_odds'].map(lambda x: format(x, '.1f'))\n",
    "wr['d_odds'] = wr['d_odds'].astype(float)\n",
    "wr['expected'] = wr['time_odds'] / wr['d_odds']\n",
    "wr['pred_rank'] = wr[['win_ratio']].groupby(level=0).rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f5fa5717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shisuu = pd.read_csv('./shisuu_new.csv')\n",
    "v = valid1.reset_index()[['race_id', 'h_num', 'id']]\n",
    "v['horse_race_id'] = v['id']\n",
    "\n",
    "sv = v.drop(['id'], axis=1).merge(shisuu, on='horse_race_id')\n",
    "sv = sv.merge(wr, on=['race_id', 'h_num'])\n",
    "\n",
    "race_grade = pd.read_csv('./csv_new2/races.csv')\n",
    "race_grade = race_grade.set_index('race_id')\n",
    "race_grade[['grade']]\n",
    "\n",
    "bets = sv.merge(race_grade, on='race_id')\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/weathers.csv')\n",
    "bets = bets.merge(weather[['race_id', 'place_id']], on='race_id')\n",
    "\n",
    "l_bt = bets[\n",
    "#     (bets['pred_rank'] <= 5)\n",
    "#     &\n",
    "    (bets['expected'] >= 1)\n",
    "    &\n",
    "    (bets['score'] > 5)\n",
    "    \n",
    "]\n",
    "# l_bt = bets\n",
    "\n",
    "# l_bt = bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2fe6ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：5963 的中数:497 的中率:8.33% 賭金:596,300円 配当合計:599,520円 最高配当:9,560円 回収率:100.54%\n"
     ]
    }
   ],
   "source": [
    "l_bh = l_bt.merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.2%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.2%}\". format(\n",
    "    len(l_bt),\\\n",
    "    len(l_bh[l_bh['h_num'] == l_bh['1着馬番']]),\\\n",
    "    len(l_bh[l_bh['h_num'] == l_bh['1着馬番']]) / (len(l_bt)),\\\n",
    "    len(l_bt) * 100,\\\n",
    "    l_bh[l_bh['h_num'] == l_bh['1着馬番']]['単勝'].sum(),\\\n",
    "    l_bh[l_bh['h_num'] == l_bh['1着馬番']]['単勝'].max(),\\\n",
    "    (l_bh[l_bh['h_num'] == l_bh['1着馬番']]['単勝'].sum() / (len(l_bt) * 100))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1878c9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：5963 的中数:1635 的中率:27.42% 賭金:596,300円 配当合計:522,580円 回収率:87.64%\n"
     ]
    }
   ],
   "source": [
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(l_bh[l_bh['h_num'] == l_bh[s + '着馬番']]['複勝' + s])\n",
    "    money += l_bh[l_bh['h_num'] == l_bh[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.2%} 賭金:{:,}円 配当合計:{:,}円 回収率:{:.2%}\". format(\n",
    "    len(l_bh),\\\n",
    "    f_c,\\\n",
    "    f_c / len(l_bh),\\\n",
    "     (len(l_bh) * 100),\\\n",
    "    int(money),\\\n",
    "    (money / (len(l_bh) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c25f72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_1 = pd.concat([\n",
    "    bt1,\n",
    "#     t_bt[['race_id', 'h_num', 'pred_time', 'pred_rank', 'popular', 'score']],\n",
    "    l_bt[['race_id', 'pred_rank', 'h_num', 'time_odds', 'score', 'expected', 'win_ratio', 'proba']]\n",
    "]).merge(haitou, on='race_id')\n",
    "\n",
    "\n",
    "# btt_2 = btt_1[btt_1.duplicated(subset=['race_id', 'h_num'], keep='last')]\n",
    "# btt_1 = btt_1.drop_duplicates(subset=['race_id', 'h_num'], keep='last')\n",
    "# btt_1['pred_rank'] = btt_1[['race_id', 'proba']].groupby('race_id').rank(ascending=False)\n",
    "# btt_1 = btt_concat.merge(sva[['race_id', 'h_num', 'pred_time', 'pred_rank']], on=['race_id', 'h_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "708d94ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>pred</th>\n",
       "      <th>proba</th>\n",
       "      <th>expected</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>popular</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>4着馬番</th>\n",
       "      <th>単勝</th>\n",
       "      <th>複勝1</th>\n",
       "      <th>複勝2</th>\n",
       "      <th>複勝3</th>\n",
       "      <th>複勝4</th>\n",
       "      <th>馬単</th>\n",
       "      <th>馬連</th>\n",
       "      <th>3連複</th>\n",
       "      <th>3連単</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851311</td>\n",
       "      <td>1.232079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>590</td>\n",
       "      <td>230.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3870</td>\n",
       "      <td>2100</td>\n",
       "      <td>4810</td>\n",
       "      <td>25730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018041509020810</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>1.576923</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076931</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>590</td>\n",
       "      <td>230.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3870</td>\n",
       "      <td>2100</td>\n",
       "      <td>4810</td>\n",
       "      <td>25730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018041503010412</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.566258</td>\n",
       "      <td>0.486076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1200</td>\n",
       "      <td>280.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4420</td>\n",
       "      <td>1790</td>\n",
       "      <td>2290</td>\n",
       "      <td>18660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018041503010412</td>\n",
       "      <td>2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584404</td>\n",
       "      <td>0.906133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.210729</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1200</td>\n",
       "      <td>280.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4420</td>\n",
       "      <td>1790</td>\n",
       "      <td>2290</td>\n",
       "      <td>18660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.931843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258845</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1660</td>\n",
       "      <td>460.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12260</td>\n",
       "      <td>5000</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31654</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.439345</td>\n",
       "      <td>3.758621</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092330</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310</td>\n",
       "      <td>140.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1780</td>\n",
       "      <td>1200</td>\n",
       "      <td>3070</td>\n",
       "      <td>11510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31655</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315327</td>\n",
       "      <td>1.347107</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066267</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310</td>\n",
       "      <td>140.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1780</td>\n",
       "      <td>1200</td>\n",
       "      <td>3070</td>\n",
       "      <td>11510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31656</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>16</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680255</td>\n",
       "      <td>1.026541</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.263216</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>120.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3790</td>\n",
       "      <td>2510</td>\n",
       "      <td>2770</td>\n",
       "      <td>15170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31657</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648918</td>\n",
       "      <td>0.605434</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242174</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>120.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3790</td>\n",
       "      <td>2510</td>\n",
       "      <td>2770</td>\n",
       "      <td>15170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31658</th>\n",
       "      <td>2021042404010505</td>\n",
       "      <td>12</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702749</td>\n",
       "      <td>1.252437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.278319</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>120.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3790</td>\n",
       "      <td>2510</td>\n",
       "      <td>2770</td>\n",
       "      <td>15170.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31659 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                race_id  h_num  odds  time_odds  pred     proba  expected  \\\n",
       "0      2018041509020810      8   4.5        4.2   1.0  0.851311  1.232079   \n",
       "1      2018041509020810      4   NaN       16.4   NaN  0.339922  1.576923   \n",
       "2      2018041503010412      4   2.5        2.4   1.0  0.566258  0.486076   \n",
       "3      2018041503010412      2   5.2        4.3   1.0  0.584404  0.906133   \n",
       "4      2018041506030807      2   4.0        3.6   1.0  0.784431  0.931843   \n",
       "...                 ...    ...   ...        ...   ...       ...       ...   \n",
       "31654  2021042405020111      3   NaN       32.7   NaN  0.439345  3.758621   \n",
       "31655  2021042405020111     14   NaN       16.3   NaN  0.315327  1.347107   \n",
       "31656  2021042404010505     16   3.8        3.9   1.0  0.680255  1.026541   \n",
       "31657  2021042404010505      9   2.5        2.5   1.0  0.648918  0.605434   \n",
       "31658  2021042404010505     12   4.4        4.5   1.0  0.702749  1.252437   \n",
       "\n",
       "       pred_rank  popular  win_ratio  ...  4着馬番    単勝    複勝1    複勝2    複勝3  \\\n",
       "0            1.0      1.0   0.293352  ...   NaN   590  230.0  200.0  260.0   \n",
       "1            6.0      NaN   0.076931  ...   NaN   590  230.0  200.0  260.0   \n",
       "2            2.0      1.0   0.202532  ...   NaN  1200  280.0  120.0  170.0   \n",
       "3            1.0      3.0   0.210729  ...   NaN  1200  280.0  120.0  170.0   \n",
       "4            1.0      1.0   0.258845  ...   NaN  1660  460.0  260.0  180.0   \n",
       "...          ...      ...        ...  ...   ...   ...    ...    ...    ...   \n",
       "31654        2.0      NaN   0.092330  ...   NaN   310  140.0  210.0  260.0   \n",
       "31655        8.0      NaN   0.066267  ...   NaN   310  140.0  210.0  260.0   \n",
       "31656        2.0      2.0   0.263216  ...   NaN   250  120.0  320.0  150.0   \n",
       "31657        3.0      1.0   0.242174  ...   NaN   250  120.0  320.0  150.0   \n",
       "31658        1.0      3.0   0.278319  ...   NaN   250  120.0  320.0  150.0   \n",
       "\n",
       "       複勝4     馬単    馬連    3連複      3連単  \n",
       "0      NaN   3870  2100   4810  25730.0  \n",
       "1      NaN   3870  2100   4810  25730.0  \n",
       "2      NaN   4420  1790   2290  18660.0  \n",
       "3      NaN   4420  1790   2290  18660.0  \n",
       "4      NaN  12260  5000  10910  90850.0  \n",
       "...    ...    ...   ...    ...      ...  \n",
       "31654  NaN   1780  1200   3070  11510.0  \n",
       "31655  NaN   1780  1200   3070  11510.0  \n",
       "31656  NaN   3790  2510   2770  15170.0  \n",
       "31657  NaN   3790  2510   2770  15170.0  \n",
       "31658  NaN   3790  2510   2770  15170.0  \n",
       "\n",
       "[31659 rows x 27 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btt_2 = btt_1\n",
    "btt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "93337cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：3803 レース出現率:25.9% レース数:2601 的中数:369 的中率:9.7% 賭金:380,300円 配当合計:361,710円 最高配当:7,930円 回収率:95.1%\n"
     ]
    }
   ],
   "source": [
    "btt = btt_2[\n",
    "    (btt_2['score'] >= 10)\n",
    "    &\n",
    "    (btt_2['expected'] >= 1)\n",
    "]\n",
    "# btt = btt_2\n",
    "\n",
    "print(\"点数：{} レース出現率:{:.1%} レース数:{} 的中数:{} 的中率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(btt),\\\n",
    "    len(btt.groupby('race_id')) / len(X_valid1.groupby('race_id')),\\\n",
    "    len(btt.groupby('race_id')),\\\n",
    "    len(btt[btt['h_num'] == btt['1着馬番']]),\\\n",
    "    len(btt[btt['h_num'] == btt['1着馬番']]) / (len(btt)), \\\n",
    "    len(btt) * 100,\\\n",
    "    btt[btt['h_num'] == btt['1着馬番']]['単勝'].sum(),\\\n",
    "    btt[btt['h_num'] == btt['1着馬番']]['単勝'].max(), \\\n",
    "    (btt[btt['h_num'] == btt['1着馬番']]['単勝'].sum() / (len(btt) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "12a4404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：3803 的中数:1127 的中率:29.6% 賭金:380,300円 配当合計:311,920円 回収率:82.0%\n"
     ]
    }
   ],
   "source": [
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(btt[btt['h_num'] == btt[s + '着馬番']]['複勝' + s])\n",
    "    money += btt[btt['h_num'] == btt[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(btt),\\\n",
    "    f_c,\\\n",
    "    f_c / len(btt),\\\n",
    "     (len(btt) * 100),\\\n",
    "    int(money),\\\n",
    "    (money / (len(btt) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "33d90895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：53 賭金:5,300円 配当合計:9,690円 最高配当:6,380円 的中率:3.8% 回収率:182.8%\n"
     ]
    }
   ],
   "source": [
    "aite = btt_1[\n",
    "    (btt_1['expected'] >= 1)\n",
    "#     &\n",
    "#     (btt_1['pred_rank'] <= 1)\n",
    "#     &\n",
    "#     (btt_1['score'] >= 10)\n",
    "]\n",
    "\n",
    "b = btt.merge(aite[['race_id', 'h_num', 'score']], on='race_id')\n",
    "\n",
    "uma_haito = b[\n",
    "    (b['score_x'] + b['score_y']) > 35\n",
    "]\n",
    "\n",
    "ut_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"点数：{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    len(uma_haito),\\\n",
    "    len(uma_haito) * 100,\\\n",
    "    ut_bt['馬連'].sum(),\\\n",
    "    ut_bt['馬連'].max(),\\\n",
    "    len(ut_bt) / len(uma_haito),\\\n",
    "    (ut_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
