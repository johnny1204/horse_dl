{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c395f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "import lightgbm as lgb\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def build_data(base_name, new_name, result, vec):\n",
    "    allrace = pd.read_pickle(base_name)\n",
    "    df = allrace.reset_index()\n",
    "\n",
    "    all_r = preprocessing(df)\n",
    "    all_r['result'] = all_r['result'].map(lambda x: 1 if x < result else 0)\n",
    "\n",
    "    categorical = process_categorical(all_r, [\n",
    "        'producer', 'owner', 'training_course', 'course',\n",
    "        'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "        '天候', '馬場状態', 'place_id',\n",
    "        'color_id', 'stallion_id', 'affiliation_id',\n",
    "    ])\n",
    "\n",
    "    categorical = categorical.reset_index()\n",
    "    categorical = categorical.merge(vec, on='horse_id')\n",
    "    categorical = categorical.set_index('race_id')\n",
    "\n",
    "    train1, valid1 = split_data(categorical)\n",
    "    valid1, test2 = train_valid_split_data(valid1)\n",
    "\n",
    "    target = pd.read_pickle('./pickle_new/new_race_20221127_shinba.pickle')\n",
    "    target = target[target['date'].notnull()]\n",
    "    target = preprocessing(target)\n",
    "    target['result'] = target['result'].map(lambda x: 1 if x < result else 0)\n",
    "\n",
    "    test1 = process_categorical(target,  [\n",
    "        'producer', 'owner', 'training_course', 'course',\n",
    "        'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "        '天候', '馬場状態', 'place_id',\n",
    "        'color_id', 'stallion_id', 'affiliation_id',\n",
    "    ])\n",
    "\n",
    "    test1 = test1.reset_index()\n",
    "    test1 = test1.merge(vec, on='horse_id')\n",
    "    test1 = test1.set_index('race_id')\n",
    "\n",
    "    X_train1 = train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_train1 = train1['result']\n",
    "    X_valid1 = valid1.drop(['date', 'result', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_valid1 = valid1['result']\n",
    "    X_test2 = test2.drop(['date', 'result', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_test2 = test2['result']\n",
    "    X_test1 = test1.drop(['date', 'result', 'popular', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速', '同周り複勝率', '同周り勝率', '同周り連対率'], axis=1)\n",
    "    y_test1 = test1['result']\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train1,\n",
    "        'y_train': y_train1,\n",
    "        'X_valid': X_valid1,\n",
    "        'y_valid': y_valid1,\n",
    "        'X_test2': X_test2,\n",
    "        'y_test2': y_test2,\n",
    "        'X_test1': X_test1,\n",
    "        'y_test1': y_test1,\n",
    "    }\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "    df.drop(['rank', 'body_weight',  'grade'], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    # df2 = pd.get_dummies(df2, sparse=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "def optuna_params(X_train, y_train, X_valid, y_valid):\n",
    "    lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n",
    "    lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'random_state': 100\n",
    "    }\n",
    "\n",
    "    lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)\n",
    "    \n",
    "    return lgb_clf_o.params\n",
    "\n",
    "def fit(params, X, y):\n",
    "    lgb_clf = lgb.LGBMClassifier(**params)\n",
    "    lgb_clf.fit(X.values, y.values)\n",
    "    \n",
    "    return lgb_clf\n",
    "\n",
    "def tansho_return_rate(X, wr):\n",
    "    wr['expected'] = wr['win_ratio'] * wr['time_odds'] \n",
    "    wr['pred_rank'] = wr[['win_ratio']].groupby('race_id').rank(ascending=False)\n",
    "    \n",
    "    race_grade = pd.read_csv('./csv_new2/races.csv')\n",
    "    race_grade = race_grade.set_index('race_id')\n",
    "    race_grade[['grade']]\n",
    "\n",
    "    bets = wr.merge(race_grade, on='race_id')\n",
    "\n",
    "    weather = pd.read_csv('./csv_new2/weathers.csv')\n",
    "    bets = bets.merge(weather[['race_id', 'place_id']], on='race_id')\n",
    "\n",
    "    # 賭ける馬\n",
    "    v_bt = bets[\n",
    "    #     (bets['pred_rank'] == 1)\n",
    "    #     &\n",
    "        (bets['expected'] >= 1)\n",
    "    ]\n",
    "\n",
    "    kaime = v_bt.merge(haitou[['1着馬番', '単勝']], on='race_id')\n",
    "    \n",
    "    print(\"点数：{} レース数:{} 対象レース率:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "        len(kaime),\\\n",
    "        len(X.groupby('race_id')),\\\n",
    "        len(kaime.groupby('race_id')) / len(X.groupby('race_id')),\\\n",
    "        len(kaime[kaime['h_num'] == kaime['1着馬番']]) / (len(kaime)),\\\n",
    "        len(kaime[kaime['h_num'] == kaime['1着馬番']]),\\\n",
    "        len(kaime) * 100,\\\n",
    "        kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum(),\\\n",
    "        kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].max(),\\\n",
    "        (kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum() / (len(kaime) * 100))\n",
    "    ))\n",
    "    \n",
    "def umaren_return_rate(fmrt, haitou):\n",
    "    uma_haito = fmrt.merge(haitou[['1着馬番', '2着馬番', '馬連', '馬単']], left_index=True, right_index=True)\n",
    "\n",
    "    f_bt = uma_haito[\n",
    "        (\n",
    "            (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "            &\n",
    "            (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "        )\n",
    "        |\n",
    "        (\n",
    "            (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "            &\n",
    "            (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "        len(uma_haito),\\\n",
    "        len(f_bt),\\\n",
    "        len(f_bt) / len(uma_haito)\n",
    "    ))\n",
    "    \n",
    "    print(\"馬連賭金:{:,}円 馬連配当合計:{:,}円 馬連最高配当:{:,}円 馬連回収率:{:.1%}\". format(\n",
    "        (len(uma_haito) * 100),\\\n",
    "        f_bt['馬連'].sum(),\\\n",
    "        f_bt['馬連'].max(),\\\n",
    "        (f_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "    ))\n",
    "    \n",
    "    print(\"馬単賭金:{:,}円 馬単配当合計:{:,}円 馬単最高配当:{:,}円 馬単回収率:{:.1%}\". format(\n",
    "        (len(uma_haito) * 200),\\\n",
    "        f_bt['馬単'].sum(),\\\n",
    "        f_bt['馬単'].max(),\\\n",
    "        (f_bt['馬単'].sum() / (len(uma_haito) * 200))\n",
    "    ))\n",
    "    \n",
    "def sanrenkei(kaime):\n",
    "    f_b3 = kaime[\n",
    "        ((kaime['h_num_x'] == kaime['1着馬番']) & (kaime['h_num_y'] == kaime['2着馬番']) & (kaime['h_num'] == kaime['3着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['1着馬番']) & (kaime['h_num_y'] == kaime['3着馬番']) & (kaime['h_num'] == kaime['2着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['2着馬番']) & (kaime['h_num_y'] == kaime['1着馬番']) & (kaime['h_num'] == kaime['3着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['2着馬番']) & (kaime['h_num_y'] == kaime['3着馬番']) & (kaime['h_num'] == kaime['1着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['3着馬番']) & (kaime['h_num_y'] == kaime['1着馬番']) & (kaime['h_num'] == kaime['2着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['3着馬番']) & (kaime['h_num_y'] == kaime['2着馬番']) & (kaime['h_num'] == kaime['1着馬番']))\n",
    "    ]\n",
    "    \n",
    "    print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "        len(kaime),\\\n",
    "        len(f_b3),\\\n",
    "        len(f_b3) / len(kaime)\n",
    "    ))\n",
    "\n",
    "    print(\"3連複賭金:{:,}円 3連複配当合計:{:,}円 3連複最高配当:{:,}円 3連複回収率:{:.1%}\". format(\n",
    "        len(kaime) * 100,\\\n",
    "        f_b3['3連複'].sum(),\\\n",
    "        f_b3['3連複'].max(),\\\n",
    "         (f_b3['3連複'].sum() / (len(kaime) * 100))\n",
    "    ))\n",
    "    \n",
    "    print(\"3連単賭金:{:,}円 3連単配当合計:{:,}円 3連単最高配当:{:,}円 3連単回収率:{:.1%}\". format(\n",
    "        len(kaime) * 600,\\\n",
    "        int(f_b3['3連単'].sum()),\\\n",
    "        int(f_b3['3連単'].max()),\\\n",
    "        (f_b3['3連単'].sum() / (len(kaime) * 600))\n",
    "    ))\n",
    "    \n",
    "def wide_rate(kaime):\n",
    "    s = kaime.reset_index()\n",
    "    s = s[['race_id', 'h_num_x', 'h_num_y', 'h_num']]\n",
    "\n",
    "    a = s[['race_id', 'h_num_x', 'h_num_y']]\n",
    "\n",
    "    b = s[['race_id', 'h_num_y', 'h_num']]\n",
    "    b['h_num_x'] = b['h_num_y']\n",
    "    b['h_num_y'] = b['h_num']\n",
    "\n",
    "    c = s[['race_id', 'h_num_x', 'h_num']]\n",
    "    c['h_num_y'] = c['h_num']\n",
    "\n",
    "    w = pd.concat([\n",
    "        a[['race_id', 'h_num_x', 'h_num_y']],\n",
    "        b[['race_id', 'h_num_x', 'h_num_y']],\n",
    "        c[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    ]).drop_duplicates()\n",
    "    wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "    w = w.merge(wide, on='race_id')\n",
    "    \n",
    "    wide = 0\n",
    "    wide_tekichu = 0\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        sm = str(i)\n",
    "        w_b3 = w[\n",
    "            (\n",
    "                ((w['h_num_x'] == w['wide' + sm + '_uma1']) | (w['h_num_x'] == w['wide' + sm + '_uma2']))\n",
    "                &\n",
    "                ((w['h_num_y'] == w['wide' + sm + '_uma1']) | (w['h_num_y'] == w['wide' + sm + '_uma2']))\n",
    "            )\n",
    "        ]\n",
    "        wide = wide + w_b3['wide_' + sm].sum()\n",
    "        wide_tekichu = wide_tekichu + len(w_b3)\n",
    "\n",
    "    print(\"ワイド点数：{} ワイド賭金:{:,}円 ワイド配当合計:{:,}円 ワイド的中数:{} ワイド的中率:{:.1%} ワイド回収率:{:.1%}\". format(\n",
    "        len(w),\\\n",
    "        len(w) * 100,\\\n",
    "        int(wide),\\\n",
    "        wide_tekichu,\\\n",
    "        wide_tekichu / len(w),\\\n",
    "        (wide / (len(w) * 100))\n",
    "    ))\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds', 'time_odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def proba(self, X):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [p for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds', 'time_odds']]\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        pred_table['proba'] = self.proba(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds', 'win_ratio', 'proba']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table[['h_num', 'odds', 'time_odds', 'win_ratio', 'proba']]\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "        std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "        .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71c78b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "haitou = pd.read_csv('./csv_new2/race_detail.csv')\n",
    "wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "\n",
    "haitou = haitou.merge(wide, on='race_id')\n",
    "haitou = haitou.set_index('race_id')\n",
    "\n",
    "shisuu = pd.read_csv('./shisuu_new.csv')\n",
    "\n",
    "vec = pd.read_pickle('./pickle_new/peds_vec.pickle')\n",
    "# vec = vec[[\n",
    "#     'horse_id', \"peds_2\",\"peds_3\",\"peds_4\",\"peds_5\",\n",
    "#     \"peds_6\",\"peds_7\",\"peds_8\",\"peds_9\",\"peds_10\",\n",
    "#     \"peds_11\",\"peds_12\",\"peds_13\",\"peds_14\"\n",
    "# ]]\n",
    "vec.drop(['peds_1'], axis=1, inplace=True)\n",
    "\n",
    "places = { 1: \"札幌\", 2: \"函館\", 3: \"福島\", 4: \"新潟\", 5: \"東京\", 6: \"中山\", 7: \"中京\", 8: \"京都\", 9: \"阪神\", 10: \"小倉\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e296943f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = build_data('./pickle_new/shinba_base.pickle', './pickle_new/peds_vec.pickle', 2, vec)\n",
    "X_train1 = d['X_train']\n",
    "y_train1 = d['y_train']\n",
    "X_valid1 = d['X_valid']\n",
    "y_valid1 = d['y_valid']\n",
    "X_test2 = d['X_test2']\n",
    "y_test2 = d['y_test2']\n",
    "X_test1 = d['X_test1']\n",
    "y_test1 = d['y_test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "04c350c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 09:13:36,521]\u001b[0m A new study created in memory with name: no-name-c898289a-7679-4cd8-b965-9cc92af273dd\u001b[0m\n",
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.250120:   0%|                                                                       | 0/7 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.250120:  14%|#########                                                      | 1/7 [00:01<00:07,  1.30s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:37,827]\u001b[0m Trial 0 finished with value: 0.25011965619103793 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.25011965619103793.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.250120:  14%|#########                                                      | 1/7 [00:01<00:07,  1.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.221156\tvalid_1's binary_logloss: 0.25012\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  14%|#########                                                      | 1/7 [00:02<00:07,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  29%|##################                                             | 2/7 [00:02<00:06,  1.22s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:38,985]\u001b[0m Trial 1 finished with value: 0.24826662790468854 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 0.24826662790468854.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  29%|##################                                             | 2/7 [00:02<00:06,  1.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.224354\tvalid_1's binary_logloss: 0.248267\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  29%|##################                                             | 2/7 [00:03<00:06,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  43%|###########################                                    | 3/7 [00:03<00:04,  1.17s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:40,104]\u001b[0m Trial 2 finished with value: 0.24906493922583156 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.24826662790468854.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.248267:  43%|###########################                                    | 3/7 [00:03<00:04,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.214002\tvalid_1's binary_logloss: 0.249065\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  43%|###########################                                    | 3/7 [00:04<00:04,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  57%|####################################                           | 4/7 [00:04<00:03,  1.16s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:41,259]\u001b[0m Trial 3 finished with value: 0.24700070705646254 and parameters: {'feature_fraction': 0.8}. Best is trial 3 with value: 0.24700070705646254.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  57%|####################################                           | 4/7 [00:04<00:03,  1.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.22209\tvalid_1's binary_logloss: 0.247001\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  57%|####################################                           | 4/7 [00:05<00:03,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  71%|#############################################                  | 5/7 [00:05<00:02,  1.18s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:42,466]\u001b[0m Trial 4 finished with value: 0.2496542810553598 and parameters: {'feature_fraction': 0.7}. Best is trial 3 with value: 0.24700070705646254.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  71%|#############################################                  | 5/7 [00:05<00:02,  1.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.217519\tvalid_1's binary_logloss: 0.249654\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  71%|#############################################                  | 5/7 [00:06<00:02,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  86%|######################################################         | 6/7 [00:06<00:01,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:43,388]\u001b[0m Trial 5 finished with value: 0.24947249028837692 and parameters: {'feature_fraction': 0.4}. Best is trial 3 with value: 0.24700070705646254.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  86%|######################################################         | 6/7 [00:06<00:01,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.232717\tvalid_1's binary_logloss: 0.249472\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.247001:  86%|######################################################         | 6/7 [00:07<00:01,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.247001: 100%|###############################################################| 7/7 [00:07<00:00,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:44,462]\u001b[0m Trial 6 finished with value: 0.25429794701771424 and parameters: {'feature_fraction': 1.0}. Best is trial 3 with value: 0.24700070705646254.\u001b[0m\n",
      "feature_fraction, val_score: 0.247001: 100%|###############################################################| 7/7 [00:07<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.232909\tvalid_1's binary_logloss: 0.254298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:   0%|                                                                            | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:   0%|                                                                            | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:   5%|###4                                                                | 1/20 [00:01<00:25,  1.32s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:45,792]\u001b[0m Trial 7 finished with value: 0.25329397590693836 and parameters: {'num_leaves': 52}. Best is trial 7 with value: 0.25329397590693836.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:   5%|###4                                                                | 1/20 [00:01<00:25,  1.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.204637\tvalid_1's binary_logloss: 0.253294\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:   5%|###4                                                                | 1/20 [00:02<00:25,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  10%|######8                                                             | 2/20 [00:02<00:26,  1.45s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:47,327]\u001b[0m Trial 8 finished with value: 0.2517310408807687 and parameters: {'num_leaves': 139}. Best is trial 8 with value: 0.2517310408807687.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  10%|######8                                                             | 2/20 [00:02<00:26,  1.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.192779\tvalid_1's binary_logloss: 0.251731\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  10%|######8                                                             | 2/20 [00:04<00:26,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  15%|##########2                                                         | 3/20 [00:04<00:24,  1.47s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:48,813]\u001b[0m Trial 9 finished with value: 0.2541258604856643 and parameters: {'num_leaves': 100}. Best is trial 8 with value: 0.2517310408807687.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  15%|##########2                                                         | 3/20 [00:04<00:24,  1.47s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.192599\tvalid_1's binary_logloss: 0.254126\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  15%|##########2                                                         | 3/20 [00:05<00:24,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  20%|#############6                                                      | 4/20 [00:05<00:21,  1.31s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:49,896]\u001b[0m Trial 10 finished with value: 0.24800320804002793 and parameters: {'num_leaves': 17}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  20%|#############6                                                      | 4/20 [00:05<00:21,  1.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.229687\tvalid_1's binary_logloss: 0.248003\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  20%|#############6                                                      | 4/20 [00:07<00:21,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  25%|#################                                                   | 5/20 [00:07<00:22,  1.48s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:51,673]\u001b[0m Trial 11 finished with value: 0.2509398706164269 and parameters: {'num_leaves': 190}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  25%|#################                                                   | 5/20 [00:07<00:22,  1.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.186051\tvalid_1's binary_logloss: 0.25094\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  25%|#################                                                   | 5/20 [00:08<00:22,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  30%|####################4                                               | 6/20 [00:08<00:20,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:53,107]\u001b[0m Trial 12 finished with value: 0.2529295150581427 and parameters: {'num_leaves': 113}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  30%|####################4                                               | 6/20 [00:08<00:20,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.210209\tvalid_1's binary_logloss: 0.25293\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  30%|####################4                                               | 6/20 [00:10<00:20,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  35%|#######################7                                            | 7/20 [00:10<00:20,  1.61s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:55,019]\u001b[0m Trial 13 finished with value: 0.25264838893532565 and parameters: {'num_leaves': 146}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  35%|#######################7                                            | 7/20 [00:10<00:20,  1.61s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.159882\tvalid_1's binary_logloss: 0.252648\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  35%|#######################7                                            | 7/20 [00:12<00:20,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  40%|###########################2                                        | 8/20 [00:12<00:19,  1.64s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:56,718]\u001b[0m Trial 14 finished with value: 0.2519433630270067 and parameters: {'num_leaves': 180}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  40%|###########################2                                        | 8/20 [00:12<00:19,  1.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.182705\tvalid_1's binary_logloss: 0.251943\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  40%|###########################2                                        | 8/20 [00:13<00:19,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  45%|##############################6                                     | 9/20 [00:13<00:17,  1.57s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:58,133]\u001b[0m Trial 15 finished with value: 0.25422343866003166 and parameters: {'num_leaves': 125}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  45%|##############################6                                     | 9/20 [00:13<00:17,  1.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.207073\tvalid_1's binary_logloss: 0.254223\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  45%|##############################6                                     | 9/20 [00:15<00:17,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  50%|#################################5                                 | 10/20 [00:15<00:16,  1.63s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:13:59,899]\u001b[0m Trial 16 finished with value: 0.25340432684947956 and parameters: {'num_leaves': 240}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  50%|#################################5                                 | 10/20 [00:15<00:16,  1.63s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.190988\tvalid_1's binary_logloss: 0.253404\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  50%|#################################5                                 | 10/20 [00:16<00:16,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  55%|####################################8                              | 11/20 [00:16<00:13,  1.49s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:01,066]\u001b[0m Trial 17 finished with value: 0.24917734810307768 and parameters: {'num_leaves': 15}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  55%|####################################8                              | 11/20 [00:16<00:13,  1.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.224856\tvalid_1's binary_logloss: 0.249177\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  55%|####################################8                              | 11/20 [00:17<00:13,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  60%|########################################1                          | 12/20 [00:17<00:10,  1.34s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:02,079]\u001b[0m Trial 18 finished with value: 0.24809635374890499 and parameters: {'num_leaves': 14}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  60%|########################################1                          | 12/20 [00:17<00:10,  1.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.235317\tvalid_1's binary_logloss: 0.248096\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  60%|########################################1                          | 12/20 [00:18<00:10,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  65%|###########################################5                       | 13/20 [00:18<00:09,  1.33s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:03,371]\u001b[0m Trial 19 finished with value: 0.2496990830327145 and parameters: {'num_leaves': 6}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  65%|###########################################5                       | 13/20 [00:18<00:09,  1.33s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.230177\tvalid_1's binary_logloss: 0.249699\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  65%|###########################################5                       | 13/20 [00:20<00:09,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  70%|##############################################9                    | 14/20 [00:20<00:08,  1.37s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:04,834]\u001b[0m Trial 20 finished with value: 0.2502118555514578 and parameters: {'num_leaves': 59}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  70%|##############################################9                    | 14/20 [00:20<00:08,  1.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.198146\tvalid_1's binary_logloss: 0.250212\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  70%|##############################################9                    | 14/20 [00:21<00:08,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  75%|##################################################2                | 15/20 [00:21<00:06,  1.38s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:06,234]\u001b[0m Trial 21 finished with value: 0.25329397590693836 and parameters: {'num_leaves': 52}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  75%|##################################################2                | 15/20 [00:21<00:06,  1.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.204637\tvalid_1's binary_logloss: 0.253294\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  75%|##################################################2                | 15/20 [00:23<00:06,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  80%|#####################################################6             | 16/20 [00:23<00:05,  1.37s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:07,582]\u001b[0m Trial 22 finished with value: 0.25495522143016824 and parameters: {'num_leaves': 81}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  80%|#####################################################6             | 16/20 [00:23<00:05,  1.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.203543\tvalid_1's binary_logloss: 0.254955\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  80%|#####################################################6             | 16/20 [00:24<00:05,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.247001:  85%|########################################################9          | 17/20 [00:24<00:03,  1.30s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:08,735]\u001b[0m Trial 23 finished with value: 0.25091996025541885 and parameters: {'num_leaves': 28}. Best is trial 10 with value: 0.24800320804002793.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.247001:  85%|########################################################9          | 17/20 [00:24<00:03,  1.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.222415\tvalid_1's binary_logloss: 0.25092\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.246894:  85%|########################################################9          | 17/20 [00:25<00:03,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.246894:  90%|############################################################3      | 18/20 [00:25<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:09,993]\u001b[0m Trial 24 finished with value: 0.2468940792820915 and parameters: {'num_leaves': 36}. Best is trial 24 with value: 0.2468940792820915.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.246894:  90%|############################################################3      | 18/20 [00:25<00:02,  1.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.21151\tvalid_1's binary_logloss: 0.246894\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.246894:  90%|############################################################3      | 18/20 [00:26<00:02,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.246894:  95%|###############################################################6   | 19/20 [00:27<00:01,  1.35s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:11,474]\u001b[0m Trial 25 finished with value: 0.2536871911279751 and parameters: {'num_leaves': 78}. Best is trial 24 with value: 0.2468940792820915.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.246894:  95%|###############################################################6   | 19/20 [00:27<00:01,  1.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.192352\tvalid_1's binary_logloss: 0.253687\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.246894:  95%|###############################################################6   | 19/20 [00:28<00:01,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.246894: 100%|###################################################################| 20/20 [00:28<00:00,  1.44s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:13,169]\u001b[0m Trial 26 finished with value: 0.24926722981652985 and parameters: {'num_leaves': 38}. Best is trial 24 with value: 0.2468940792820915.\u001b[0m\n",
      "num_leaves, val_score: 0.246894: 100%|###################################################################| 20/20 [00:28<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.221578\tvalid_1's binary_logloss: 0.249267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246894:   0%|                                                                               | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.246894:   0%|                                                                               | 0/10 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246894:  10%|#######1                                                               | 1/10 [00:01<00:09,  1.01s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:14,201]\u001b[0m Trial 27 finished with value: 0.24902473205135048 and parameters: {'bagging_fraction': 0.444386203237662, 'bagging_freq': 3}. Best is trial 27 with value: 0.24902473205135048.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.246894:  10%|#######1                                                               | 1/10 [00:01<00:09,  1.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.241346\tvalid_1's binary_logloss: 0.249025\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.246894:  10%|#######1                                                               | 1/10 [00:02<00:09,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246894:  20%|##############2                                                        | 2/10 [00:02<00:09,  1.17s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:15,486]\u001b[0m Trial 28 finished with value: 0.25371235229073297 and parameters: {'bagging_fraction': 0.8125833715230513, 'bagging_freq': 5}. Best is trial 27 with value: 0.24902473205135048.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.246894:  20%|##############2                                                        | 2/10 [00:02<00:09,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.222415\tvalid_1's binary_logloss: 0.253712\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.246894:  20%|##############2                                                        | 2/10 [00:03<00:09,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246894:  30%|#####################3                                                 | 3/10 [00:03<00:07,  1.13s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:16,564]\u001b[0m Trial 29 finished with value: 0.24887188237135596 and parameters: {'bagging_fraction': 0.4603584783787247, 'bagging_freq': 5}. Best is trial 29 with value: 0.24887188237135596.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.246894:  30%|#####################3                                                 | 3/10 [00:03<00:07,  1.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.222622\tvalid_1's binary_logloss: 0.248872\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.246232:  30%|#####################3                                                 | 3/10 [00:04<00:07,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246232:  40%|############################4                                          | 4/10 [00:04<00:07,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:18,096]\u001b[0m Trial 30 finished with value: 0.24623192103282796 and parameters: {'bagging_fraction': 0.7308511163984397, 'bagging_freq': 5}. Best is trial 30 with value: 0.24623192103282796.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.246232:  40%|############################4                                          | 4/10 [00:04<00:07,  1.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.210193\tvalid_1's binary_logloss: 0.246232\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.246232:  40%|############################4                                          | 4/10 [00:06<00:07,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.246232:  50%|###################################5                                   | 5/10 [00:06<00:07,  1.55s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:20,130]\u001b[0m Trial 31 finished with value: 0.2509334914296106 and parameters: {'bagging_fraction': 0.867493795520367, 'bagging_freq': 1}. Best is trial 30 with value: 0.24623192103282796.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.246232:  50%|###################################5                                   | 5/10 [00:06<00:07,  1.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.226471\tvalid_1's binary_logloss: 0.250933\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  50%|###################################5                                   | 5/10 [00:08<00:07,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.244718:  60%|##########################################6                            | 6/10 [00:08<00:06,  1.50s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:21,528]\u001b[0m Trial 32 finished with value: 0.2447177160914369 and parameters: {'bagging_fraction': 0.5609926769577654, 'bagging_freq': 5}. Best is trial 32 with value: 0.2447177160914369.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  60%|##########################################6                            | 6/10 [00:08<00:06,  1.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.219823\tvalid_1's binary_logloss: 0.244718\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  60%|##########################################6                            | 6/10 [00:09<00:06,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.244718:  70%|#################################################6                     | 7/10 [00:09<00:04,  1.42s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:22,758]\u001b[0m Trial 33 finished with value: 0.25219629015156486 and parameters: {'bagging_fraction': 0.7667412683881585, 'bagging_freq': 7}. Best is trial 32 with value: 0.2447177160914369.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  70%|#################################################6                     | 7/10 [00:09<00:04,  1.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.223146\tvalid_1's binary_logloss: 0.252196\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  70%|#################################################6                     | 7/10 [00:10<00:04,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.244718:  80%|########################################################8              | 8/10 [00:10<00:02,  1.36s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:24,007]\u001b[0m Trial 34 finished with value: 0.255853449243965 and parameters: {'bagging_fraction': 0.9003548600671125, 'bagging_freq': 7}. Best is trial 32 with value: 0.2447177160914369.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.244718:  80%|########################################################8              | 8/10 [00:10<00:02,  1.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.229059\tvalid_1's binary_logloss: 0.255853\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.243671:  80%|########################################################8              | 8/10 [00:13<00:02,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.243671:  90%|###############################################################9       | 9/10 [00:13<00:01,  1.87s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:27,007]\u001b[0m Trial 35 finished with value: 0.24367120870518627 and parameters: {'bagging_fraction': 0.5122235475816761, 'bagging_freq': 3}. Best is trial 35 with value: 0.24367120870518627.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.243671:  90%|###############################################################9       | 9/10 [00:13<00:01,  1.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216685\tvalid_1's binary_logloss: 0.243671\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.243671:  90%|###############################################################9       | 9/10 [00:15<00:01,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.243671: 100%|######################################################################| 10/10 [00:15<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:28,865]\u001b[0m Trial 36 finished with value: 0.24420966856150994 and parameters: {'bagging_fraction': 0.8585275785593824, 'bagging_freq': 2}. Best is trial 35 with value: 0.24367120870518627.\u001b[0m\n",
      "bagging, val_score: 0.243671: 100%|######################################################################| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.198028\tvalid_1's binary_logloss: 0.24421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.243671:   0%|                                                                | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.243671:   0%|                                                                | 0/6 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.243671:  17%|#########3                                              | 1/6 [00:01<00:06,  1.37s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:30,245]\u001b[0m Trial 37 finished with value: 0.24427897045037558 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.24427897045037558.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.243671:  17%|#########3                                              | 1/6 [00:01<00:06,  1.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216949\tvalid_1's binary_logloss: 0.244279\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  17%|#########3                                              | 1/6 [00:02<00:06,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  33%|##################6                                     | 2/6 [00:02<00:05,  1.44s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:31,728]\u001b[0m Trial 38 finished with value: 0.24260023988931018 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 38 with value: 0.24260023988931018.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  33%|##################6                                     | 2/6 [00:02<00:05,  1.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.213835\tvalid_1's binary_logloss: 0.2426\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  33%|##################6                                     | 2/6 [00:04<00:05,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  50%|############################                            | 3/6 [00:04<00:04,  1.60s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:33,533]\u001b[0m Trial 39 finished with value: 0.24396890270309388 and parameters: {'feature_fraction': 0.784}. Best is trial 38 with value: 0.24260023988931018.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  50%|############################                            | 3/6 [00:04<00:04,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.22076\tvalid_1's binary_logloss: 0.243969\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  50%|############################                            | 3/6 [00:05<00:04,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  67%|#####################################3                  | 4/6 [00:05<00:02,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:34,760]\u001b[0m Trial 40 finished with value: 0.24603532859275537 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 38 with value: 0.24260023988931018.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  67%|#####################################3                  | 4/6 [00:05<00:02,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216296\tvalid_1's binary_logloss: 0.246035\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  67%|#####################################3                  | 4/6 [00:07<00:02,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  83%|##############################################6         | 5/6 [00:07<00:01,  1.49s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:36,312]\u001b[0m Trial 41 finished with value: 0.24886991126089117 and parameters: {'feature_fraction': 0.88}. Best is trial 38 with value: 0.24260023988931018.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  83%|##############################################6         | 5/6 [00:07<00:01,  1.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.214688\tvalid_1's binary_logloss: 0.24887\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600:  83%|##############################################6         | 5/6 [00:09<00:01,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.242600: 100%|########################################################| 6/6 [00:09<00:00,  1.84s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:38,841]\u001b[0m Trial 42 finished with value: 0.24909071942498665 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 38 with value: 0.24260023988931018.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.242600: 100%|########################################################| 6/6 [00:09<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.216018\tvalid_1's binary_logloss: 0.249091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.242600:   0%|                                                                | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.242600:   0%|                                                                | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.242600:   5%|##8                                                     | 1/20 [00:01<00:28,  1.51s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:40,361]\u001b[0m Trial 43 finished with value: 0.24622846116494315 and parameters: {'lambda_l1': 1.1729185441106883e-05, 'lambda_l2': 4.3127393101615355e-05}. Best is trial 43 with value: 0.24622846116494315.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.242600:   5%|##8                                                     | 1/20 [00:01<00:28,  1.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216861\tvalid_1's binary_logloss: 0.246228\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:   5%|##8                                                     | 1/20 [00:02<00:28,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  10%|#####6                                                  | 2/20 [00:02<00:26,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:41,778]\u001b[0m Trial 44 finished with value: 0.24198707220276267 and parameters: {'lambda_l1': 0.02219835881173074, 'lambda_l2': 8.13336478153471e-08}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  10%|#####6                                                  | 2/20 [00:02<00:26,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217127\tvalid_1's binary_logloss: 0.241987\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  10%|#####6                                                  | 2/20 [00:04<00:26,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  15%|########4                                               | 3/20 [00:04<00:23,  1.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:43,083]\u001b[0m Trial 45 finished with value: 0.24622463013691126 and parameters: {'lambda_l1': 2.1942331272424906e-08, 'lambda_l2': 0.0004377552355449219}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  15%|########4                                               | 3/20 [00:04<00:23,  1.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216863\tvalid_1's binary_logloss: 0.246225\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  15%|########4                                               | 3/20 [00:05<00:23,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  20%|###########2                                            | 4/20 [00:05<00:20,  1.30s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:44,245]\u001b[0m Trial 46 finished with value: 0.24307198109819375 and parameters: {'lambda_l1': 0.25273651762146604, 'lambda_l2': 2.9559150546785744e-05}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  20%|###########2                                            | 4/20 [00:05<00:20,  1.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.218702\tvalid_1's binary_logloss: 0.243072\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  20%|###########2                                            | 4/20 [00:06<00:20,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  25%|##############                                          | 5/20 [00:06<00:20,  1.38s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:45,769]\u001b[0m Trial 47 finished with value: 0.2498976113204293 and parameters: {'lambda_l1': 2.3101609324706485, 'lambda_l2': 0.21226406620073593}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  25%|##############                                          | 5/20 [00:06<00:20,  1.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.206313\tvalid_1's binary_logloss: 0.249898\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  25%|##############                                          | 5/20 [00:08<00:20,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  30%|################8                                       | 6/20 [00:08<00:20,  1.48s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:47,439]\u001b[0m Trial 48 finished with value: 0.24559459356336472 and parameters: {'lambda_l1': 1.7644460375062927, 'lambda_l2': 7.157584701725432e-06}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  30%|################8                                       | 6/20 [00:08<00:20,  1.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.213928\tvalid_1's binary_logloss: 0.245595\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  30%|################8                                       | 6/20 [00:09<00:20,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  35%|###################5                                    | 7/20 [00:09<00:17,  1.38s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:48,602]\u001b[0m Trial 49 finished with value: 0.24620302818588416 and parameters: {'lambda_l1': 4.885689880150603e-07, 'lambda_l2': 1.2114536905452614e-07}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  35%|###################5                                    | 7/20 [00:09<00:17,  1.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.21686\tvalid_1's binary_logloss: 0.246203\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  35%|###################5                                    | 7/20 [00:11<00:17,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  40%|######################4                                 | 8/20 [00:11<00:18,  1.50s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:50,380]\u001b[0m Trial 50 finished with value: 0.24863020302664224 and parameters: {'lambda_l1': 2.775847135769235e-05, 'lambda_l2': 4.153322926733303}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  40%|######################4                                 | 8/20 [00:11<00:18,  1.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.208506\tvalid_1's binary_logloss: 0.24863\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  40%|######################4                                 | 8/20 [00:12<00:18,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  45%|#########################2                              | 9/20 [00:12<00:15,  1.42s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:51,614]\u001b[0m Trial 51 finished with value: 0.24219778158680944 and parameters: {'lambda_l1': 2.24246820322182e-06, 'lambda_l2': 0.015545265850638871}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  45%|#########################2                              | 9/20 [00:12<00:15,  1.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217143\tvalid_1's binary_logloss: 0.242198\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  45%|#########################2                              | 9/20 [00:14<00:15,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  50%|###########################5                           | 10/20 [00:14<00:13,  1.37s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:52,862]\u001b[0m Trial 52 finished with value: 0.2461916525546348 and parameters: {'lambda_l1': 0.0007496636757289597, 'lambda_l2': 0.001079126911119451}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  50%|###########################5                           | 10/20 [00:14<00:13,  1.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.216822\tvalid_1's binary_logloss: 0.246192\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  50%|###########################5                           | 10/20 [00:15<00:13,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  55%|##############################2                        | 11/20 [00:15<00:11,  1.31s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:54,035]\u001b[0m Trial 53 finished with value: 0.24708606619760282 and parameters: {'lambda_l1': 0.010240204034114646, 'lambda_l2': 2.185313535621972e-08}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  55%|##############################2                        | 11/20 [00:15<00:11,  1.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.217786\tvalid_1's binary_logloss: 0.247086\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  55%|##############################2                        | 11/20 [00:16<00:11,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  60%|#################################                      | 12/20 [00:16<00:10,  1.26s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:55,202]\u001b[0m Trial 54 finished with value: 0.24539892204692784 and parameters: {'lambda_l1': 0.0036366359621051816, 'lambda_l2': 0.020077038371611764}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  60%|#################################                      | 12/20 [00:16<00:10,  1.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.218144\tvalid_1's binary_logloss: 0.245399\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  60%|#################################                      | 12/20 [00:17<00:10,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  65%|###################################7                   | 13/20 [00:17<00:08,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:56,351]\u001b[0m Trial 55 finished with value: 0.24620295907702433 and parameters: {'lambda_l1': 2.4749608280422542e-05, 'lambda_l2': 8.051624017551887e-07}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  65%|###################################7                   | 13/20 [00:17<00:08,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.21686\tvalid_1's binary_logloss: 0.246203\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  65%|###################################7                   | 13/20 [00:18<00:08,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  70%|######################################5                | 14/20 [00:18<00:07,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:57,570]\u001b[0m Trial 56 finished with value: 0.24363212298094153 and parameters: {'lambda_l1': 0.028533210732060603, 'lambda_l2': 0.012230981514229735}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  70%|######################################5                | 14/20 [00:18<00:07,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217403\tvalid_1's binary_logloss: 0.243632\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  70%|######################################5                | 14/20 [00:20<00:07,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  75%|#########################################2             | 15/20 [00:20<00:06,  1.32s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:14:59,093]\u001b[0m Trial 57 finished with value: 0.24406219091339806 and parameters: {'lambda_l1': 5.110964060143719e-07, 'lambda_l2': 9.025533485808314}. Best is trial 44 with value: 0.24198707220276267.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241987:  75%|#########################################2             | 15/20 [00:20<00:06,  1.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.214584\tvalid_1's binary_logloss: 0.244062\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  75%|#########################################2             | 15/20 [00:21<00:06,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  80%|############################################           | 16/20 [00:21<00:05,  1.27s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:00,245]\u001b[0m Trial 58 finished with value: 0.24197990305517758 and parameters: {'lambda_l1': 0.0002536948056253431, 'lambda_l2': 0.015547769283306171}. Best is trial 58 with value: 0.24197990305517758.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  80%|############################################           | 16/20 [00:21<00:05,  1.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217193\tvalid_1's binary_logloss: 0.24198\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  80%|############################################           | 16/20 [00:22<00:05,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  85%|##############################################7        | 17/20 [00:22<00:04,  1.33s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:01,736]\u001b[0m Trial 59 finished with value: 0.2455853826486585 and parameters: {'lambda_l1': 0.00039069795628847063, 'lambda_l2': 0.24494596979897099}. Best is trial 58 with value: 0.24197990305517758.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  85%|##############################################7        | 17/20 [00:22<00:04,  1.33s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.192825\tvalid_1's binary_logloss: 0.245585\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  85%|##############################################7        | 17/20 [00:24<00:04,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  90%|#################################################5     | 18/20 [00:24<00:02,  1.28s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:02,895]\u001b[0m Trial 60 finished with value: 0.2432135786486843 and parameters: {'lambda_l1': 0.06543575139868417, 'lambda_l2': 5.770132943926466e-07}. Best is trial 58 with value: 0.24197990305517758.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  90%|#################################################5     | 18/20 [00:24<00:02,  1.28s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.220237\tvalid_1's binary_logloss: 0.243214\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  90%|#################################################5     | 18/20 [00:25<00:02,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  95%|####################################################2  | 19/20 [00:25<00:01,  1.24s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:04,032]\u001b[0m Trial 61 finished with value: 0.2463956480785321 and parameters: {'lambda_l1': 0.002049030700920323, 'lambda_l2': 0.004617767835063844}. Best is trial 58 with value: 0.24197990305517758.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  95%|####################################################2  | 19/20 [00:25<00:01,  1.24s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.217912\tvalid_1's binary_logloss: 0.246396\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.241980:  95%|####################################################2  | 19/20 [00:26<00:01,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.241980: 100%|#######################################################| 20/20 [00:26<00:00,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:05,166]\u001b[0m Trial 62 finished with value: 0.24622265386583891 and parameters: {'lambda_l1': 0.00015142773328304617, 'lambda_l2': 1.425435693105377e-08}. Best is trial 58 with value: 0.24197990305517758.\u001b[0m\n",
      "regularization_factors, val_score: 0.241980: 100%|#######################################################| 20/20 [00:26<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.21686\tvalid_1's binary_logloss: 0.246223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:   0%|                                                                       | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:   0%|                                                                       | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:  20%|############6                                                  | 1/5 [00:00<00:03,  1.03it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:06,146]\u001b[0m Trial 63 finished with value: 0.2558459614105758 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.2558459614105758.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:  20%|############6                                                  | 1/5 [00:00<00:03,  1.03it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.237337\tvalid_1's binary_logloss: 0.255846\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:  20%|############6                                                  | 1/5 [00:02<00:03,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.06s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:07,266]\u001b[0m Trial 64 finished with value: 0.24427683026526847 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.24427683026526847.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241980:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.220309\tvalid_1's binary_logloss: 0.244277\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  40%|#########################2                                     | 2/5 [00:03<00:03,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:08,660]\u001b[0m Trial 65 finished with value: 0.24128949488329593 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.24128949488329593.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.214426\tvalid_1's binary_logloss: 0.241289\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  60%|#####################################8                         | 3/5 [00:04<00:02,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  80%|##################################################4            | 4/5 [00:04<00:01,  1.24s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:09,933]\u001b[0m Trial 66 finished with value: 0.24236255092218226 and parameters: {'min_child_samples': 50}. Best is trial 65 with value: 0.24128949488329593.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  80%|##################################################4            | 4/5 [00:04<00:01,  1.24s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.215725\tvalid_1's binary_logloss: 0.242363\n",
      "[LightGBM] [Info] Number of positive: 2888, number of negative: 36015\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074236 -> initscore=-2.523371\n",
      "[LightGBM] [Info] Start training from score -2.523371\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289:  80%|##################################################4            | 4/5 [00:05<00:01,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.241289: 100%|###############################################################| 5/5 [00:05<00:00,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:15:11,095]\u001b[0m Trial 67 finished with value: 0.2488106125720543 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.24128949488329593.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.241289: 100%|###############################################################| 5/5 [00:05<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.221051\tvalid_1's binary_logloss: 0.248811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.0002536948056253431,\n",
       " 'lambda_l2': 0.015547769283306171,\n",
       " 'num_leaves': 36,\n",
       " 'feature_fraction': 0.8160000000000001,\n",
       " 'bagging_fraction': 0.5122235475816761,\n",
       " 'bagging_freq': 3,\n",
       " 'min_child_samples': 100,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_train1, y_train1, X_valid1, y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff605d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8160000000000001, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8160000000000001\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5122235475816761, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5122235475816761\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.015547769283306171, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015547769283306171\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0002536948056253431, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002536948056253431\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 0.0002536948056253431,\n",
    " 'lambda_l2': 0.015547769283306171,\n",
    " 'num_leaves': 36,\n",
    " 'feature_fraction': 0.8160000000000001,\n",
    " 'bagging_fraction': 0.5122235475816761,\n",
    " 'bagging_freq': 3,\n",
    " 'min_child_samples': 100,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "lgb_clf = fit(params, X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644c37fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120895538387197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf, haitou, std=True)\n",
    "me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8818dc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyterthemes import jtplot\n",
    "\n",
    "y_pred = me_valid.predict_proba(X_valid1).values\n",
    "\n",
    "jtplot.style(theme='monokai')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid1, y_pred)\n",
    "pit.plot(fpr, tpr, marker='o')\n",
    "pit.xlabel(\"False positive rate\")\n",
    "pit.ylabel(\"True positive rate\")\n",
    "pit.grid()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bf9a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>調教師同コース同距離別騎乗回数偏差</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>調教師コース別騎乗回数偏差</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>騎乗騎手年間出遅れ率偏差</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>調教師競馬場別騎乗回数偏差</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>調教師年齢別年間複勝率偏差</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>種牡馬同コース同距離別勝率偏差</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>騎手同コース同距離別勝率偏差</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>furlong_5</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>騎手同コース同距離別複勝率偏差</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>調教師距離別騎乗回数偏差</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>騎手距離別騎乗回数偏差</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>種牡馬同コース同距離別複勝率偏差</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>調教師年齢別年間勝率偏差</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>騎手同コース同距離別連対率偏差</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>調教師同コース同距離別複勝率偏差</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>騎手同コース同距離別騎乗回数偏差</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>調教師競馬場別複勝率偏差</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>調教師距離別複勝率偏差</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>調教師競馬場別勝率偏差</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>調教師年齢別年間連対率偏差</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>peds_2</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>調教師距離別連対率偏差</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>furlong_5_4</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>騎手距離別勝率偏差</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>調教師距離別勝率偏差</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>peds_62</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trainer_id</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>騎手競馬場別騎乗回数偏差</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>調教師同コース同距離別勝率偏差</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>種牡馬競馬場別複勝率偏差</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>peds_61</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>調教師年齢別勝率偏差</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>peds_60</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>種牡馬競馬場別勝率偏差</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>種牡馬距離別複勝率偏差</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>調教師同コース同距離別連対率偏差</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>調教師出走回数</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>peds_29</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>父系統同コース同距離別連対率偏差</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>父系統同コース同距離別勝率偏差</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>種牡馬同コース同距離別連対率偏差</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>調教師競馬場別連対率偏差</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>兄弟複勝率</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>騎手騎乗回数</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>peds_6</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>騎手距離別複勝率偏差</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>騎手競馬場別勝率偏差</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>peds_14</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>種牡馬競馬場別連対率偏差</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "87   調教師同コース同距離別騎乗回数偏差         354\n",
       "79       調教師コース別騎乗回数偏差         353\n",
       "176       騎乗騎手年間出遅れ率偏差         350\n",
       "75       調教師競馬場別騎乗回数偏差         342\n",
       "91       調教師年齢別年間複勝率偏差         337\n",
       "113    種牡馬同コース同距離別勝率偏差         332\n",
       "69      騎手同コース同距離別勝率偏差         323\n",
       "22           furlong_5         322\n",
       "71     騎手同コース同距離別複勝率偏差         318\n",
       "83        調教師距離別騎乗回数偏差         313\n",
       "64         騎手距離別騎乗回数偏差         309\n",
       "115   種牡馬同コース同距離別複勝率偏差         307\n",
       "89        調教師年齢別年間勝率偏差         306\n",
       "70     騎手同コース同距離別連対率偏差         301\n",
       "96    調教師同コース同距離別複勝率偏差         300\n",
       "68    騎手同コース同距離別騎乗回数偏差         292\n",
       "78        調教師競馬場別複勝率偏差         291\n",
       "86         調教師距離別複勝率偏差         290\n",
       "76         調教師競馬場別勝率偏差         288\n",
       "90       調教師年齢別年間連対率偏差         288\n",
       "177             peds_2         283\n",
       "85         調教師距離別連対率偏差         278\n",
       "27         furlong_5_4         273\n",
       "65           騎手距離別勝率偏差         273\n",
       "84          調教師距離別勝率偏差         272\n",
       "237            peds_62         268\n",
       "6           trainer_id         267\n",
       "56        騎手競馬場別騎乗回数偏差         266\n",
       "88     調教師同コース同距離別勝率偏差         265\n",
       "103       種牡馬競馬場別複勝率偏差         265\n",
       "236            peds_61         262\n",
       "92          調教師年齢別勝率偏差         261\n",
       "235            peds_60         259\n",
       "101        種牡馬競馬場別勝率偏差         257\n",
       "111        種牡馬距離別複勝率偏差         257\n",
       "95    調教師同コース同距離別連対率偏差         256\n",
       "36             調教師出走回数         255\n",
       "204            peds_29         254\n",
       "140   父系統同コース同距離別連対率偏差         253\n",
       "3            jockey_id         251\n",
       "139    父系統同コース同距離別勝率偏差         251\n",
       "114   種牡馬同コース同距離別連対率偏差         250\n",
       "77        調教師競馬場別連対率偏差         250\n",
       "20               兄弟複勝率         249\n",
       "35              騎手騎乗回数         248\n",
       "181             peds_6         248\n",
       "67          騎手距離別複勝率偏差         245\n",
       "57          騎手競馬場別勝率偏差         244\n",
       "189            peds_14         242\n",
       "102       種牡馬競馬場別連対率偏差         238"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid.feature_importance(X_train1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01f7cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：371 レース数:861 対象レース率:43.0% 的中率:11.9% 的中数:44 賭金:37,100円 配当合計:41,020円 最高配当:8,020円 回収率:110.6%\n"
     ]
    }
   ],
   "source": [
    "wr = me_valid.pred_table(X_valid1, 0.7, True)\n",
    "tansho_return_rate(X_valid1, wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "907c7aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：160 レース数:369 対象レース率:43.4% 的中率:12.5% 的中数:20 賭金:16,000円 配当合計:16,710円 最高配当:3,090円 回収率:104.4%\n"
     ]
    }
   ],
   "source": [
    "me_test= ModelEvaluator(lgb_clf, haitou, std=True)\n",
    "\n",
    "twr = me_test.pred_table(X_test2, 0.7)\n",
    "tansho_return_rate(X_test2, twr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07d73688",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 連対\n",
    "dr = build_data('./pickle_new/shinba_base.pickle', './pickle_new/peds_vec.pickle', 3, vec)\n",
    "X_rentai_train1 = dr['X_train']\n",
    "y_rentai_train1 = dr['y_train']\n",
    "X_rentai_valid1 = dr['X_valid']\n",
    "y_rentai_valid1 = dr['y_valid']\n",
    "X_rentai_test2 = dr['X_test2']\n",
    "y_rentai_test2 = dr['y_test2']\n",
    "X_rentai_test1 = dr['X_test1']\n",
    "y_rentai_test1 = dr['y_test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "c07608b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 09:26:26,614]\u001b[0m A new study created in memory with name: no-name-55e852f2-1004-4c94-9728-23ad9da43bc9\u001b[0m\n",
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:   0%|                                                                       | 0/7 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  14%|#########                                                      | 1/7 [00:01<00:07,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:27,854]\u001b[0m Trial 0 finished with value: 0.3923202477617546 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.3923202477617546.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  14%|#########                                                      | 1/7 [00:01<00:07,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.362467\tvalid_1's binary_logloss: 0.39232\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  14%|#########                                                      | 1/7 [00:02<00:07,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  29%|##################                                             | 2/7 [00:02<00:06,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:29,080]\u001b[0m Trial 1 finished with value: 0.3970027763350277 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.3923202477617546.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  29%|##################                                             | 2/7 [00:02<00:06,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.373494\tvalid_1's binary_logloss: 0.397003\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  29%|##################                                             | 2/7 [00:03<00:06,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  43%|###########################                                    | 3/7 [00:03<00:04,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:30,236]\u001b[0m Trial 2 finished with value: 0.3982645053799014 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.3923202477617546.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  43%|###########################                                    | 3/7 [00:03<00:04,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.375711\tvalid_1's binary_logloss: 0.398265\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  43%|###########################                                    | 3/7 [00:04<00:04,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  57%|####################################                           | 4/7 [00:04<00:03,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:31,429]\u001b[0m Trial 3 finished with value: 0.3927441168674192 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.3923202477617546.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.392320:  57%|####################################                           | 4/7 [00:04<00:03,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.367057\tvalid_1's binary_logloss: 0.392744\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  57%|####################################                           | 4/7 [00:05<00:03,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  71%|#############################################                  | 5/7 [00:05<00:02,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:32,613]\u001b[0m Trial 4 finished with value: 0.39187720184769115 and parameters: {'feature_fraction': 0.6}. Best is trial 4 with value: 0.39187720184769115.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  71%|#############################################                  | 5/7 [00:05<00:02,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.367469\tvalid_1's binary_logloss: 0.391877\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  71%|#############################################                  | 5/7 [00:07<00:02,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  86%|######################################################         | 6/7 [00:07<00:01,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:33,794]\u001b[0m Trial 5 finished with value: 0.39210705998108564 and parameters: {'feature_fraction': 0.8}. Best is trial 4 with value: 0.39187720184769115.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  86%|######################################################         | 6/7 [00:07<00:01,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.37143\tvalid_1's binary_logloss: 0.392107\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.391877:  86%|######################################################         | 6/7 [00:08<00:01,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.391877: 100%|###############################################################| 7/7 [00:08<00:00,  1.16s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:34,882]\u001b[0m Trial 6 finished with value: 0.39663760581247137 and parameters: {'feature_fraction': 0.4}. Best is trial 4 with value: 0.39187720184769115.\u001b[0m\n",
      "feature_fraction, val_score: 0.391877: 100%|###############################################################| 7/7 [00:08<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.357783\tvalid_1's binary_logloss: 0.396638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.391877:   0%|                                                                            | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.390997:   0%|                                                                            | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.390997:   5%|###4                                                                | 1/20 [00:01<00:22,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:36,099]\u001b[0m Trial 7 finished with value: 0.39099685969338416 and parameters: {'num_leaves': 37}. Best is trial 7 with value: 0.39099685969338416.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.390997:   5%|###4                                                                | 1/20 [00:01<00:22,  1.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.364454\tvalid_1's binary_logloss: 0.390997\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:   5%|###4                                                                | 1/20 [00:02<00:22,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  10%|######8                                                             | 2/20 [00:02<00:21,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:37,283]\u001b[0m Trial 8 finished with value: 0.38718379896835514 and parameters: {'num_leaves': 43}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  10%|######8                                                             | 2/20 [00:02<00:21,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  10%|######8                                                             | 2/20 [00:04<00:21,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  15%|##########2                                                         | 3/20 [00:04<00:25,  1.48s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:39,111]\u001b[0m Trial 9 finished with value: 0.39754402076602957 and parameters: {'num_leaves': 249}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  15%|##########2                                                         | 3/20 [00:04<00:25,  1.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.292768\tvalid_1's binary_logloss: 0.397544\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  15%|##########2                                                         | 3/20 [00:06<00:25,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  20%|#############6                                                      | 4/20 [00:06<00:27,  1.69s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:41,127]\u001b[0m Trial 10 finished with value: 0.3976463704733179 and parameters: {'num_leaves': 234}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  20%|#############6                                                      | 4/20 [00:06<00:27,  1.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.303068\tvalid_1's binary_logloss: 0.397646\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  20%|#############6                                                      | 4/20 [00:08<00:27,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  25%|#################                                                   | 5/20 [00:08<00:27,  1.82s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:43,169]\u001b[0m Trial 11 finished with value: 0.39714798881829827 and parameters: {'num_leaves': 222}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  25%|#################                                                   | 5/20 [00:08<00:27,  1.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.293238\tvalid_1's binary_logloss: 0.397148\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  25%|#################                                                   | 5/20 [00:09<00:27,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  30%|####################4                                               | 6/20 [00:09<00:25,  1.79s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:44,888]\u001b[0m Trial 12 finished with value: 0.40207955819683966 and parameters: {'num_leaves': 246}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  30%|####################4                                               | 6/20 [00:10<00:25,  1.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.339544\tvalid_1's binary_logloss: 0.40208\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  30%|####################4                                               | 6/20 [00:11<00:25,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  35%|#######################7                                            | 7/20 [00:11<00:20,  1.54s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:45,930]\u001b[0m Trial 13 finished with value: 0.39327829990778046 and parameters: {'num_leaves': 26}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  35%|#######################7                                            | 7/20 [00:11<00:20,  1.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.370303\tvalid_1's binary_logloss: 0.393278\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  35%|#######################7                                            | 7/20 [00:12<00:20,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  40%|###########################2                                        | 8/20 [00:12<00:17,  1.48s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:47,267]\u001b[0m Trial 14 finished with value: 0.39781915209115926 and parameters: {'num_leaves': 88}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  40%|###########################2                                        | 8/20 [00:12<00:17,  1.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.33309\tvalid_1's binary_logloss: 0.397819\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  40%|###########################2                                        | 8/20 [00:13<00:17,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  45%|##############################6                                     | 9/20 [00:14<00:16,  1.52s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:48,892]\u001b[0m Trial 15 finished with value: 0.39753184888603293 and parameters: {'num_leaves': 141}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  45%|##############################6                                     | 9/20 [00:14<00:16,  1.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.330164\tvalid_1's binary_logloss: 0.397532\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  45%|##############################6                                     | 9/20 [00:16<00:16,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  50%|#################################5                                 | 10/20 [00:16<00:17,  1.78s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:51,259]\u001b[0m Trial 16 finished with value: 0.3970274022579841 and parameters: {'num_leaves': 256}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  50%|#################################5                                 | 10/20 [00:16<00:17,  1.78s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.264388\tvalid_1's binary_logloss: 0.397027\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  50%|#################################5                                 | 10/20 [00:17<00:17,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  55%|####################################8                              | 11/20 [00:17<00:15,  1.73s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:52,853]\u001b[0m Trial 17 finished with value: 0.39688894276989556 and parameters: {'num_leaves': 95}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  55%|####################################8                              | 11/20 [00:17<00:15,  1.73s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.326291\tvalid_1's binary_logloss: 0.396889\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  55%|####################################8                              | 11/20 [00:18<00:15,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  60%|########################################1                          | 12/20 [00:19<00:12,  1.52s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:53,893]\u001b[0m Trial 18 finished with value: 0.3921284186587476 and parameters: {'num_leaves': 4}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  60%|########################################1                          | 12/20 [00:19<00:12,  1.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.376498\tvalid_1's binary_logloss: 0.392128\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  60%|########################################1                          | 12/20 [00:20<00:12,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  65%|###########################################5                       | 13/20 [00:20<00:10,  1.49s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:55,318]\u001b[0m Trial 19 finished with value: 0.39267579833460486 and parameters: {'num_leaves': 52}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  65%|###########################################5                       | 13/20 [00:20<00:10,  1.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.342774\tvalid_1's binary_logloss: 0.392676\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  65%|###########################################5                       | 13/20 [00:22<00:10,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  70%|##############################################9                    | 14/20 [00:22<00:09,  1.53s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:56,935]\u001b[0m Trial 20 finished with value: 0.39671096648967596 and parameters: {'num_leaves': 157}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  70%|##############################################9                    | 14/20 [00:22<00:09,  1.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.303653\tvalid_1's binary_logloss: 0.396711\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  70%|##############################################9                    | 14/20 [00:23<00:09,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  75%|##################################################2                | 15/20 [00:23<00:07,  1.43s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:58,145]\u001b[0m Trial 21 finished with value: 0.39488967061943825 and parameters: {'num_leaves': 67}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  75%|##################################################2                | 15/20 [00:23<00:07,  1.43s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.346787\tvalid_1's binary_logloss: 0.39489\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  75%|##################################################2                | 15/20 [00:24<00:07,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  80%|#####################################################6             | 16/20 [00:24<00:05,  1.48s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:26:59,721]\u001b[0m Trial 22 finished with value: 0.39459817622941873 and parameters: {'num_leaves': 111}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  80%|#####################################################6             | 16/20 [00:24<00:05,  1.48s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.331175\tvalid_1's binary_logloss: 0.394598\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  80%|#####################################################6             | 16/20 [00:25<00:05,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  85%|########################################################9          | 17/20 [00:25<00:04,  1.36s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:00,806]\u001b[0m Trial 23 finished with value: 0.39343112320204915 and parameters: {'num_leaves': 40}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  85%|########################################################9          | 17/20 [00:25<00:04,  1.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.367821\tvalid_1's binary_logloss: 0.393431\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  85%|########################################################9          | 17/20 [00:27<00:04,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  90%|############################################################3      | 18/20 [00:27<00:02,  1.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:02,284]\u001b[0m Trial 24 finished with value: 0.39790909600258506 and parameters: {'num_leaves': 179}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  90%|############################################################3      | 18/20 [00:27<00:02,  1.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.330351\tvalid_1's binary_logloss: 0.397909\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  90%|############################################################3      | 18/20 [00:28<00:02,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184:  95%|###############################################################6   | 19/20 [00:28<00:01,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:03,336]\u001b[0m Trial 25 finished with value: 0.3916671490344022 and parameters: {'num_leaves': 13}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  95%|###############################################################6   | 19/20 [00:28<00:01,  1.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.369486\tvalid_1's binary_logloss: 0.391667\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.387184:  95%|###############################################################6   | 19/20 [00:29<00:01,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.387184: 100%|###################################################################| 20/20 [00:29<00:00,  1.33s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:04,761]\u001b[0m Trial 26 finished with value: 0.3968524529779862 and parameters: {'num_leaves': 65}. Best is trial 8 with value: 0.38718379896835514.\u001b[0m\n",
      "num_leaves, val_score: 0.387184: 100%|###################################################################| 20/20 [00:29<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.341968\tvalid_1's binary_logloss: 0.396852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:   0%|                                                                               | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:   0%|                                                                               | 0/10 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  10%|#######1                                                               | 1/10 [00:01<00:09,  1.06s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:05,833]\u001b[0m Trial 27 finished with value: 0.39367271353085614 and parameters: {'bagging_fraction': 0.864323790667658, 'bagging_freq': 1}. Best is trial 27 with value: 0.39367271353085614.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  10%|#######1                                                               | 1/10 [00:01<00:09,  1.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.366423\tvalid_1's binary_logloss: 0.393673\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  10%|#######1                                                               | 1/10 [00:02<00:09,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  20%|##############2                                                        | 2/10 [00:02<00:08,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:06,938]\u001b[0m Trial 28 finished with value: 0.3889937287505266 and parameters: {'bagging_fraction': 0.4212232701091194, 'bagging_freq': 7}. Best is trial 28 with value: 0.3889937287505266.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  20%|##############2                                                        | 2/10 [00:02<00:08,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.348956\tvalid_1's binary_logloss: 0.388994\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  20%|##############2                                                        | 2/10 [00:03<00:08,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  30%|#####################3                                                 | 3/10 [00:03<00:07,  1.03s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:07,912]\u001b[0m Trial 29 finished with value: 0.3993372371887554 and parameters: {'bagging_fraction': 0.5693592241987779, 'bagging_freq': 3}. Best is trial 28 with value: 0.3889937287505266.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  30%|#####################3                                                 | 3/10 [00:03<00:07,  1.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.3755\tvalid_1's binary_logloss: 0.399337\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  30%|#####################3                                                 | 3/10 [00:04<00:07,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  40%|############################4                                          | 4/10 [00:04<00:06,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:09,088]\u001b[0m Trial 30 finished with value: 0.4007601535273484 and parameters: {'bagging_fraction': 0.7650438748670704, 'bagging_freq': 4}. Best is trial 28 with value: 0.3889937287505266.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  40%|############################4                                          | 4/10 [00:04<00:06,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.366121\tvalid_1's binary_logloss: 0.40076\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  40%|############################4                                          | 4/10 [00:05<00:06,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  50%|###################################5                                   | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:10,230]\u001b[0m Trial 31 finished with value: 0.39473130886547014 and parameters: {'bagging_fraction': 0.9730317015718298, 'bagging_freq': 7}. Best is trial 28 with value: 0.3889937287505266.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  50%|###################################5                                   | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.363563\tvalid_1's binary_logloss: 0.394731\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  50%|###################################5                                   | 5/10 [00:06<00:05,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  60%|##########################################6                            | 6/10 [00:06<00:04,  1.15s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:11,445]\u001b[0m Trial 32 finished with value: 0.3941777156641666 and parameters: {'bagging_fraction': 0.8479568296823062, 'bagging_freq': 5}. Best is trial 28 with value: 0.3889937287505266.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  60%|##########################################6                            | 6/10 [00:06<00:04,  1.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.347302\tvalid_1's binary_logloss: 0.394178\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  60%|##########################################6                            | 6/10 [00:07<00:04,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  70%|#################################################6                     | 7/10 [00:07<00:03,  1.17s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:12,671]\u001b[0m Trial 33 finished with value: 0.3877453394118215 and parameters: {'bagging_fraction': 0.7205795955648044, 'bagging_freq': 4}. Best is trial 33 with value: 0.3877453394118215.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  70%|#################################################6                     | 7/10 [00:07<00:03,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.357073\tvalid_1's binary_logloss: 0.387745\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  70%|#################################################6                     | 7/10 [00:09<00:03,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  80%|########################################################8              | 8/10 [00:09<00:02,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:13,954]\u001b[0m Trial 34 finished with value: 0.393134111842304 and parameters: {'bagging_fraction': 0.62872216332561, 'bagging_freq': 4}. Best is trial 33 with value: 0.3877453394118215.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  80%|########################################################8              | 8/10 [00:09<00:02,  1.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.34654\tvalid_1's binary_logloss: 0.393134\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  80%|########################################################8              | 8/10 [00:10<00:02,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184:  90%|###############################################################9       | 9/10 [00:10<00:01,  1.22s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:15,203]\u001b[0m Trial 35 finished with value: 0.39082546558441433 and parameters: {'bagging_fraction': 0.8684257433048116, 'bagging_freq': 3}. Best is trial 33 with value: 0.3877453394118215.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  90%|###############################################################9       | 9/10 [00:10<00:01,  1.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.354366\tvalid_1's binary_logloss: 0.390825\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.387184:  90%|###############################################################9       | 9/10 [00:11<00:01,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.387184: 100%|######################################################################| 10/10 [00:11<00:00,  1.22s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:16,427]\u001b[0m Trial 36 finished with value: 0.39910846263964217 and parameters: {'bagging_fraction': 0.9019995491355968, 'bagging_freq': 4}. Best is trial 33 with value: 0.3877453394118215.\u001b[0m\n",
      "bagging, val_score: 0.387184: 100%|######################################################################| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.361491\tvalid_1's binary_logloss: 0.399108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:   0%|                                                                | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:   0%|                                                                | 0/6 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  17%|#########3                                              | 1/6 [00:01<00:06,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:17,675]\u001b[0m Trial 37 finished with value: 0.39462899560618614 and parameters: {'feature_fraction': 0.552}. Best is trial 37 with value: 0.39462899560618614.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  17%|#########3                                              | 1/6 [00:01<00:06,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352859\tvalid_1's binary_logloss: 0.394629\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  17%|#########3                                              | 1/6 [00:02<00:06,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  33%|##################6                                     | 2/6 [00:02<00:05,  1.33s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:19,076]\u001b[0m Trial 38 finished with value: 0.39070811239776787 and parameters: {'feature_fraction': 0.6479999999999999}. Best is trial 38 with value: 0.39070811239776787.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  33%|##################6                                     | 2/6 [00:02<00:05,  1.33s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.348213\tvalid_1's binary_logloss: 0.390708\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  33%|##################6                                     | 2/6 [00:03<00:05,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  50%|############################                            | 3/6 [00:03<00:03,  1.30s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:20,335]\u001b[0m Trial 39 finished with value: 0.39556316657660107 and parameters: {'feature_fraction': 0.52}. Best is trial 38 with value: 0.39070811239776787.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  50%|############################                            | 3/6 [00:03<00:03,  1.30s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.347341\tvalid_1's binary_logloss: 0.395563\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  50%|############################                            | 3/6 [00:05<00:03,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  67%|#####################################3                  | 4/6 [00:05<00:02,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:21,464]\u001b[0m Trial 40 finished with value: 0.3939609798611426 and parameters: {'feature_fraction': 0.584}. Best is trial 38 with value: 0.39070811239776787.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  67%|#####################################3                  | 4/6 [00:05<00:02,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.361524\tvalid_1's binary_logloss: 0.393961\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  67%|#####################################3                  | 4/6 [00:06<00:02,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  83%|##############################################6         | 5/6 [00:06<00:01,  1.23s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:22,698]\u001b[0m Trial 41 finished with value: 0.39220786276390784 and parameters: {'feature_fraction': 0.6799999999999999}. Best is trial 38 with value: 0.39070811239776787.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  83%|##############################################6         | 5/6 [00:06<00:01,  1.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.366023\tvalid_1's binary_logloss: 0.392208\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184:  83%|##############################################6         | 5/6 [00:07<00:01,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.387184: 100%|########################################################| 6/6 [00:07<00:00,  1.27s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:24,038]\u001b[0m Trial 42 finished with value: 0.39351005406959616 and parameters: {'feature_fraction': 0.616}. Best is trial 38 with value: 0.39070811239776787.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.387184: 100%|########################################################| 6/6 [00:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.35459\tvalid_1's binary_logloss: 0.39351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387184:   0%|                                                                | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387184:   0%|                                                                | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387184:   5%|##8                                                     | 1/20 [00:01<00:35,  1.86s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:25,913]\u001b[0m Trial 43 finished with value: 0.3892803586305149 and parameters: {'lambda_l1': 0.0008866165022299193, 'lambda_l2': 8.314982681817135}. Best is trial 43 with value: 0.3892803586305149.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387184:   5%|##8                                                     | 1/20 [00:01<00:35,  1.86s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.335588\tvalid_1's binary_logloss: 0.38928\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387178:   5%|##8                                                     | 1/20 [00:03<00:35,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387178:  10%|#####6                                                  | 2/20 [00:03<00:28,  1.61s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:27,346]\u001b[0m Trial 44 finished with value: 0.3871775096468315 and parameters: {'lambda_l1': 0.010054954169240392, 'lambda_l2': 7.92703932849694e-06}. Best is trial 44 with value: 0.3871775096468315.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387178:  10%|#####6                                                  | 2/20 [00:03<00:28,  1.61s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352071\tvalid_1's binary_logloss: 0.387178\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  10%|#####6                                                  | 2/20 [00:04<00:28,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  15%|########4                                               | 3/20 [00:04<00:25,  1.50s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:28,720]\u001b[0m Trial 45 finished with value: 0.3871664599584862 and parameters: {'lambda_l1': 6.59308382247429e-05, 'lambda_l2': 8.862806639932555e-06}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  15%|########4                                               | 3/20 [00:04<00:25,  1.50s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387166\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  15%|########4                                               | 3/20 [00:06<00:25,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  20%|###########2                                            | 4/20 [00:06<00:23,  1.45s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:30,093]\u001b[0m Trial 46 finished with value: 0.38716648765295714 and parameters: {'lambda_l1': 5.407114243566522e-06, 'lambda_l2': 1.4783398284531251e-05}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  20%|###########2                                            | 4/20 [00:06<00:23,  1.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387166\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  20%|###########2                                            | 4/20 [00:07<00:23,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  25%|##############                                          | 5/20 [00:07<00:22,  1.51s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:31,713]\u001b[0m Trial 47 finished with value: 0.3871837979714515 and parameters: {'lambda_l1': 1.4585904415539556e-08, 'lambda_l2': 5.733947498876181e-07}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  25%|##############                                          | 5/20 [00:07<00:22,  1.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  25%|##############                                          | 5/20 [00:09<00:22,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  30%|################8                                       | 6/20 [00:09<00:21,  1.55s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:33,339]\u001b[0m Trial 48 finished with value: 0.39070842716848836 and parameters: {'lambda_l1': 2.5123706672641012e-08, 'lambda_l2': 8.500097260126799}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  30%|################8                                       | 6/20 [00:09<00:21,  1.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.353077\tvalid_1's binary_logloss: 0.390708\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  30%|################8                                       | 6/20 [00:10<00:21,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  35%|###################5                                    | 7/20 [00:10<00:19,  1.49s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:34,710]\u001b[0m Trial 49 finished with value: 0.38718292687126143 and parameters: {'lambda_l1': 0.0003034226590108639, 'lambda_l2': 0.0004010561912889589}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  35%|###################5                                    | 7/20 [00:10<00:19,  1.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352061\tvalid_1's binary_logloss: 0.387183\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  35%|###################5                                    | 7/20 [00:12<00:19,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  40%|######################4                                 | 8/20 [00:12<00:17,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:36,091]\u001b[0m Trial 50 finished with value: 0.38718243705093736 and parameters: {'lambda_l1': 0.0016763334972845177, 'lambda_l2': 0.00018476057200506152}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  40%|######################4                                 | 8/20 [00:12<00:17,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352062\tvalid_1's binary_logloss: 0.387182\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  40%|######################4                                 | 8/20 [00:13<00:17,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  45%|#########################2                              | 9/20 [00:13<00:16,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:37,573]\u001b[0m Trial 51 finished with value: 0.39142814210871085 and parameters: {'lambda_l1': 0.0011590122525491709, 'lambda_l2': 0.90270441784917}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  45%|#########################2                              | 9/20 [00:13<00:16,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.350564\tvalid_1's binary_logloss: 0.391428\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  45%|#########################2                              | 9/20 [00:14<00:16,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  50%|###########################5                           | 10/20 [00:14<00:14,  1.44s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:38,951]\u001b[0m Trial 52 finished with value: 0.3871820694224132 and parameters: {'lambda_l1': 0.0027678014595099547, 'lambda_l2': 3.248256108212266e-08}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  50%|###########################5                           | 10/20 [00:14<00:14,  1.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352063\tvalid_1's binary_logloss: 0.387182\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  50%|###########################5                           | 10/20 [00:16<00:14,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  55%|##############################2                        | 11/20 [00:16<00:13,  1.51s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:40,617]\u001b[0m Trial 53 finished with value: 0.3950591901522945 and parameters: {'lambda_l1': 8.560212944684656, 'lambda_l2': 0.006756123423568128}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  55%|##############################2                        | 11/20 [00:16<00:13,  1.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.344393\tvalid_1's binary_logloss: 0.395059\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  55%|##############################2                        | 11/20 [00:17<00:13,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  60%|#################################                      | 12/20 [00:17<00:11,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:41,979]\u001b[0m Trial 54 finished with value: 0.3871837867336048 and parameters: {'lambda_l1': 2.8557572378671665e-06, 'lambda_l2': 6.1583117455985835e-06}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  60%|#################################                      | 12/20 [00:17<00:11,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  60%|#################################                      | 12/20 [00:19<00:11,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  65%|###################################7                   | 13/20 [00:19<00:09,  1.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:43,214]\u001b[0m Trial 55 finished with value: 0.3871806556956294 and parameters: {'lambda_l1': 1.039056855310923e-05, 'lambda_l2': 0.0018451049212492086}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  65%|###################################7                   | 13/20 [00:19<00:09,  1.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352065\tvalid_1's binary_logloss: 0.387181\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  65%|###################################7                   | 13/20 [00:20<00:09,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  70%|######################################5                | 14/20 [00:20<00:08,  1.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:44,583]\u001b[0m Trial 56 finished with value: 0.3871837779124705 and parameters: {'lambda_l1': 2.9727813244634297e-06, 'lambda_l2': 1.1292752456255652e-05}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  70%|######################################5                | 14/20 [00:20<00:08,  1.39s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  70%|######################################5                | 14/20 [00:21<00:08,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  75%|#########################################2             | 15/20 [00:21<00:06,  1.36s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:45,865]\u001b[0m Trial 57 finished with value: 0.3914779287474018 and parameters: {'lambda_l1': 0.07735921254356083, 'lambda_l2': 1.0714494544420077e-08}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  75%|#########################################2             | 15/20 [00:21<00:06,  1.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.352629\tvalid_1's binary_logloss: 0.391478\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  75%|#########################################2             | 15/20 [00:23<00:06,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  80%|############################################           | 16/20 [00:23<00:05,  1.32s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:47,091]\u001b[0m Trial 58 finished with value: 0.3871837850949866 and parameters: {'lambda_l1': 2.1225552033396894e-05, 'lambda_l2': 3.677520204703331e-07}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  80%|############################################           | 16/20 [00:23<00:05,  1.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  80%|############################################           | 16/20 [00:24<00:05,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  85%|##############################################7        | 17/20 [00:24<00:03,  1.32s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:48,409]\u001b[0m Trial 59 finished with value: 0.3924807158025944 and parameters: {'lambda_l1': 2.0801807483843914e-07, 'lambda_l2': 0.028735932054457757}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  85%|##############################################7        | 17/20 [00:24<00:03,  1.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.361239\tvalid_1's binary_logloss: 0.392481\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  85%|##############################################7        | 17/20 [00:25<00:03,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  90%|#################################################5     | 18/20 [00:25<00:02,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:49,639]\u001b[0m Trial 60 finished with value: 0.3871822352255209 and parameters: {'lambda_l1': 6.309556105595765e-05, 'lambda_l2': 6.217795902486028e-05}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  90%|#################################################5     | 18/20 [00:25<00:02,  1.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387182\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  90%|#################################################5     | 18/20 [00:26<00:02,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  95%|####################################################2  | 19/20 [00:26<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:51,007]\u001b[0m Trial 61 finished with value: 0.38718379817315224 and parameters: {'lambda_l1': 5.737245136805639e-07, 'lambda_l2': 2.45791150497666e-07}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  95%|####################################################2  | 19/20 [00:26<00:01,  1.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.35206\tvalid_1's binary_logloss: 0.387184\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.387166:  95%|####################################################2  | 19/20 [00:28<00:01,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.387166: 100%|#######################################################| 20/20 [00:28<00:00,  1.39s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:52,576]\u001b[0m Trial 62 finished with value: 0.39032160347009187 and parameters: {'lambda_l1': 0.054723274249982086, 'lambda_l2': 1.6580657272318788e-06}. Best is trial 45 with value: 0.3871664599584862.\u001b[0m\n",
      "regularization_factors, val_score: 0.387166: 100%|#######################################################| 20/20 [00:28<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.339892\tvalid_1's binary_logloss: 0.390322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.387166:   0%|                                                                       | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:   0%|                                                                       | 0/5 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  20%|############6                                                  | 1/5 [00:01<00:04,  1.24s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:53,825]\u001b[0m Trial 63 finished with value: 0.3856929198789611 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.3856929198789611.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  20%|############6                                                  | 1/5 [00:01<00:04,  1.24s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.352476\tvalid_1's binary_logloss: 0.385693\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  20%|############6                                                  | 1/5 [00:02<00:04,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.27s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:55,123]\u001b[0m Trial 64 finished with value: 0.3943791832485365 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.3856929198789611.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.360617\tvalid_1's binary_logloss: 0.394379\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  40%|#########################2                                     | 2/5 [00:03<00:03,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.35s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:56,555]\u001b[0m Trial 65 finished with value: 0.3894239762843065 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.3856929198789611.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.349873\tvalid_1's binary_logloss: 0.389424\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  60%|#####################################8                         | 3/5 [00:05<00:02,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  80%|##################################################4            | 4/5 [00:05<00:01,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:57,753]\u001b[0m Trial 66 finished with value: 0.394184525287063 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.3856929198789611.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  80%|##################################################4            | 4/5 [00:05<00:01,  1.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.358667\tvalid_1's binary_logloss: 0.394185\n",
      "[LightGBM] [Info] Number of positive: 5758, number of negative: 33145\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.148009 -> initscore=-1.750302\n",
      "[LightGBM] [Info] Start training from score -1.750302\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693:  80%|##################################################4            | 4/5 [00:06<00:01,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.385693: 100%|###############################################################| 5/5 [00:06<00:00,  1.29s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:27:59,057]\u001b[0m Trial 67 finished with value: 0.39163256694055615 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.3856929198789611.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.385693: 100%|###############################################################| 5/5 [00:06<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.346777\tvalid_1's binary_logloss: 0.391633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 6.59308382247429e-05,\n",
       " 'lambda_l2': 8.862806639932555e-06,\n",
       " 'num_leaves': 43,\n",
       " 'feature_fraction': 0.6,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 100,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_rentai_train1, y_rentai_train1, X_rentai_valid1, y_rentai_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae79cfdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    "'lambda_l1': 6.59308382247429e-05,\n",
    " 'lambda_l2': 8.862806639932555e-06,\n",
    " 'num_leaves': 43,\n",
    " 'feature_fraction': 0.6,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 100,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "r_lgb_clf = fit(params, X_rentai_train1, y_rentai_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "32d6cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7249498715975286"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_me_valid = ModelEvaluator(r_lgb_clf, haitou, std=True)\n",
    "r_me_valid.score(y_rentai_valid1, X_rentai_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "568647b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：707 的中数:62 的中率:8.8%\n",
      "馬連賭金:70,700円 馬連配当合計:57,370円 馬連最高配当:6,190円 馬連回収率:81.1%\n",
      "馬単賭金:141,400円 馬単配当合計:111,900円 馬単最高配当:12,730円 馬単回収率:79.1%\n"
     ]
    }
   ],
   "source": [
    "fm = r_me_valid.pred_table(X_rentai_valid1, 0.5)\n",
    "fmr = wr[['h_num']].merge(fm[['h_num']], left_index=True, right_index=True)\n",
    "fmr = fmr[fmr['h_num_x'] != fmr['h_num_y']]\n",
    "umaren_return_rate(fmr, haitou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "fda6b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：238 的中数:13 的中率:5.5%\n",
      "馬連賭金:23,800円 馬連配当合計:9,020円 馬連最高配当:1,740円 馬連回収率:37.9%\n",
      "馬単賭金:47,600円 馬単配当合計:18,240円 馬単最高配当:4,070円 馬単回収率:38.3%\n"
     ]
    }
   ],
   "source": [
    "r_me_test = ModelEvaluator(r_lgb_clf, haitou, std=True)\n",
    "t_fm = r_me_test.pred_table(X_rentai_test2, 0.5)\n",
    "fmrt = twr[['h_num']].merge(t_fm[['h_num']], left_index=True, right_index=True)\n",
    "fmrt = fmrt[fmrt['h_num_x'] != fmrt['h_num_y']]\n",
    "\n",
    "umaren_return_rate(fmrt, haitou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9e32b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 複勝\n",
    "df = build_data('./pickle_new/shinba_base.pickle', './pickle_new/peds_vec.pickle', 4, vec)\n",
    "X_fukusho_train1 = df['X_train']\n",
    "y_fukusho_train1 = df['y_train']\n",
    "X_fukusho_valid1 = df['X_valid']\n",
    "y_fukusho_valid1 = df['y_valid']\n",
    "X_fukusho_test2 = df['X_test2']\n",
    "y_fukusho_test2 = df['y_test2']\n",
    "X_fukusho_test1 = df['X_test1']\n",
    "y_fukusho_test1 = df['y_test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "b4aaa819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 09:37:38,631]\u001b[0m A new study created in memory with name: no-name-c30ea940-27e5-4c71-8d30-96d86529b431\u001b[0m\n",
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.513959:   0%|                                                                       | 0/7 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.513959:  14%|#########                                                      | 1/7 [00:01<00:07,  1.21s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:39,844]\u001b[0m Trial 0 finished with value: 0.51395872477023 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.51395872477023.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.513959:  14%|#########                                                      | 1/7 [00:01<00:07,  1.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.471953\tvalid_1's binary_logloss: 0.513959\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.513959:  14%|#########                                                      | 1/7 [00:02<00:07,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.513959:  29%|##################                                             | 2/7 [00:02<00:06,  1.26s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:41,141]\u001b[0m Trial 1 finished with value: 0.5143719733472065 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.51395872477023.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.513959:  29%|##################                                             | 2/7 [00:02<00:06,  1.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.469698\tvalid_1's binary_logloss: 0.514372\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  29%|##################                                             | 2/7 [00:03<00:06,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  43%|###########################                                    | 3/7 [00:03<00:04,  1.16s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:42,182]\u001b[0m Trial 2 finished with value: 0.5068797467716084 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.5068797467716084.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  43%|###########################                                    | 3/7 [00:03<00:04,  1.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.471314\tvalid_1's binary_logloss: 0.50688\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  43%|###########################                                    | 3/7 [00:04<00:04,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  57%|####################################                           | 4/7 [00:04<00:03,  1.16s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:43,345]\u001b[0m Trial 3 finished with value: 0.519340676463741 and parameters: {'feature_fraction': 1.0}. Best is trial 2 with value: 0.5068797467716084.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  57%|####################################                           | 4/7 [00:04<00:03,  1.16s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.480824\tvalid_1's binary_logloss: 0.519341\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  57%|####################################                           | 4/7 [00:05<00:03,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  71%|#############################################                  | 5/7 [00:05<00:02,  1.19s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:44,574]\u001b[0m Trial 4 finished with value: 0.5094075600712527 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.5068797467716084.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  71%|#############################################                  | 5/7 [00:05<00:02,  1.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.468119\tvalid_1's binary_logloss: 0.509408\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  71%|#############################################                  | 5/7 [00:07<00:02,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  86%|######################################################         | 6/7 [00:07<00:01,  1.24s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:45,921]\u001b[0m Trial 5 finished with value: 0.5134934286104336 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 0.5068797467716084.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  86%|######################################################         | 6/7 [00:07<00:01,  1.24s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.472434\tvalid_1's binary_logloss: 0.513493\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction, val_score: 0.506880:  86%|######################################################         | 6/7 [00:08<00:01,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction, val_score: 0.506880: 100%|###############################################################| 7/7 [00:08<00:00,  1.25s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:47,193]\u001b[0m Trial 6 finished with value: 0.5105789592880338 and parameters: {'feature_fraction': 0.5}. Best is trial 2 with value: 0.5068797467716084.\u001b[0m\n",
      "feature_fraction, val_score: 0.506880: 100%|###############################################################| 7/7 [00:08<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.468421\tvalid_1's binary_logloss: 0.510579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.506880:   0%|                                                                            | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.506880:   0%|                                                                            | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.506880:   5%|###4                                                                | 1/20 [00:01<00:34,  1.81s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:49,017]\u001b[0m Trial 7 finished with value: 0.5125892646387579 and parameters: {'num_leaves': 210}. Best is trial 7 with value: 0.5125892646387579.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.506880:   5%|###4                                                                | 1/20 [00:01<00:34,  1.81s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.410331\tvalid_1's binary_logloss: 0.512589\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:   5%|###4                                                                | 1/20 [00:02<00:34,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  10%|######8                                                             | 2/20 [00:02<00:24,  1.36s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:50,062]\u001b[0m Trial 8 finished with value: 0.5029954690436886 and parameters: {'num_leaves': 19}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  10%|######8                                                             | 2/20 [00:02<00:24,  1.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.477704\tvalid_1's binary_logloss: 0.502995\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  10%|######8                                                             | 2/20 [00:04<00:24,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  15%|##########2                                                         | 3/20 [00:04<00:24,  1.42s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:51,540]\u001b[0m Trial 9 finished with value: 0.5171624043558594 and parameters: {'num_leaves': 239}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  15%|##########2                                                         | 3/20 [00:04<00:24,  1.42s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.459851\tvalid_1's binary_logloss: 0.517162\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  15%|##########2                                                         | 3/20 [00:06<00:24,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  20%|#############6                                                      | 4/20 [00:06<00:25,  1.58s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:53,376]\u001b[0m Trial 10 finished with value: 0.5167629593939853 and parameters: {'num_leaves': 243}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  20%|#############6                                                      | 4/20 [00:06<00:25,  1.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.423731\tvalid_1's binary_logloss: 0.516763\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  20%|#############6                                                      | 4/20 [00:07<00:25,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  25%|#################                                                   | 5/20 [00:07<00:24,  1.65s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:55,152]\u001b[0m Trial 11 finished with value: 0.509765718448088 and parameters: {'num_leaves': 219}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  25%|#################                                                   | 5/20 [00:07<00:24,  1.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.408251\tvalid_1's binary_logloss: 0.509766\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  25%|#################                                                   | 5/20 [00:09<00:24,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  30%|####################4                                               | 6/20 [00:09<00:23,  1.65s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:56,804]\u001b[0m Trial 12 finished with value: 0.5171624043558594 and parameters: {'num_leaves': 239}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  30%|####################4                                               | 6/20 [00:09<00:23,  1.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.459851\tvalid_1's binary_logloss: 0.517162\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  30%|####################4                                               | 6/20 [00:11<00:23,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  35%|#######################7                                            | 7/20 [00:11<00:20,  1.57s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:58,219]\u001b[0m Trial 13 finished with value: 0.5093348454362675 and parameters: {'num_leaves': 172}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  35%|#######################7                                            | 7/20 [00:11<00:20,  1.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.439679\tvalid_1's binary_logloss: 0.509335\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  35%|#######################7                                            | 7/20 [00:12<00:20,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.502995:  40%|###########################2                                        | 8/20 [00:12<00:17,  1.46s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:37:59,447]\u001b[0m Trial 14 finished with value: 0.5176883644417931 and parameters: {'num_leaves': 249}. Best is trial 8 with value: 0.5029954690436886.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.502995:  40%|###########################2                                        | 8/20 [00:12<00:17,  1.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.510645\tvalid_1's binary_logloss: 0.517688\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.500808:  40%|###########################2                                        | 8/20 [00:13<00:17,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.500808:  45%|##############################6                                     | 9/20 [00:13<00:14,  1.32s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:00,438]\u001b[0m Trial 15 finished with value: 0.5008078034721838 and parameters: {'num_leaves': 7}. Best is trial 15 with value: 0.5008078034721838.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.500808:  45%|##############################6                                     | 9/20 [00:13<00:14,  1.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.472374\tvalid_1's binary_logloss: 0.500808\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.500808:  45%|##############################6                                     | 9/20 [00:14<00:14,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.500808:  50%|#################################5                                 | 10/20 [00:14<00:13,  1.38s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:01,957]\u001b[0m Trial 16 finished with value: 0.5093348454362675 and parameters: {'num_leaves': 172}. Best is trial 15 with value: 0.5008078034721838.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.500808:  50%|#################################5                                 | 10/20 [00:14<00:13,  1.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.439679\tvalid_1's binary_logloss: 0.509335\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  50%|#################################5                                 | 10/20 [00:15<00:13,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  55%|####################################8                              | 11/20 [00:15<00:11,  1.27s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:02,988]\u001b[0m Trial 17 finished with value: 0.4986223655759058 and parameters: {'num_leaves': 11}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  55%|####################################8                              | 11/20 [00:15<00:11,  1.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.466701\tvalid_1's binary_logloss: 0.498622\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  55%|####################################8                              | 11/20 [00:16<00:11,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  60%|########################################1                          | 12/20 [00:16<00:09,  1.20s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:04,035]\u001b[0m Trial 18 finished with value: 0.5058584453735522 and parameters: {'num_leaves': 26}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  60%|########################################1                          | 12/20 [00:16<00:09,  1.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.460458\tvalid_1's binary_logloss: 0.505858\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  60%|########################################1                          | 12/20 [00:17<00:09,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  65%|###########################################5                       | 13/20 [00:17<00:08,  1.18s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:05,168]\u001b[0m Trial 19 finished with value: 0.5038591235161635 and parameters: {'num_leaves': 68}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  65%|###########################################5                       | 13/20 [00:17<00:08,  1.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.456011\tvalid_1's binary_logloss: 0.503859\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  65%|###########################################5                       | 13/20 [00:19<00:08,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  70%|##############################################9                    | 14/20 [00:19<00:07,  1.18s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:06,358]\u001b[0m Trial 20 finished with value: 0.5025353927953486 and parameters: {'num_leaves': 77}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  70%|##############################################9                    | 14/20 [00:19<00:07,  1.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.449155\tvalid_1's binary_logloss: 0.502535\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  70%|##############################################9                    | 14/20 [00:20<00:07,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  75%|##################################################2                | 15/20 [00:20<00:05,  1.17s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:07,502]\u001b[0m Trial 21 finished with value: 0.5012390681642991 and parameters: {'num_leaves': 64}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  75%|##################################################2                | 15/20 [00:20<00:05,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.454577\tvalid_1's binary_logloss: 0.501239\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  75%|##################################################2                | 15/20 [00:21<00:05,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  80%|#####################################################6             | 16/20 [00:21<00:04,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:08,438]\u001b[0m Trial 22 finished with value: 0.5008078034721838 and parameters: {'num_leaves': 7}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  80%|#####################################################6             | 16/20 [00:21<00:04,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.472374\tvalid_1's binary_logloss: 0.500808\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  80%|#####################################################6             | 16/20 [00:22<00:04,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  85%|########################################################9          | 17/20 [00:22<00:03,  1.14s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:09,668]\u001b[0m Trial 23 finished with value: 0.505840338075511 and parameters: {'num_leaves': 102}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  85%|########################################################9          | 17/20 [00:22<00:03,  1.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.448479\tvalid_1's binary_logloss: 0.50584\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  85%|########################################################9          | 17/20 [00:23<00:03,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  90%|############################################################3      | 18/20 [00:23<00:02,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:10,679]\u001b[0m Trial 24 finished with value: 0.506086235270843 and parameters: {'num_leaves': 42}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  90%|############################################################3      | 18/20 [00:23<00:02,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.466415\tvalid_1's binary_logloss: 0.506086\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  90%|############################################################3      | 18/20 [00:24<00:02,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622:  95%|###############################################################6   | 19/20 [00:24<00:01,  1.12s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:11,839]\u001b[0m Trial 25 finished with value: 0.5105722238518535 and parameters: {'num_leaves': 112}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  95%|###############################################################6   | 19/20 [00:24<00:01,  1.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.465261\tvalid_1's binary_logloss: 0.510572\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "num_leaves, val_score: 0.498622:  95%|###############################################################6   | 19/20 [00:25<00:01,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "num_leaves, val_score: 0.498622: 100%|###################################################################| 20/20 [00:25<00:00,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:12,881]\u001b[0m Trial 26 finished with value: 0.506086235270843 and parameters: {'num_leaves': 42}. Best is trial 17 with value: 0.4986223655759058.\u001b[0m\n",
      "num_leaves, val_score: 0.498622: 100%|###################################################################| 20/20 [00:25<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.466415\tvalid_1's binary_logloss: 0.506086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:   0%|                                                                               | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:   0%|                                                                               | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  10%|#######1                                                               | 1/10 [00:00<00:08,  1.04it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:13,853]\u001b[0m Trial 27 finished with value: 0.5029893715690007 and parameters: {'bagging_fraction': 0.7705467630972831, 'bagging_freq': 7}. Best is trial 27 with value: 0.5029893715690007.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  10%|#######1                                                               | 1/10 [00:00<00:08,  1.04it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.464453\tvalid_1's binary_logloss: 0.502989\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  10%|#######1                                                               | 1/10 [00:02<00:08,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  20%|##############2                                                        | 2/10 [00:02<00:08,  1.01s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:14,895]\u001b[0m Trial 28 finished with value: 0.5032379155897103 and parameters: {'bagging_fraction': 0.7464673390160204, 'bagging_freq': 5}. Best is trial 27 with value: 0.5029893715690007.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  20%|##############2                                                        | 2/10 [00:02<00:08,  1.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.45845\tvalid_1's binary_logloss: 0.503238\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  20%|##############2                                                        | 2/10 [00:02<00:08,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  30%|#####################3                                                 | 3/10 [00:02<00:06,  1.05it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:15,777]\u001b[0m Trial 29 finished with value: 0.5007917333080415 and parameters: {'bagging_fraction': 0.9422382364130405, 'bagging_freq': 6}. Best is trial 29 with value: 0.5007917333080415.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  30%|#####################3                                                 | 3/10 [00:02<00:06,  1.05it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.481201\tvalid_1's binary_logloss: 0.500792\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  30%|#####################3                                                 | 3/10 [00:03<00:06,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  40%|############################4                                          | 4/10 [00:03<00:05,  1.04it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:16,761]\u001b[0m Trial 30 finished with value: 0.5024480855163592 and parameters: {'bagging_fraction': 0.7925094761445364, 'bagging_freq': 2}. Best is trial 29 with value: 0.5007917333080415.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  40%|############################4                                          | 4/10 [00:03<00:05,  1.04it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.464454\tvalid_1's binary_logloss: 0.502448\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  40%|############################4                                          | 4/10 [00:04<00:05,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  50%|###################################5                                   | 5/10 [00:04<00:04,  1.04it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:17,717]\u001b[0m Trial 31 finished with value: 0.4992383393501547 and parameters: {'bagging_fraction': 0.9928283957325778, 'bagging_freq': 4}. Best is trial 31 with value: 0.4992383393501547.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  50%|###################################5                                   | 5/10 [00:04<00:04,  1.04it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.47137\tvalid_1's binary_logloss: 0.499238\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  50%|###################################5                                   | 5/10 [00:05<00:04,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498622:  60%|##########################################6                            | 6/10 [00:05<00:03,  1.08it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:18,569]\u001b[0m Trial 32 finished with value: 0.5053996145117096 and parameters: {'bagging_fraction': 0.6460372192821763, 'bagging_freq': 7}. Best is trial 31 with value: 0.4992383393501547.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498622:  60%|##########################################6                            | 6/10 [00:05<00:03,  1.08it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.483126\tvalid_1's binary_logloss: 0.5054\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498575:  60%|##########################################6                            | 6/10 [00:06<00:03,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498575:  70%|#################################################6                     | 7/10 [00:06<00:02,  1.09it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:19,466]\u001b[0m Trial 33 finished with value: 0.4985753720883653 and parameters: {'bagging_fraction': 0.7423951320188933, 'bagging_freq': 4}. Best is trial 33 with value: 0.4985753720883653.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498575:  70%|#################################################6                     | 7/10 [00:06<00:02,  1.09it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.476499\tvalid_1's binary_logloss: 0.498575\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498331:  70%|#################################################6                     | 7/10 [00:07<00:02,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498331:  80%|########################################################8              | 8/10 [00:07<00:01,  1.08it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:20,424]\u001b[0m Trial 34 finished with value: 0.4983309999374369 and parameters: {'bagging_fraction': 0.9493683214443642, 'bagging_freq': 3}. Best is trial 34 with value: 0.4983309999374369.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498331:  80%|########################################################8              | 8/10 [00:07<00:01,  1.08it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.471545\tvalid_1's binary_logloss: 0.498331\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.498331:  80%|########################################################8              | 8/10 [00:08<00:01,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.498331:  90%|###############################################################9       | 9/10 [00:08<00:00,  1.06it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:21,408]\u001b[0m Trial 35 finished with value: 0.5030204210227899 and parameters: {'bagging_fraction': 0.8370553157406617, 'bagging_freq': 6}. Best is trial 34 with value: 0.4983309999374369.\u001b[0m\n",
      "\n",
      "\n",
      "bagging, val_score: 0.498331:  90%|###############################################################9       | 9/10 [00:08<00:00,  1.06it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.46443\tvalid_1's binary_logloss: 0.50302\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bagging, val_score: 0.496966:  90%|###############################################################9       | 9/10 [00:09<00:00,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "bagging, val_score: 0.496966: 100%|######################################################################| 10/10 [00:09<00:00,  1.02it/s]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:22,469]\u001b[0m Trial 36 finished with value: 0.4969663414968572 and parameters: {'bagging_fraction': 0.5297970154731599, 'bagging_freq': 4}. Best is trial 36 with value: 0.4969663414968572.\u001b[0m\n",
      "bagging, val_score: 0.496966: 100%|######################################################################| 10/10 [00:09<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's binary_logloss: 0.454573\tvalid_1's binary_logloss: 0.496966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.496966:   0%|                                                                | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.496966:   0%|                                                                | 0/3 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.496966:  33%|##################6                                     | 1/3 [00:01<00:02,  1.07s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:23,549]\u001b[0m Trial 37 finished with value: 0.5000982404616019 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 0.5000982404616019.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.496966:  33%|##################6                                     | 1/3 [00:01<00:02,  1.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457037\tvalid_1's binary_logloss: 0.500098\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.492694:  33%|##################6                                     | 1/3 [00:02<00:02,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.492694:  67%|#####################################3                  | 2/3 [00:02<00:01,  1.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:24,685]\u001b[0m Trial 38 finished with value: 0.492693951127877 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 38 with value: 0.492693951127877.\u001b[0m\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.492694:  67%|#####################################3                  | 2/3 [00:02<00:01,  1.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.451777\tvalid_1's binary_logloss: 0.492694\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.492694:  67%|#####################################3                  | 2/3 [00:03<00:01,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.492694: 100%|########################################################| 3/3 [00:03<00:00,  1.05s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:25,670]\u001b[0m Trial 39 finished with value: 0.4967419247946353 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 38 with value: 0.492693951127877.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.492694: 100%|########################################################| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.462482\tvalid_1's binary_logloss: 0.496742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                             | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492694:   0%|                                                                | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:   0%|                                                                | 0/20 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:   5%|##8                                                     | 1/20 [00:01<00:21,  1.14s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:26,822]\u001b[0m Trial 40 finished with value: 0.49259842549610594 and parameters: {'lambda_l1': 0.0037341833907004714, 'lambda_l2': 8.766254944268345e-06}. Best is trial 40 with value: 0.49259842549610594.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:   5%|##8                                                     | 1/20 [00:01<00:21,  1.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451385\tvalid_1's binary_logloss: 0.492598\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:   5%|##8                                                     | 1/20 [00:02<00:21,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  10%|#####6                                                  | 2/20 [00:02<00:19,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:27,886]\u001b[0m Trial 41 finished with value: 0.494824144240539 and parameters: {'lambda_l1': 1.5067092928000082e-06, 'lambda_l2': 5.224500887709913e-06}. Best is trial 40 with value: 0.49259842549610594.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  10%|#####6                                                  | 2/20 [00:02<00:19,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457645\tvalid_1's binary_logloss: 0.494824\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  10%|#####6                                                  | 2/20 [00:03<00:19,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  15%|########4                                               | 3/20 [00:03<00:19,  1.12s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:29,035]\u001b[0m Trial 42 finished with value: 0.4925983755612147 and parameters: {'lambda_l1': 0.0008968994589519624, 'lambda_l2': 0.00028174933897974074}. Best is trial 42 with value: 0.4925983755612147.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  15%|########4                                               | 3/20 [00:03<00:19,  1.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451384\tvalid_1's binary_logloss: 0.492598\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  15%|########4                                               | 3/20 [00:04<00:19,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  20%|###########2                                            | 4/20 [00:04<00:18,  1.13s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:30,190]\u001b[0m Trial 43 finished with value: 0.49259830117356734 and parameters: {'lambda_l1': 1.0540212292036327e-06, 'lambda_l2': 0.0038889009848536125}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  20%|###########2                                            | 4/20 [00:04<00:18,  1.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451385\tvalid_1's binary_logloss: 0.492598\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  20%|###########2                                            | 4/20 [00:05<00:18,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  25%|##############                                          | 5/20 [00:05<00:17,  1.15s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:31,357]\u001b[0m Trial 44 finished with value: 0.4934564645926595 and parameters: {'lambda_l1': 6.334256801065898, 'lambda_l2': 1.2730028184969986e-05}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  25%|##############                                          | 5/20 [00:05<00:17,  1.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.453111\tvalid_1's binary_logloss: 0.493456\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  25%|##############                                          | 5/20 [00:06<00:17,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  30%|################8                                       | 6/20 [00:06<00:15,  1.12s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:32,425]\u001b[0m Trial 45 finished with value: 0.49706608707626243 and parameters: {'lambda_l1': 3.2804716293209104, 'lambda_l2': 5.175893467236281}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  30%|################8                                       | 6/20 [00:06<00:15,  1.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.458069\tvalid_1's binary_logloss: 0.497066\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  30%|################8                                       | 6/20 [00:07<00:15,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  35%|###################5                                    | 7/20 [00:07<00:14,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:33,497]\u001b[0m Trial 46 finished with value: 0.49657839071944493 and parameters: {'lambda_l1': 1.7160898613867799, 'lambda_l2': 7.706162574192265e-06}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  35%|###################5                                    | 7/20 [00:07<00:14,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.458085\tvalid_1's binary_logloss: 0.496578\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  35%|###################5                                    | 7/20 [00:08<00:14,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  40%|######################4                                 | 8/20 [00:08<00:13,  1.12s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:34,661]\u001b[0m Trial 47 finished with value: 0.4925985126635042 and parameters: {'lambda_l1': 0.00925378337891499, 'lambda_l2': 0.00010124127135000476}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  40%|######################4                                 | 8/20 [00:08<00:13,  1.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451387\tvalid_1's binary_logloss: 0.492599\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  40%|######################4                                 | 8/20 [00:10<00:13,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  45%|#########################2                              | 9/20 [00:10<00:12,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:35,718]\u001b[0m Trial 48 finished with value: 0.4948241441249636 and parameters: {'lambda_l1': 1.1037697929202378e-06, 'lambda_l2': 3.1241171122812036e-07}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  45%|#########################2                              | 9/20 [00:10<00:12,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457645\tvalid_1's binary_logloss: 0.494824\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  45%|#########################2                              | 9/20 [00:11<00:12,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  50%|###########################5                           | 10/20 [00:11<00:10,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:36,790]\u001b[0m Trial 49 finished with value: 0.4960355195834047 and parameters: {'lambda_l1': 1.2211597072870697, 'lambda_l2': 1.7399716804410915e-07}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  50%|###########################5                           | 10/20 [00:11<00:10,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.458328\tvalid_1's binary_logloss: 0.496036\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  50%|###########################5                           | 10/20 [00:12<00:10,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  55%|##############################2                        | 11/20 [00:12<00:09,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:37,878]\u001b[0m Trial 50 finished with value: 0.4950428197773051 and parameters: {'lambda_l1': 1.1754679277646765e-08, 'lambda_l2': 0.07150302813517316}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  55%|##############################2                        | 11/20 [00:12<00:09,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.456622\tvalid_1's binary_logloss: 0.495043\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  55%|##############################2                        | 11/20 [00:13<00:09,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  60%|#################################                      | 12/20 [00:13<00:08,  1.12s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:39,049]\u001b[0m Trial 51 finished with value: 0.4925983077804765 and parameters: {'lambda_l1': 2.3797813288619324e-05, 'lambda_l2': 0.003510124147492787}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  60%|#################################                      | 12/20 [00:13<00:08,  1.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451385\tvalid_1's binary_logloss: 0.492598\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  60%|#################################                      | 12/20 [00:14<00:08,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  65%|###################################7                   | 13/20 [00:14<00:07,  1.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:40,156]\u001b[0m Trial 52 finished with value: 0.49491664117687756 and parameters: {'lambda_l1': 1.1316575742385512e-05, 'lambda_l2': 0.014540489409459229}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  65%|###################################7                   | 13/20 [00:14<00:07,  1.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.456124\tvalid_1's binary_logloss: 0.494917\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  65%|###################################7                   | 13/20 [00:15<00:07,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  70%|######################################5                | 14/20 [00:15<00:06,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:41,237]\u001b[0m Trial 53 finished with value: 0.4949167260650425 and parameters: {'lambda_l1': 3.7343753258010245e-05, 'lambda_l2': 0.013246923813862787}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  70%|######################################5                | 14/20 [00:15<00:06,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.456124\tvalid_1's binary_logloss: 0.494917\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  70%|######################################5                | 14/20 [00:16<00:06,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  75%|#########################################2             | 15/20 [00:16<00:05,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:42,297]\u001b[0m Trial 54 finished with value: 0.4950167828011999 and parameters: {'lambda_l1': 2.2013268212567695e-08, 'lambda_l2': 0.3057683039445158}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  75%|#########################################2             | 15/20 [00:16<00:05,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.458082\tvalid_1's binary_logloss: 0.495017\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  75%|#########################################2             | 15/20 [00:17<00:05,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  80%|############################################           | 16/20 [00:17<00:04,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:43,382]\u001b[0m Trial 55 finished with value: 0.49482393393035096 and parameters: {'lambda_l1': 2.8048593622345324e-07, 'lambda_l2': 0.0015013823426511688}. Best is trial 43 with value: 0.49259830117356734.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  80%|############################################           | 16/20 [00:17<00:04,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457645\tvalid_1's binary_logloss: 0.494824\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  80%|############################################           | 16/20 [00:18<00:04,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  85%|##############################################7        | 17/20 [00:18<00:03,  1.11s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:44,533]\u001b[0m Trial 56 finished with value: 0.4925983004940676 and parameters: {'lambda_l1': 3.640904686873066e-05, 'lambda_l2': 0.003963059756688013}. Best is trial 56 with value: 0.4925983004940676.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  85%|##############################################7        | 17/20 [00:18<00:03,  1.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.451386\tvalid_1's binary_logloss: 0.492598\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  85%|##############################################7        | 17/20 [00:19<00:03,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  90%|#################################################5     | 18/20 [00:19<00:02,  1.09s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:45,575]\u001b[0m Trial 57 finished with value: 0.49686763706923437 and parameters: {'lambda_l1': 0.03280262160679251, 'lambda_l2': 0.5445956749707008}. Best is trial 56 with value: 0.4925983004940676.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  90%|#################################################5     | 18/20 [00:19<00:02,  1.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.459945\tvalid_1's binary_logloss: 0.496868\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  90%|#################################################5     | 18/20 [00:20<00:02,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  95%|####################################################2  | 19/20 [00:20<00:01,  1.08s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:46,652]\u001b[0m Trial 58 finished with value: 0.49482414273924097 and parameters: {'lambda_l1': 0.00018713490960522444, 'lambda_l2': 1.3327447822996377e-08}. Best is trial 56 with value: 0.4925983004940676.\u001b[0m\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  95%|####################################################2  | 19/20 [00:20<00:01,  1.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457645\tvalid_1's binary_logloss: 0.494824\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regularization_factors, val_score: 0.492598:  95%|####################################################2  | 19/20 [00:22<00:01,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "regularization_factors, val_score: 0.492598: 100%|#######################################################| 20/20 [00:22<00:00,  1.08s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:47,709]\u001b[0m Trial 59 finished with value: 0.49482392146449655 and parameters: {'lambda_l1': 1.2358394856391743e-07, 'lambda_l2': 0.0009216120563662283}. Best is trial 56 with value: 0.4925983004940676.\u001b[0m\n",
      "regularization_factors, val_score: 0.492598: 100%|#######################################################| 20/20 [00:22<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.457645\tvalid_1's binary_logloss: 0.494824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                              | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:   0%|                                                                       | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:   0%|                                                                       | 0/5 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  20%|############6                                                  | 1/5 [00:01<00:04,  1.04s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:48,755]\u001b[0m Trial 60 finished with value: 0.4952446680149182 and parameters: {'min_child_samples': 10}. Best is trial 60 with value: 0.4952446680149182.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  20%|############6                                                  | 1/5 [00:01<00:04,  1.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.458474\tvalid_1's binary_logloss: 0.495245\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  20%|############6                                                  | 1/5 [00:02<00:04,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.10s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:49,900]\u001b[0m Trial 61 finished with value: 0.49507773416997036 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.49507773416997036.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  40%|#########################2                                     | 2/5 [00:02<00:03,  1.10s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.458023\tvalid_1's binary_logloss: 0.495078\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  40%|#########################2                                     | 2/5 [00:03<00:03,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.08s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:50,962]\u001b[0m Trial 62 finished with value: 0.49828121726108876 and parameters: {'min_child_samples': 50}. Best is trial 61 with value: 0.49507773416997036.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  60%|#####################################8                         | 3/5 [00:03<00:02,  1.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.459634\tvalid_1's binary_logloss: 0.498281\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  60%|#####################################8                         | 3/5 [00:04<00:02,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  80%|##################################################4            | 4/5 [00:04<00:01,  1.06s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:51,999]\u001b[0m Trial 63 finished with value: 0.49800722881893783 and parameters: {'min_child_samples': 100}. Best is trial 61 with value: 0.49507773416997036.\u001b[0m\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  80%|##################################################4            | 4/5 [00:04<00:01,  1.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.459739\tvalid_1's binary_logloss: 0.498007\n",
      "[LightGBM] [Info] Number of positive: 8626, number of negative: 30277\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36823\n",
      "[LightGBM] [Info] Number of data points in the train set: 38903, number of used features: 238\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221731 -> initscore=-1.255607\n",
      "[LightGBM] [Info] Start training from score -1.255607\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598:  80%|##################################################4            | 4/5 [00:05<00:01,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "min_data_in_leaf, val_score: 0.492598: 100%|###############################################################| 5/5 [00:05<00:00,  1.06s/it]\u001b[A\u001b[A\u001b[32m[I 2022-10-27 09:38:53,054]\u001b[0m Trial 64 finished with value: 0.49545534714531403 and parameters: {'min_child_samples': 5}. Best is trial 61 with value: 0.49507773416997036.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.492598: 100%|###############################################################| 5/5 [00:05<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.458517\tvalid_1's binary_logloss: 0.495455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 3.640904686873066e-05,\n",
       " 'lambda_l2': 0.003963059756688013,\n",
       " 'num_leaves': 11,\n",
       " 'feature_fraction': 0.41600000000000004,\n",
       " 'bagging_fraction': 0.5297970154731599,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_fukusho_train1, y_fukusho_train1, X_fukusho_valid1, y_fukusho_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6937fc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 3.640904686873066e-05,\n",
    " 'lambda_l2': 0.003963059756688013,\n",
    " 'num_leaves': 11,\n",
    " 'feature_fraction': 0.41600000000000004,\n",
    " 'bagging_fraction': 0.5297970154731599,\n",
    " 'bagging_freq': 4,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "f_lgb_clf = fit(params, X_fukusho_train1, y_fukusho_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "3e36307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7289507372280624"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_me_valid = ModelEvaluator(f_lgb_clf, haitou, std=True)\n",
    "f_me_valid.score(y_fukusho_valid1, X_fukusho_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "ad86e188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1488 的中数:123 的中率:8.3%\n",
      "馬連賭金:148,800円 馬連配当合計:181,760円 馬連最高配当:41,660円 馬連回収率:122.2%\n",
      "馬単賭金:297,600円 馬単配当合計:374,990円 馬単最高配当:86,120円 馬単回収率:126.0%\n",
      "点数：2472 的中数:76 的中率:3.1%\n",
      "3連複賭金:247,200円 3連複配当合計:149,380円 3連複最高配当:8,750円 3連複回収率:60.4%\n",
      "3連単賭金:1,483,200円 3連単配当合計:776,520円 3連単最高配当:45,760円 3連単回収率:52.4%\n",
      "ワイド点数：3756 ワイド賭金:375,600円 ワイド配当合計:317,410円 ワイド的中数:482 ワイド的中率:12.8% ワイド回収率:84.5%\n"
     ]
    }
   ],
   "source": [
    "rfm = f_me_valid.pred_table(X_fukusho_valid1, 0.5)\n",
    "\n",
    "fmr = wr[['h_num']].merge(rfm[['h_num']], left_index=True, right_index=True)\n",
    "fmr = fmr[fmr['h_num_x'] != fmr['h_num_y']]\n",
    "umaren_return_rate(fmr, haitou)\n",
    "\n",
    "sanren = fmr[['h_num_x', 'h_num_y']].merge(rfm, left_index=True, right_index=True)\n",
    "uma3 = sanren[((sanren['h_num'] != sanren['h_num_y']) & (sanren['h_num'] != sanren['h_num_x']))]\n",
    "uma3 = uma3.merge(haitou[[\n",
    "    '1着馬番', '2着馬番', '3着馬番', '3連複', '3連単',\n",
    "    \"wide1_uma1\", \"wide1_uma2\", \"wide2_uma1\", \"wide2_uma2\", \"wide3_uma1\", \"wide3_uma2\", \"wide4_uma1\", \"wide4_uma2\", \"wide5_uma1\", \"wide5_uma2\", \"wide6_uma1\", \"wide6_uma2\", \"wide7_uma1\", \"wide7_uma2\",\n",
    "    \"wide_1\", \"wide_2\", \"wide_3\", \"wide_4\", \"wide_5\", \"wide_6\", \"wide_7\"\n",
    "]], on='race_id')\n",
    "\n",
    "sanrenkei(uma3)\n",
    "wide_rate(uma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "15569110",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：631 的中数:34 的中率:5.4%\n",
      "馬連賭金:63,100円 馬連配当合計:49,450円 馬連最高配当:9,110円 馬連回収率:78.4%\n",
      "馬単賭金:126,200円 馬単配当合計:106,920円 馬単最高配当:27,470円 馬単回収率:84.7%\n",
      "点数：443 的中数:6 的中率:1.4%\n",
      "3連複賭金:44,300円 3連複配当合計:12,720円 3連複最高配当:3,750円 3連複回収率:28.7%\n",
      "3連単賭金:265,800円 3連単配当合計:49,220円 3連単最高配当:11,220円 3連単回収率:18.5%\n",
      "ワイド点数：926 ワイド賭金:92,600円 ワイド配当合計:48,340円 ワイド的中数:86 ワイド的中率:9.3% ワイド回収率:52.2%\n"
     ]
    }
   ],
   "source": [
    "f_me_test = ModelEvaluator(f_lgb_clf, haitou, std=True)\n",
    "t_rfm = f_me_test.pred_table(X_fukusho_test2, 0.5)\n",
    "\n",
    "t_fmr = twr[['h_num']].merge(t_rfm[['h_num']], left_index=True, right_index=True)\n",
    "t_fmr = t_fmr[t_fmr['h_num_x'] != t_fmr['h_num_y']]\n",
    "umaren_return_rate(t_fmr, haitou)\n",
    "\n",
    "t_sanren = fmrt[['h_num_x', 'h_num_y']].merge(t_rfm, left_index=True, right_index=True)\n",
    "uma3 = t_sanren[((t_sanren['h_num'] != t_sanren['h_num_y']) & (t_sanren['h_num'] != t_sanren['h_num_x']))]\n",
    "uma3 = uma3.merge(haitou[[\n",
    "    '1着馬番', '2着馬番', '3着馬番', '3連複', '3連単',\n",
    "    \"wide1_uma1\", \"wide1_uma2\", \"wide2_uma1\", \"wide2_uma2\", \"wide3_uma1\", \"wide3_uma2\", \"wide4_uma1\", \"wide4_uma2\", \"wide5_uma1\", \"wide5_uma2\", \"wide6_uma1\", \"wide6_uma2\", \"wide7_uma1\", \"wide7_uma2\",\n",
    "    \"wide_1\", \"wide_2\", \"wide_3\", \"wide_4\", \"wide_5\", \"wide_6\", \"wide_7\"\n",
    "]], on='race_id')\n",
    "\n",
    "sanrenkei(uma3)\n",
    "wide_rate(uma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebd2a70d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_34096/3198004530.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t_bt['place'] = t_bt['place_id'].map(lambda x: places[x])\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_34096/3198004530.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t_bt['ratio'] = t_bt['win_ratio'].map(lambda x: math.floor(x * 100))\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_34096/3198004530.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t_bt['num'] = t_bt['race_id'].astype(str).map(lambda x: x[14:])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "race_grade = pd.read_csv('./csv_new2/20221127/races.csv')\n",
    "race_grade = race_grade.set_index('race_id')\n",
    "\n",
    "me_test = ModelEvaluator(lgb_clf, haitou, True)\n",
    "wrm = me_test.pred_table(X_test1, 0.7, False)\n",
    "wrm['pred_rank'] = wrm[['win_ratio']].groupby(level=0).rank(ascending=False)\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/20221127/weathers.csv')\n",
    "t_bets = wrm.merge(weather, on='race_id')\n",
    "\n",
    "t_bets = t_bets.merge(race_grade, on='race_id')\n",
    "t_bt = t_bets[\n",
    "    (t_bets['pred_rank'] == 1)\n",
    "#     (t_bets['shisuu'] >= 10)\n",
    "]\n",
    "\n",
    "# t_bt = t_bets\n",
    "t_bt['place'] = t_bt['place_id'].map(lambda x: places[x])\n",
    "t_bt['ratio'] = t_bt['win_ratio'].map(lambda x: math.floor(x * 100))\n",
    "t_bt['num'] = t_bt['race_id'].astype(str).map(lambda x: x[14:])\n",
    "\n",
    "th_csv = t_bt[['race_id', 'place', 'num', 'h_num', 'ratio', 'pred_rank']].sort_values(['race_id', 'pred_rank'])\n",
    "th_csv[['place', 'num', 'h_num', 'ratio']].to_csv('./1127_shinba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3c1b44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "me_test = ModelEvaluator(r_lgb_clf, haitou, True)\n",
    "fm_test = me_test.pred_table(X_rentai_test1, 0.5, True)\n",
    "\n",
    "fm_test['pred_rank'] = fm_test[['win_ratio']].groupby(level=0).rank(ascending=False)\n",
    "\n",
    "t_local_umaren = t_bt[['race_id', 'place_id', 'h_num']].merge(fm_test, on='race_id')\n",
    "t_local_umaren = t_local_umaren[t_local_umaren['h_num_x'] != t_local_umaren['h_num_y']]\n",
    "# t_local_umaren = t_local_umaren[\n",
    "#     t_local_umaren['pred_rank'] <= 6\n",
    "# ]\n",
    "t_local_umaren['place'] = t_local_umaren['place_id'].map(lambda x: places[x])\n",
    "t_local_umaren['ratio'] = t_local_umaren['win_ratio'].map(lambda x: math.floor(x * 100))\n",
    "t_local_umaren['num'] = t_local_umaren['race_id'].astype(str).map(lambda x: x[14:])\n",
    "\n",
    "t_local_umaren = t_local_umaren[['race_id', 'place', 'num', 'h_num_y', 'pred_rank']].drop_duplicates()\n",
    "tu_csv = t_local_umaren[['race_id','place', 'num', 'h_num_y', 'pred_rank']].sort_values(['race_id', 'pred_rank'])\n",
    "\n",
    "tu_csv[['place', 'num', 'h_num_y']].to_csv('./1127_shinba_ren.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "189106c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_test = ModelEvaluator(f_lgb_clf, haitou, True)\n",
    "fm_test = me_test.pred_table(X_fukusho_test1, 0.5, True)\n",
    "\n",
    "fm_test['pred_rank'] = fm_test[['win_ratio']].groupby(level=0).rank(ascending=False)\n",
    "\n",
    "t_local_umaren = t_bt[['race_id', 'place_id', 'h_num']].merge(fm_test, on='race_id')\n",
    "t_local_umaren = t_local_umaren[t_local_umaren['h_num_x'] != t_local_umaren['h_num_y']]\n",
    "# t_local_umaren = t_local_umaren[\n",
    "#     t_local_umaren['pred_rank'] <= 6\n",
    "# ]\n",
    "t_local_umaren['place'] = t_local_umaren['place_id'].map(lambda x: places[x])\n",
    "t_local_umaren['ratio'] = t_local_umaren['win_ratio'].map(lambda x: math.floor(x * 100))\n",
    "t_local_umaren['num'] = t_local_umaren['race_id'].astype(str).map(lambda x: x[14:])\n",
    "\n",
    "t_local_umaren = t_local_umaren[['race_id', 'place', 'num', 'h_num_y', 'pred_rank']].drop_duplicates()\n",
    "t_csv = t_local_umaren[['race_id','place', 'num', 'h_num_y', 'pred_rank']].sort_values(['race_id', 'pred_rank'])\n",
    "t_csv[['place', 'num', 'h_num_y']].to_csv('./1127_shinba_sanren.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
