{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c395f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "# from google.colab import drive\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "\n",
    "    df.drop([\n",
    "        'compi',\n",
    "        'compi_num', \n",
    "        \"speed\", \n",
    "        'rank', '気温', '風速', '風向',\n",
    "'1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "'1走前スピードZI','2走前スピードZI', '3走前スピードZI','4走前スピードZI', '5走前スピードZI',\n",
    "'1走前スピード指数','2走前スピード指数', '3走前スピード指数','4走前スピード指数', '5走前スピード指数',\n",
    "'1走前コンピ指数','2走前コンピ指数', '3走前コンピ指数','4走前コンピ指数', '5走前コンピ指数',\n",
    "'1走前コンピ順位','2走前コンピ順位', '3走前コンピ順位','4走前コンピ順位', '5走前コンピ順位',\n",
    "'1走前走破タイム', '2走前走破タイム', '3走前走破タイム', '4走前走破タイム', '5走前走破タイム',\n",
    "'1走前補正タイム', '2走前補正タイム', '3走前補正タイム', '4走前補正タイム', '5走前補正タイム',\n",
    "'1走前結果', '2走前結果', '3走前結果', '4走前結果', '5走前結果',\n",
    "'1走前オッズ', '2走前オッズ', '3走前オッズ', '4走前オッズ', '5走前オッズ',\n",
    "        'horse_race_id',  'body_weight','body_weight_in_de',\n",
    "\n",
    "    ], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    # df2 = pd.get_dummies(df2, sparse=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds']]\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'win_ratio']]\n",
    "#             pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "#         std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "#         .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "        \n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # 閾値を増やす        \n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e296943f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./pickle_new/chihou/chihou_race_detail.pickle')\n",
    "j = pd.read_pickle('./pickle_new/chihou/jockey_rate.pickle')\n",
    "s = pd.read_pickle('./pickle_new/chihou/horse_stallion_rate.pickle')\n",
    "df = df.merge(j, how='left', on='id')\n",
    "df = df.merge(s, how='left', on=['race_id', 'stallion_id'])\n",
    "\n",
    "# df = df.query('(place_id == 42)|(place_id == 43)|(place_id == 44)|(place_id == 45)')\n",
    "df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "df.drop(['1走前オッズ', '2走前オッズ','3走前オッズ','4走前オッズ','5走前オッズ'], axis=1, inplace=True)\n",
    "\n",
    "df = df.sort_values(by='date', ascending = False)\n",
    "all_r = df.set_index('race_id')\n",
    "all_r = all_r.fillna({\n",
    "'1走前騎手ID': 0, '1走前距離': 0, '1走前場所': 0,'1走前コース': 0,\n",
    "'2走前騎手ID': 0, '2走前距離': 0, '2走前場所': 0,'2走前コース': 0,\n",
    "'3走前騎手ID': 0, '3走前距離': 0, '3走前場所': 0,'3走前コース': 0,\n",
    "'4走前騎手ID': 0, '4走前距離': 0, '4走前場所': 0,'4走前コース': 0,\n",
    "'5走前騎手ID': 0, '5走前距離': 0, '5走前場所': 0,'5走前コース': 0,\n",
    "})\n",
    "\n",
    "all_r['result'] = all_r['result'].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "categorical = process_categorical(all_r, [\n",
    "    'owner',  'course',\n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    'grade', 'age', 'place_id',\n",
    "    'stallion_id', 'affiliation_id',\n",
    "    '1走前騎手ID', '1走前場所', '1走前距離', '1走前コース',\n",
    "    '2走前騎手ID', '2走前場所', '2走前距離', '2走前コース',\n",
    "    '3走前騎手ID', '3走前場所', '3走前距離', '3走前コース',\n",
    "    '4走前騎手ID', '4走前場所', '4走前距離', '4走前コース',\n",
    "    '5走前騎手ID', '5走前場所', '5走前距離', '5走前コース',\n",
    "])\n",
    "vec = pd.read_pickle('./pickle_new/peds_vec.pickle')\n",
    "# vec = vec[[\n",
    "#     'horse_id', \"peds_1\",\"peds_2\",\"peds_3\",\"peds_4\",\"peds_5\",\n",
    "#     \"peds_6\",\"peds_7\",\"peds_8\",\"peds_9\",\"peds_10\",\n",
    "#     \"peds_11\",\"peds_12\",\"peds_13\",\"peds_14\"\n",
    "# ]]\n",
    "\n",
    "categorical = categorical.reset_index()\n",
    "categorical = categorical.merge(vec, on='horse_id')\n",
    "categorical = categorical.set_index('race_id')\n",
    "\n",
    "train1, valid1 = split_data(categorical)\n",
    "valid1, test1 = train_valid_split_data(valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ff8419bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train1 = train1.drop(['id', 'date', 'body_weight','body_weight_in_de','result', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "y_train1 = train1['result']\n",
    "X_valid1 = valid1.drop(['date', 'body_weight','body_weight_in_de','result', 'popular',  'horse_id'], axis=1)\n",
    "y_valid1 = valid1['result']\n",
    "X_test1 = test1.drop(['date','body_weight','body_weight_in_de', 'result', 'popular',  'horse_id'], axis=1)\n",
    "y_test1 = test1['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "74580db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-02 21:52:06,753]\u001b[0m A new study created in memory with name: no-name-57b6942c-17d2-44a8-931e-beabd3ab9ef6\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  14%|#########                                                      | 1/7 [00:19<01:59, 19.98s/it]\u001b[32m[I 2022-11-02 21:52:26,772]\u001b[0m Trial 0 finished with value: 0.2988013153677138 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  14%|#########                                                      | 1/7 [00:20<01:59, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's binary_logloss: 0.269701\tvalid_1's binary_logloss: 0.298801\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  29%|##################                                             | 2/7 [00:38<01:36, 19.21s/it]\u001b[32m[I 2022-11-02 21:52:45,440]\u001b[0m Trial 1 finished with value: 0.3153238154117272 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  29%|##################                                             | 2/7 [00:38<01:36, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's binary_logloss: 0.274966\tvalid_1's binary_logloss: 0.315324\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  43%|###########################                                    | 3/7 [00:49<01:01, 15.47s/it]\u001b[32m[I 2022-11-02 21:52:56,448]\u001b[0m Trial 2 finished with value: 0.31974489833574554 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  43%|###########################                                    | 3/7 [00:49<01:01, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.301085\tvalid_1's binary_logloss: 0.319745\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  57%|####################################                           | 4/7 [01:00<00:40, 13.62s/it]\u001b[32m[I 2022-11-02 21:53:07,240]\u001b[0m Trial 3 finished with value: 0.3201857729720311 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  57%|####################################                           | 4/7 [01:00<00:40, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.30092\tvalid_1's binary_logloss: 0.320186\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  71%|#############################################                  | 5/7 [01:13<00:27, 13.59s/it]\u001b[32m[I 2022-11-02 21:53:20,771]\u001b[0m Trial 4 finished with value: 0.31900715541195795 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  71%|#############################################                  | 5/7 [01:14<00:27, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278415\tvalid_1's binary_logloss: 0.319007\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.519492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801:  86%|######################################################         | 6/7 [01:26<00:13, 13.23s/it]\u001b[32m[I 2022-11-02 21:53:33,303]\u001b[0m Trial 5 finished with value: 0.3186367376634923 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801:  86%|######################################################         | 6/7 [01:26<00:13, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.282821\tvalid_1's binary_logloss: 0.318637\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.515066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.298801: 100%|###############################################################| 7/7 [01:37<00:00, 12.35s/it]\u001b[32m[I 2022-11-02 21:53:43,833]\u001b[0m Trial 6 finished with value: 0.31989491446317553 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.2988013153677138.\u001b[0m\n",
      "feature_fraction, val_score: 0.298801: 100%|###############################################################| 7/7 [01:37<00:00, 13.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.301083\tvalid_1's binary_logloss: 0.319895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:   0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:   5%|###4                                                                | 1/20 [00:12<04:03, 12.80s/it]\u001b[32m[I 2022-11-02 21:53:56,664]\u001b[0m Trial 7 finished with value: 0.31843244118094416 and parameters: {'num_leaves': 200}. Best is trial 7 with value: 0.31843244118094416.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:   5%|###4                                                                | 1/20 [00:12<04:03, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.271956\tvalid_1's binary_logloss: 0.318432\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  10%|######8                                                             | 2/20 [00:23<03:30, 11.68s/it]\u001b[32m[I 2022-11-02 21:54:07,558]\u001b[0m Trial 8 finished with value: 0.3192463361122745 and parameters: {'num_leaves': 255}. Best is trial 7 with value: 0.31843244118094416.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  10%|######8                                                             | 2/20 [00:23<03:30, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.292325\tvalid_1's binary_logloss: 0.319246\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  15%|##########2                                                         | 3/20 [00:34<03:13, 11.38s/it]\u001b[32m[I 2022-11-02 21:54:18,593]\u001b[0m Trial 9 finished with value: 0.3187455703527469 and parameters: {'num_leaves': 206}. Best is trial 7 with value: 0.31843244118094416.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  15%|##########2                                                         | 3/20 [00:34<03:13, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.284904\tvalid_1's binary_logloss: 0.318746\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  20%|#############6                                                      | 4/20 [00:46<03:01, 11.36s/it]\u001b[32m[I 2022-11-02 21:54:29,921]\u001b[0m Trial 10 finished with value: 0.31697672562763085 and parameters: {'num_leaves': 184}. Best is trial 10 with value: 0.31697672562763085.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  20%|#############6                                                      | 4/20 [00:46<03:01, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.280133\tvalid_1's binary_logloss: 0.316977\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  25%|#################                                                   | 5/20 [00:59<03:02, 12.14s/it]\u001b[32m[I 2022-11-02 21:54:43,437]\u001b[0m Trial 11 finished with value: 0.3172469548802065 and parameters: {'num_leaves': 142}. Best is trial 10 with value: 0.31697672562763085.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  25%|#################                                                   | 5/20 [00:59<03:02, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.269417\tvalid_1's binary_logloss: 0.317247\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  30%|####################4                                               | 6/20 [01:18<03:20, 14.34s/it]\u001b[32m[I 2022-11-02 21:55:02,042]\u001b[0m Trial 12 finished with value: 0.3084196074727791 and parameters: {'num_leaves': 73}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  30%|####################4                                               | 6/20 [01:18<03:20, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.262968\tvalid_1's binary_logloss: 0.30842\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  35%|#######################7                                            | 7/20 [01:36<03:21, 15.50s/it]\u001b[32m[I 2022-11-02 21:55:19,923]\u001b[0m Trial 13 finished with value: 0.3173898852589785 and parameters: {'num_leaves': 130}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  35%|#######################7                                            | 7/20 [01:36<03:21, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.267308\tvalid_1's binary_logloss: 0.31739\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  40%|###########################2                                        | 8/20 [01:50<03:01, 15.09s/it]\u001b[32m[I 2022-11-02 21:55:34,135]\u001b[0m Trial 14 finished with value: 0.31893414093615 and parameters: {'num_leaves': 135}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  40%|###########################2                                        | 8/20 [01:50<03:01, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.282078\tvalid_1's binary_logloss: 0.318934\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  45%|##############################6                                     | 9/20 [02:04<02:43, 14.90s/it]\u001b[32m[I 2022-11-02 21:55:48,627]\u001b[0m Trial 15 finished with value: 0.31762869281226663 and parameters: {'num_leaves': 153}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  45%|##############################6                                     | 9/20 [02:04<02:43, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.279929\tvalid_1's binary_logloss: 0.317629\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  50%|#################################5                                 | 10/20 [02:18<02:24, 14.46s/it]\u001b[32m[I 2022-11-02 21:56:02,095]\u001b[0m Trial 16 finished with value: 0.3170091480165388 and parameters: {'num_leaves': 54}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  50%|#################################5                                 | 10/20 [02:18<02:24, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.276141\tvalid_1's binary_logloss: 0.317009\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  55%|####################################8                              | 11/20 [02:32<02:08, 14.27s/it]\u001b[32m[I 2022-11-02 21:56:15,928]\u001b[0m Trial 17 finished with value: 0.31448992755899274 and parameters: {'num_leaves': 45}. Best is trial 12 with value: 0.3084196074727791.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  55%|####################################8                              | 11/20 [02:32<02:08, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.276995\tvalid_1's binary_logloss: 0.31449\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  60%|########################################1                          | 12/20 [02:50<02:05, 15.68s/it]\u001b[32m[I 2022-11-02 21:56:34,835]\u001b[0m Trial 18 finished with value: 0.2993977295235557 and parameters: {'num_leaves': 49}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  60%|########################################1                          | 12/20 [02:50<02:05, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's binary_logloss: 0.267999\tvalid_1's binary_logloss: 0.299398\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  65%|###########################################5                       | 13/20 [03:01<01:37, 13.98s/it]\u001b[32m[I 2022-11-02 21:56:44,915]\u001b[0m Trial 19 finished with value: 0.31972277348082034 and parameters: {'num_leaves': 6}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  65%|###########################################5                       | 13/20 [03:01<01:37, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.283998\tvalid_1's binary_logloss: 0.319723\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  70%|##############################################9                    | 14/20 [03:11<01:17, 12.95s/it]\u001b[32m[I 2022-11-02 21:56:55,495]\u001b[0m Trial 20 finished with value: 0.3181081220102772 and parameters: {'num_leaves': 83}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  70%|##############################################9                    | 14/20 [03:11<01:17, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.280936\tvalid_1's binary_logloss: 0.318108\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  75%|##################################################2                | 15/20 [03:24<01:04, 12.85s/it]\u001b[32m[I 2022-11-02 21:57:08,076]\u001b[0m Trial 21 finished with value: 0.31418405040789543 and parameters: {'num_leaves': 87}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  75%|##################################################2                | 15/20 [03:24<01:04, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.272413\tvalid_1's binary_logloss: 0.314184\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  80%|#####################################################6             | 16/20 [03:32<00:45, 11.39s/it]\u001b[32m[I 2022-11-02 21:57:16,083]\u001b[0m Trial 22 finished with value: 0.32078145451012446 and parameters: {'num_leaves': 15}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  80%|#####################################################6             | 16/20 [03:32<00:45, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.316241\tvalid_1's binary_logloss: 0.320781\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  85%|########################################################9          | 17/20 [03:47<00:37, 12.43s/it]\u001b[32m[I 2022-11-02 21:57:30,923]\u001b[0m Trial 23 finished with value: 0.3131432729927963 and parameters: {'num_leaves': 90}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  85%|########################################################9          | 17/20 [03:47<00:37, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's binary_logloss: 0.266517\tvalid_1's binary_logloss: 0.313143\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  90%|############################################################3      | 18/20 [04:02<00:26, 13.23s/it]\u001b[32m[I 2022-11-02 21:57:46,037]\u001b[0m Trial 24 finished with value: 0.3013347803749849 and parameters: {'num_leaves': 52}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  90%|############################################################3      | 18/20 [04:02<00:26, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.269506\tvalid_1's binary_logloss: 0.301335\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801:  95%|###############################################################6   | 19/20 [04:13<00:12, 12.71s/it]\u001b[32m[I 2022-11-02 21:57:57,528]\u001b[0m Trial 25 finished with value: 0.31533873439446786 and parameters: {'num_leaves': 44}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801:  95%|###############################################################6   | 19/20 [04:13<00:12, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.277079\tvalid_1's binary_logloss: 0.315339\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.298801: 100%|###################################################################| 20/20 [04:29<00:00, 13.55s/it]\u001b[32m[I 2022-11-02 21:58:13,037]\u001b[0m Trial 26 finished with value: 0.3013704302645939 and parameters: {'num_leaves': 26}. Best is trial 18 with value: 0.2993977295235557.\u001b[0m\n",
      "num_leaves, val_score: 0.298801: 100%|###################################################################| 20/20 [04:29<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's binary_logloss: 0.272746\tvalid_1's binary_logloss: 0.30137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.298801:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.298801:  10%|#######1                                                               | 1/10 [00:09<01:24,  9.35s/it]\u001b[32m[I 2022-11-02 21:58:22,421]\u001b[0m Trial 27 finished with value: 0.316704474679761 and parameters: {'bagging_fraction': 0.43369808703743745, 'bagging_freq': 6}. Best is trial 27 with value: 0.316704474679761.\u001b[0m\n",
      "bagging, val_score: 0.298801:  10%|#######1                                                               | 1/10 [00:09<01:24,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.281741\tvalid_1's binary_logloss: 0.316704\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  20%|##############2                                                        | 2/10 [00:24<01:44, 13.03s/it]\u001b[32m[I 2022-11-02 21:58:38,035]\u001b[0m Trial 28 finished with value: 0.29580474497632253 and parameters: {'bagging_fraction': 0.9330618671023129, 'bagging_freq': 4}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  20%|##############2                                                        | 2/10 [00:24<01:44, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's binary_logloss: 0.271529\tvalid_1's binary_logloss: 0.295805\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  30%|#####################3                                                 | 3/10 [00:32<01:14, 10.69s/it]\u001b[32m[I 2022-11-02 21:58:45,937]\u001b[0m Trial 29 finished with value: 0.3210446010068803 and parameters: {'bagging_fraction': 0.5599998887848661, 'bagging_freq': 1}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  30%|#####################3                                                 | 3/10 [00:32<01:14, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.315447\tvalid_1's binary_logloss: 0.321045\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  40%|############################4                                          | 4/10 [00:41<00:58,  9.70s/it]\u001b[32m[I 2022-11-02 21:58:54,107]\u001b[0m Trial 30 finished with value: 0.32031764102168064 and parameters: {'bagging_fraction': 0.8727282496144824, 'bagging_freq': 5}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  40%|############################4                                          | 4/10 [00:41<00:58,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.315467\tvalid_1's binary_logloss: 0.320318\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  50%|###################################5                                   | 5/10 [00:54<00:55, 11.02s/it]\u001b[32m[I 2022-11-02 21:59:07,472]\u001b[0m Trial 31 finished with value: 0.30695831041342136 and parameters: {'bagging_fraction': 0.47723441166987124, 'bagging_freq': 3}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  50%|###################################5                                   | 5/10 [00:54<00:55, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's binary_logloss: 0.272377\tvalid_1's binary_logloss: 0.306958\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  60%|##########################################6                            | 6/10 [01:02<00:40, 10.05s/it]\u001b[32m[I 2022-11-02 21:59:15,644]\u001b[0m Trial 32 finished with value: 0.32261583991735915 and parameters: {'bagging_fraction': 0.6902004194605169, 'bagging_freq': 5}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  60%|##########################################6                            | 6/10 [01:02<00:40, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.315495\tvalid_1's binary_logloss: 0.322616\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  70%|#################################################6                     | 7/10 [01:10<00:28,  9.43s/it]\u001b[32m[I 2022-11-02 21:59:23,777]\u001b[0m Trial 33 finished with value: 0.32259368358037915 and parameters: {'bagging_fraction': 0.6339418215820484, 'bagging_freq': 5}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  70%|#################################################6                     | 7/10 [01:10<00:28,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.315479\tvalid_1's binary_logloss: 0.322594\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  80%|########################################################8              | 8/10 [01:25<00:22, 11.18s/it]\u001b[32m[I 2022-11-02 21:59:38,723]\u001b[0m Trial 34 finished with value: 0.30833358364919283 and parameters: {'bagging_fraction': 0.9673167656716128, 'bagging_freq': 6}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  80%|########################################################8              | 8/10 [01:25<00:22, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.272387\tvalid_1's binary_logloss: 0.308334\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805:  90%|###############################################################9       | 9/10 [01:41<00:12, 12.72s/it]\u001b[32m[I 2022-11-02 21:59:54,821]\u001b[0m Trial 35 finished with value: 0.30183387144570006 and parameters: {'bagging_fraction': 0.7738374127646642, 'bagging_freq': 7}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805:  90%|###############################################################9       | 9/10 [01:41<00:12, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's binary_logloss: 0.270946\tvalid_1's binary_logloss: 0.301834\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.295805: 100%|######################################################################| 10/10 [01:49<00:00, 11.33s/it]\u001b[32m[I 2022-11-02 22:00:03,043]\u001b[0m Trial 36 finished with value: 0.32039780616673863 and parameters: {'bagging_fraction': 0.5922974316475675, 'bagging_freq': 3}. Best is trial 28 with value: 0.29580474497632253.\u001b[0m\n",
      "bagging, val_score: 0.295805: 100%|######################################################################| 10/10 [01:50<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.315471\tvalid_1's binary_logloss: 0.320398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.295805:   0%|                                                                | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.295805:  33%|##################6                                     | 1/3 [00:10<00:20, 10.32s/it]\u001b[32m[I 2022-11-02 22:00:13,393]\u001b[0m Trial 37 finished with value: 0.31911874179132754 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 0.31911874179132754.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.295805:  33%|##################6                                     | 1/3 [00:10<00:20, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.286189\tvalid_1's binary_logloss: 0.319119\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.293675:  67%|#####################################3                  | 2/3 [00:28<00:15, 15.04s/it]\u001b[32m[I 2022-11-02 22:00:31,742]\u001b[0m Trial 38 finished with value: 0.29367521880606295 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 38 with value: 0.29367521880606295.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.293675:  67%|#####################################3                  | 2/3 [00:28<00:15, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.293675: 100%|########################################################| 3/3 [00:43<00:00, 15.17s/it]\u001b[32m[I 2022-11-02 22:00:47,062]\u001b[0m Trial 39 finished with value: 0.3002635999150605 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 38 with value: 0.29367521880606295.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.293675: 100%|########################################################| 3/3 [00:44<00:00, 14.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's binary_logloss: 0.272195\tvalid_1's binary_logloss: 0.300264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:   5%|##8                                                     | 1/20 [00:16<05:10, 16.35s/it]\u001b[32m[I 2022-11-02 22:01:03,445]\u001b[0m Trial 40 finished with value: 0.3037778167971391 and parameters: {'lambda_l1': 2.9027710713271834e-08, 'lambda_l2': 0.016619535965153656}. Best is trial 40 with value: 0.3037778167971391.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:   5%|##8                                                     | 1/20 [00:16<05:10, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's binary_logloss: 0.272016\tvalid_1's binary_logloss: 0.303778\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  10%|#####6                                                  | 2/20 [00:27<04:04, 13.58s/it]\u001b[32m[I 2022-11-02 22:01:15,086]\u001b[0m Trial 41 finished with value: 0.3181039352793857 and parameters: {'lambda_l1': 1.0906720663200369, 'lambda_l2': 1.243122889343004e-06}. Best is trial 40 with value: 0.3037778167971391.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  10%|#####6                                                  | 2/20 [00:28<04:04, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.281738\tvalid_1's binary_logloss: 0.318104\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  15%|########4                                               | 3/20 [00:45<04:24, 15.57s/it]\u001b[32m[I 2022-11-02 22:01:33,011]\u001b[0m Trial 42 finished with value: 0.29367522063207036 and parameters: {'lambda_l1': 1.2750648275267467e-08, 'lambda_l2': 3.2379773172367715e-06}. Best is trial 42 with value: 0.29367522063207036.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  15%|########4                                               | 3/20 [00:45<04:24, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  20%|###########2                                            | 4/20 [00:57<03:40, 13.80s/it]\u001b[32m[I 2022-11-02 22:01:44,130]\u001b[0m Trial 43 finished with value: 0.316359539987756 and parameters: {'lambda_l1': 4.790796443099532, 'lambda_l2': 0.00011410055617168604}. Best is trial 42 with value: 0.29367522063207036.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  20%|###########2                                            | 4/20 [00:57<03:40, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.28079\tvalid_1's binary_logloss: 0.31636\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  25%|##############                                          | 5/20 [01:14<03:49, 15.27s/it]\u001b[32m[I 2022-11-02 22:02:01,985]\u001b[0m Trial 44 finished with value: 0.29367525258998456 and parameters: {'lambda_l1': 4.014436007703783e-05, 'lambda_l2': 1.2112413414845028e-05}. Best is trial 42 with value: 0.29367522063207036.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  25%|##############                                          | 5/20 [01:14<03:49, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  30%|################8                                       | 6/20 [01:27<03:18, 14.20s/it]\u001b[32m[I 2022-11-02 22:02:14,108]\u001b[0m Trial 45 finished with value: 0.3166723186470206 and parameters: {'lambda_l1': 3.347298555478262e-08, 'lambda_l2': 2.9201396164182367}. Best is trial 42 with value: 0.29367522063207036.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  30%|################8                                       | 6/20 [01:27<03:18, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278413\tvalid_1's binary_logloss: 0.316672\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.293675:  35%|###################5                                    | 7/20 [01:44<03:20, 15.40s/it]\u001b[32m[I 2022-11-02 22:02:31,966]\u001b[0m Trial 46 finished with value: 0.29367828932975915 and parameters: {'lambda_l1': 0.0029463237635498993, 'lambda_l2': 2.7625369746379443e-05}. Best is trial 42 with value: 0.29367522063207036.\u001b[0m\n",
      "regularization_factors, val_score: 0.293675:  35%|###################5                                    | 7/20 [01:44<03:20, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293678\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.268002\tvalid_1's binary_logloss: 0.296131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.292929:  40%|######################4                                 | 8/20 [02:06<03:28, 17.39s/it]\u001b[32m[I 2022-11-02 22:02:53,636]\u001b[0m Trial 47 finished with value: 0.2929291961749201 and parameters: {'lambda_l1': 0.9427803074760679, 'lambda_l2': 0.04388695505198632}. Best is trial 47 with value: 0.2929291961749201.\u001b[0m\n",
      "regularization_factors, val_score: 0.292929:  40%|######################4                                 | 8/20 [02:06<03:28, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.267073\tvalid_1's binary_logloss: 0.292929\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.292929:  45%|#########################2                              | 9/20 [02:28<03:27, 18.86s/it]\u001b[32m[I 2022-11-02 22:03:15,737]\u001b[0m Trial 48 finished with value: 0.2936753151336077 and parameters: {'lambda_l1': 0.00014162211547226496, 'lambda_l2': 1.3642675799206107e-06}. Best is trial 47 with value: 0.2929291961749201.\u001b[0m\n",
      "regularization_factors, val_score: 0.292929:  45%|#########################2                              | 9/20 [02:28<03:27, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.292929:  50%|###########################5                           | 10/20 [02:48<03:12, 19.21s/it]\u001b[32m[I 2022-11-02 22:03:35,705]\u001b[0m Trial 49 finished with value: 0.2936752237098265 and parameters: {'lambda_l1': 7.0591530160057386e-06, 'lambda_l2': 1.8366916867194227e-07}. Best is trial 47 with value: 0.2929291961749201.\u001b[0m\n",
      "regularization_factors, val_score: 0.292929:  50%|###########################5                           | 10/20 [02:48<03:12, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.292929:  55%|##############################2                        | 11/20 [03:00<02:32, 16.90s/it]\u001b[32m[I 2022-11-02 22:03:47,379]\u001b[0m Trial 50 finished with value: 0.3175977877911326 and parameters: {'lambda_l1': 0.02637881097568518, 'lambda_l2': 0.012245138543488883}. Best is trial 47 with value: 0.2929291961749201.\u001b[0m\n",
      "regularization_factors, val_score: 0.292929:  55%|##############################2                        | 11/20 [03:00<02:32, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278379\tvalid_1's binary_logloss: 0.317598\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  60%|#################################                      | 12/20 [03:19<02:21, 17.63s/it]\u001b[32m[I 2022-11-02 22:04:06,685]\u001b[0m Trial 51 finished with value: 0.2902831010333156 and parameters: {'lambda_l1': 9.579887367998411e-07, 'lambda_l2': 0.003442774081046108}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  60%|#################################                      | 12/20 [03:19<02:21, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.267933\tvalid_1's binary_logloss: 0.292535\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's binary_logloss: 0.268973\tvalid_1's binary_logloss: 0.290283\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  65%|###################################7                   | 13/20 [03:37<02:03, 17.63s/it]\u001b[32m[I 2022-11-02 22:04:24,330]\u001b[0m Trial 52 finished with value: 0.30088290703758325 and parameters: {'lambda_l1': 1.6744422748950005e-06, 'lambda_l2': 0.008991018199656928}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  65%|###################################7                   | 13/20 [03:37<02:03, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's binary_logloss: 0.271893\tvalid_1's binary_logloss: 0.300883\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  70%|######################################5                | 14/20 [03:52<01:41, 16.96s/it]\u001b[32m[I 2022-11-02 22:04:39,750]\u001b[0m Trial 53 finished with value: 0.3012747838157243 and parameters: {'lambda_l1': 0.01934888268968745, 'lambda_l2': 1.5734718407408788}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  70%|######################################5                | 14/20 [03:52<01:41, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's binary_logloss: 0.273246\tvalid_1's binary_logloss: 0.301275\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  75%|#########################################2             | 15/20 [04:12<01:29, 17.95s/it]\u001b[32m[I 2022-11-02 22:04:59,981]\u001b[0m Trial 54 finished with value: 0.29367708215439897 and parameters: {'lambda_l1': 6.084814678681447e-07, 'lambda_l2': 0.001425454162872355}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  75%|#########################################2             | 15/20 [04:12<01:29, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293677\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  80%|############################################           | 16/20 [04:25<01:04, 16.23s/it]\u001b[32m[I 2022-11-02 22:05:12,224]\u001b[0m Trial 55 finished with value: 0.3172731330278483 and parameters: {'lambda_l1': 0.0008057730278055503, 'lambda_l2': 0.28422911093322084}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  80%|############################################           | 16/20 [04:25<01:04, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.280221\tvalid_1's binary_logloss: 0.317273\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  85%|##############################################7        | 17/20 [04:45<00:52, 17.59s/it]\u001b[32m[I 2022-11-02 22:05:32,959]\u001b[0m Trial 56 finished with value: 0.29659096397316886 and parameters: {'lambda_l1': 0.15197968778734447, 'lambda_l2': 0.14983588079184668}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  85%|##############################################7        | 17/20 [04:45<00:52, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.269992\tvalid_1's binary_logloss: 0.296591\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  90%|#################################################5     | 18/20 [05:05<00:36, 18.09s/it]\u001b[32m[I 2022-11-02 22:05:52,220]\u001b[0m Trial 57 finished with value: 0.29367724063186 and parameters: {'lambda_l1': 3.6890031369493775e-07, 'lambda_l2': 0.0017068404546420463}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  90%|#################################################5     | 18/20 [05:05<00:36, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293677\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283:  95%|####################################################2  | 19/20 [05:25<00:18, 18.66s/it]\u001b[32m[I 2022-11-02 22:06:12,212]\u001b[0m Trial 58 finished with value: 0.29367524027024655 and parameters: {'lambda_l1': 3.1756009286467624e-05, 'lambda_l2': 1.0696430663685008e-08}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283:  95%|####################################################2  | 19/20 [05:25<00:18, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.270058\tvalid_1's binary_logloss: 0.293675\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.290283: 100%|#######################################################| 20/20 [05:45<00:00, 19.24s/it]\u001b[32m[I 2022-11-02 22:06:32,807]\u001b[0m Trial 59 finished with value: 0.29774727533170403 and parameters: {'lambda_l1': 0.3777916453523174, 'lambda_l2': 0.13978324216414775}. Best is trial 51 with value: 0.2902831010333156.\u001b[0m\n",
      "regularization_factors, val_score: 0.290283: 100%|#######################################################| 20/20 [05:45<00:00, 17.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.269237\tvalid_1's binary_logloss: 0.297747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283:   0%|                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283:  20%|############6                                                  | 1/5 [00:11<00:45, 11.44s/it]\u001b[32m[I 2022-11-02 22:06:44,276]\u001b[0m Trial 60 finished with value: 0.31816019324245887 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 0.31816019324245887.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.290283:  20%|############6                                                  | 1/5 [00:11<00:45, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.281721\tvalid_1's binary_logloss: 0.31816\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283:  40%|#########################2                                     | 2/5 [00:24<00:37, 12.50s/it]\u001b[32m[I 2022-11-02 22:06:57,523]\u001b[0m Trial 61 finished with value: 0.317594358903417 and parameters: {'min_child_samples': 10}. Best is trial 61 with value: 0.317594358903417.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.290283:  40%|#########################2                                     | 2/5 [00:24<00:37, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278378\tvalid_1's binary_logloss: 0.317594\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283:  60%|#####################################8                         | 3/5 [00:37<00:25, 12.72s/it]\u001b[32m[I 2022-11-02 22:07:10,514]\u001b[0m Trial 62 finished with value: 0.3175979921199184 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.317594358903417.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.290283:  60%|#####################################8                         | 3/5 [00:37<00:25, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278378\tvalid_1's binary_logloss: 0.317598\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.268136\tvalid_1's binary_logloss: 0.302479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283:  80%|##################################################4            | 4/5 [01:00<00:16, 16.84s/it]\u001b[32m[I 2022-11-02 22:07:33,656]\u001b[0m Trial 63 finished with value: 0.29854491916544423 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.29854491916544423.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.290283:  80%|##################################################4            | 4/5 [01:00<00:16, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.26742\tvalid_1's binary_logloss: 0.298545\n",
      "[LightGBM] [Info] Number of positive: 54703, number of negative: 497981\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.098977 -> initscore=-2.208643\n",
      "[LightGBM] [Info] Start training from score -2.208643\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.290283: 100%|###############################################################| 5/5 [01:13<00:00, 15.25s/it]\u001b[32m[I 2022-11-02 22:07:46,101]\u001b[0m Trial 64 finished with value: 0.31759991582304786 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.29854491916544423.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.290283: 100%|###############################################################| 5/5 [01:13<00:00, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.278378\tvalid_1's binary_logloss: 0.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "lgb_train = lgb_o.Dataset(X_train1.values, y_train1.values)\n",
    "lgb_valid = lgb_o.Dataset(X_valid1.values, y_valid1.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "04c350c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 9.579887367998411e-07,\n",
       " 'lambda_l2': 0.003442774081046108,\n",
       " 'num_leaves': 31,\n",
       " 'feature_fraction': 0.44800000000000006,\n",
       " 'bagging_fraction': 0.9330618671023129,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ff605d37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9436453052101297, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9436453052101297\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.0660659894978455, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0660659894978455\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6698020366638395, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6698020366638395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.9436453052101297, bagging_freq=1,\n",
       "               feature_fraction=0.62, feature_pre_filter=False,\n",
       "               lambda_l1=1.6698020366638395, lambda_l2=5.0660659894978455,\n",
       "               min_child_samples=10, num_iterations=1000, num_leaves=168,\n",
       "               objective='binary', random_state=100)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    " 'objective': 'binary',\n",
    "'random_state': 100,\n",
    "'feature_pre_filter': False,\n",
    " 'lambda_l1': 1.6698020366638395,\n",
    " 'lambda_l2': 5.0660659894978455,\n",
    " 'num_leaves': 168,\n",
    " 'feature_fraction': 0.62,\n",
    " 'bagging_fraction': 0.9436453052101297,\n",
    " 'bagging_freq': 1,\n",
    " 'min_child_samples': 10,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "lgb_clf1 = lgb.LGBMClassifier(**params)\n",
    "lgb_clf1.fit(X_train1.values, y_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48d3450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/708840992.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  haitou = pd.read_csv('./csv_new2/chihou/chihou_haitou.csv')\n"
     ]
    }
   ],
   "source": [
    "haitou = pd.read_csv('./csv_new2/chihou/chihou_haitou.csv')\n",
    "haitou = haitou.set_index('race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d38261a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7772391842882118"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf1, haitou, std=True)\n",
    "me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ac0ec168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyterthemes import jtplot\n",
    "\n",
    "y_pred = me_valid.predict_proba(X_valid1).values\n",
    "\n",
    "jtplot.style(theme='monokai')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid1, y_pred)\n",
    "pit.plot(fpr, tpr, marker='o')\n",
    "pit.xlabel(\"False positive rate\")\n",
    "pit.ylabel(\"True positive rate\")\n",
    "pit.grid()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "05bf9a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1走前上がり3F</td>\n",
       "      <td>1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1走前走破タイム</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2走前走破タイム</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trainer_id</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1走前馬場指数</td>\n",
       "      <td>1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1走前着差</td>\n",
       "      <td>1327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>5走前走破タイム</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>owner</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3走前走破タイム</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4走前馬場指数</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3走前馬場指数</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2走前上がり3F</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1走前相対着順</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4走前上がり3F</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5走前馬場指数</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2走前馬場指数</td>\n",
       "      <td>1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>1192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>種牡馬同コース同距離別複勝率</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4走前走破タイム</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>peds_14</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>種牡馬同コース同距離別勝率</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>種牡馬同コース同距離別連対率</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>peds_61</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3走前着差</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2走前着差</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>騎手同コース同距離別複勝率</td>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5走前上がり3F</td>\n",
       "      <td>1121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2走前相対着順</td>\n",
       "      <td>1117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3走前上がり3F</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>peds_6</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>peds_62</td>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>種牡馬距離別複勝率</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2走前相対人気</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>peds_30</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>peds_2</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>騎手競馬場別勝率</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>4走前相対着順</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2走前騎手ID</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3走前相対着順</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>騎手競馬場別騎乗回数</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>peds_29</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>peds_59</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1走前相対人気</td>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5走前着差</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>騎手競馬場別複勝率</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1走前騎手ID</td>\n",
       "      <td>1013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5走前相対人気</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>peds_13</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>騎手距離別騎乗回数</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3走前相対人気</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           features  importance\n",
       "33         1走前上がり3F        1544\n",
       "27         1走前走破タイム        1428\n",
       "59         2走前走破タイム        1421\n",
       "10       trainer_id        1375\n",
       "35          1走前馬場指数        1367\n",
       "28            1走前着差        1327\n",
       "156        5走前走破タイム        1299\n",
       "17            owner        1295\n",
       "92         3走前走破タイム        1281\n",
       "132         4走前馬場指数        1252\n",
       "100         3走前馬場指数        1248\n",
       "65         2走前上がり3F        1248\n",
       "19          1走前相対着順        1233\n",
       "130        4走前上がり3F        1220\n",
       "164         5走前馬場指数        1206\n",
       "67          2走前馬場指数        1201\n",
       "8         jockey_id        1192\n",
       "218  種牡馬同コース同距離別複勝率        1187\n",
       "124        4走前走破タイム        1181\n",
       "239         peds_14        1173\n",
       "216   種牡馬同コース同距離別勝率        1172\n",
       "217  種牡馬同コース同距離別連対率        1162\n",
       "286         peds_61        1151\n",
       "93            3走前着差        1140\n",
       "60            2走前着差        1134\n",
       "198   騎手同コース同距離別複勝率        1129\n",
       "162        5走前上がり3F        1121\n",
       "51          2走前相対着順        1117\n",
       "98         3走前上がり3F        1113\n",
       "231          peds_6        1110\n",
       "287         peds_62        1087\n",
       "214       種牡馬距離別複勝率        1084\n",
       "56          2走前相対人気        1073\n",
       "255         peds_30        1066\n",
       "227          peds_2        1059\n",
       "184        騎手競馬場別勝率        1042\n",
       "116         4走前相対着順        1041\n",
       "52          2走前騎手ID        1039\n",
       "83          3走前相対着順        1038\n",
       "183      騎手競馬場別騎乗回数        1032\n",
       "254         peds_29        1030\n",
       "284         peds_59        1027\n",
       "24          1走前相対人気        1021\n",
       "157           5走前着差        1019\n",
       "186       騎手競馬場別複勝率        1016\n",
       "20          1走前騎手ID        1013\n",
       "153         5走前相対人気        1010\n",
       "238         peds_13        1004\n",
       "191       騎手距離別騎乗回数        1003\n",
       "89          3走前相対人気        1003"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid.feature_importance(X_train1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6775f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf1, haitou, std=True)\n",
    "wr = me_valid.pred_table(X_valid1, 0.7, True)\n",
    "wr['pred_rank'] = wr[['win_ratio']].groupby(level=0).rank(ascending=False)\n",
    "wr['expected'] = wr['win_ratio'] * wr['odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "5c7c508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = wr[\n",
    "    wr['expected'] < 1\n",
    "]\n",
    "# bt = wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a7d35daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：9336 レース出現確率:56.2% 賭金:933,600円 配当合計:765,920円 最高配当:750円 的中数:3989 的中率:42.7% 回収率:82.0%\n"
     ]
    }
   ],
   "source": [
    "bh = bt.merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} レース出現確率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    len(bt),\\\n",
    "    len(bt.groupby(level=0)) / len(X_valid1.groupby(level=0)),\\\n",
    "    len(bt) * 100,\\\n",
    "    int(bh[bh['h_num'] == bh['1着馬番']]['単勝'].sum()),\\\n",
    "    int(bh[bh['h_num'] == bh['1着馬番']]['単勝'].max()),\\\n",
    "    len(bh[bh['h_num'] == bh['1着馬番']]),\\\n",
    "    len(bh[bh['h_num'] == bh['1着馬番']]) / (len(bt)),\\\n",
    "    (bh[bh['h_num'] == bh['1着馬番']]['単勝'].sum() / (len(bt) * 100)), \\\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a4e1aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "賭金:933,600円 配当合計:816,800円 的中数:6851 的中率:73.4% 回収率:87.5%\n"
     ]
    }
   ],
   "source": [
    "h = bt.merge(haitou, on='race_id')\n",
    "\n",
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(h[h['h_num'] == h[s + '着馬番']]['複勝' + s])\n",
    "    money += h[h['h_num'] == h[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "print(\"賭金:{:,}円 配当合計:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    (len(bt) * 100),\\\n",
    "    int(money),\\\n",
    "    f_c,\\\n",
    "    f_c / len(bt),\\\n",
    "    (money / (len(bt) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a4d96509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "cc = categorical.copy()\n",
    "result_d = cc.fillna(0)\n",
    "\n",
    "train1_d, valid1_d  = split_data(result_d)\n",
    "valid1_d, test1_d  = train_valid_split_data(valid1_d)\n",
    "\n",
    "X_train1_d  = train1_d.drop(['id', 'date', 'result', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "t_train1_d  = train1_d['result']\n",
    "X_valid1_d  = valid1_d.drop(['id', 'date', 'result', 'horse_id'], axis=1)\n",
    "t_valid1_d  = valid1_d['result']\n",
    "\n",
    "X_train_d = torch.Tensor(X_train1_d.values)\n",
    "t_train_d = torch.Tensor(t_train1_d.values)\n",
    "X_valid_d = torch.Tensor(X_valid1_d.drop(['odds', 'popular'], axis=1).values)\n",
    "t_valid_d = torch.Tensor(t_valid1_d.values)\n",
    "\n",
    "t_train_d = t_train_d.reshape([-1, 1])\n",
    "t_valid_d = t_valid_d.reshape([-1, 1])\n",
    "\n",
    "X_test1_d = test1.drop(['id', 'date', 'result', 'horse_id'], axis=1)\n",
    "t_test1_d = test1['result']\n",
    "X_test_d = torch.Tensor(X_test1_d.drop(['odds', 'popular'], axis=1).values)\n",
    "t_test_d = torch.Tensor(t_test1_d.values)\n",
    "\n",
    "t_test_d = t_test_d.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "64ed0408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train:[loss=0.088, AUC=0.721], test:[loss=0.088, AUC=0.714]\n",
      "epoch: 1, train:[loss=0.082, AUC=0.736], test:[loss=0.082, AUC=0.736]\n",
      "epoch: 2, train:[loss=0.082, AUC=0.742], test:[loss=0.082, AUC=0.738]\n",
      "epoch: 3, train:[loss=0.083, AUC=0.736], test:[loss=0.082, AUC=0.732]\n",
      "epoch: 4, train:[loss=0.082, AUC=0.745], test:[loss=0.082, AUC=0.743]\n",
      "epoch: 5, train:[loss=0.082, AUC=0.743], test:[loss=0.083, AUC=0.735]\n",
      "epoch: 6, train:[loss=0.083, AUC=0.750], test:[loss=0.082, AUC=0.749]\n",
      "epoch: 7, train:[loss=0.083, AUC=0.751], test:[loss=0.083, AUC=0.749]\n",
      "epoch: 8, train:[loss=0.082, AUC=0.752], test:[loss=0.082, AUC=0.750]\n",
      "epoch: 9, train:[loss=0.083, AUC=0.748], test:[loss=0.083, AUC=0.747]\n",
      "epoch: 10, train:[loss=0.088, AUC=0.745], test:[loss=0.088, AUC=0.743]\n",
      "epoch: 11, train:[loss=0.084, AUC=0.753], test:[loss=0.084, AUC=0.753]\n",
      "epoch: 12, train:[loss=0.081, AUC=0.753], test:[loss=0.081, AUC=0.752]\n",
      "epoch: 13, train:[loss=0.081, AUC=0.753], test:[loss=0.081, AUC=0.751]\n",
      "epoch: 14, train:[loss=0.083, AUC=0.746], test:[loss=0.082, AUC=0.743]\n",
      "epoch: 15, train:[loss=0.084, AUC=0.752], test:[loss=0.084, AUC=0.750]\n",
      "epoch: 16, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 17, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 18, train:[loss=0.082, AUC=0.750], test:[loss=0.082, AUC=0.746]\n",
      "epoch: 19, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 20, train:[loss=0.081, AUC=0.755], test:[loss=0.081, AUC=0.753]\n",
      "epoch: 21, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 22, train:[loss=0.082, AUC=0.754], test:[loss=0.082, AUC=0.751]\n",
      "epoch: 23, train:[loss=0.081, AUC=0.750], test:[loss=0.082, AUC=0.745]\n",
      "epoch: 24, train:[loss=0.081, AUC=0.757], test:[loss=0.080, AUC=0.756]\n",
      "epoch: 25, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 26, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.756]\n",
      "epoch: 27, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 28, train:[loss=0.081, AUC=0.755], test:[loss=0.081, AUC=0.752]\n",
      "epoch: 29, train:[loss=0.081, AUC=0.752], test:[loss=0.081, AUC=0.748]\n",
      "epoch: 30, train:[loss=0.081, AUC=0.758], test:[loss=0.080, AUC=0.756]\n",
      "epoch: 31, train:[loss=0.081, AUC=0.758], test:[loss=0.080, AUC=0.756]\n",
      "epoch: 32, train:[loss=0.081, AUC=0.754], test:[loss=0.081, AUC=0.751]\n",
      "epoch: 33, train:[loss=0.081, AUC=0.755], test:[loss=0.081, AUC=0.753]\n",
      "epoch: 34, train:[loss=0.081, AUC=0.759], test:[loss=0.081, AUC=0.757]\n",
      "epoch: 35, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.753]\n",
      "epoch: 36, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 37, train:[loss=0.081, AUC=0.754], test:[loss=0.081, AUC=0.751]\n",
      "epoch: 38, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 39, train:[loss=0.081, AUC=0.754], test:[loss=0.081, AUC=0.751]\n",
      "epoch: 40, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 41, train:[loss=0.081, AUC=0.756], test:[loss=0.081, AUC=0.752]\n",
      "epoch: 42, train:[loss=0.082, AUC=0.756], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 43, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.756]\n",
      "epoch: 44, train:[loss=0.081, AUC=0.759], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 45, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 46, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 47, train:[loss=0.082, AUC=0.755], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 48, train:[loss=0.082, AUC=0.756], test:[loss=0.082, AUC=0.752]\n",
      "epoch: 49, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 50, train:[loss=0.081, AUC=0.759], test:[loss=0.081, AUC=0.757]\n",
      "epoch: 51, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 52, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 53, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.756]\n",
      "epoch: 54, train:[loss=0.081, AUC=0.754], test:[loss=0.081, AUC=0.751]\n",
      "epoch: 55, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 56, train:[loss=0.081, AUC=0.759], test:[loss=0.081, AUC=0.757]\n",
      "epoch: 57, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 58, train:[loss=0.080, AUC=0.761], test:[loss=0.080, AUC=0.759]\n",
      "epoch: 59, train:[loss=0.081, AUC=0.755], test:[loss=0.081, AUC=0.752]\n",
      "epoch: 60, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 61, train:[loss=0.080, AUC=0.761], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 62, train:[loss=0.081, AUC=0.758], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 63, train:[loss=0.081, AUC=0.759], test:[loss=0.081, AUC=0.757]\n",
      "epoch: 64, train:[loss=0.081, AUC=0.759], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 65, train:[loss=0.081, AUC=0.759], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 66, train:[loss=0.081, AUC=0.759], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 67, train:[loss=0.081, AUC=0.758], test:[loss=0.080, AUC=0.756]\n",
      "epoch: 68, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.755]\n",
      "epoch: 69, train:[loss=0.081, AUC=0.757], test:[loss=0.081, AUC=0.756]\n",
      "epoch: 70, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 71, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.756]\n",
      "epoch: 72, train:[loss=0.080, AUC=0.761], test:[loss=0.080, AUC=0.759]\n",
      "epoch: 73, train:[loss=0.081, AUC=0.758], test:[loss=0.081, AUC=0.754]\n",
      "epoch: 74, train:[loss=0.081, AUC=0.761], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 75, train:[loss=0.081, AUC=0.761], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 76, train:[loss=0.081, AUC=0.761], test:[loss=0.081, AUC=0.758]\n",
      "epoch: 77, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.757]\n",
      "epoch: 78, train:[loss=0.080, AUC=0.760], test:[loss=0.080, AUC=0.758]\n",
      "epoch: 79, train:[loss=0.081, AUC=0.761], test:[loss=0.081, AUC=0.758]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_train_d, t_train_d)\n",
    "loader = DataLoader(dataset, batch_size=290, shuffle=True)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(290, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "\n",
    "    for X, t in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(X)\n",
    "        loss = loss_fn(y, t)\n",
    "        # 傾きの計算\n",
    "        loss.backward()\n",
    "        # optimizerの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_train_d = model(X_train_d)\n",
    "    y_valid_d = model(X_valid_d)\n",
    "   #  平均二乗誤差 予測値と正解値の誤差の計算\n",
    "    loss_train = loss_fn(y_train_d, t_train_d)\n",
    "    loss_valid = loss_fn(y_valid_d, t_valid_d)\n",
    "    auc_train = roc_auc_score(t_train_d.detach().numpy(), y_train_d.detach().numpy())\n",
    "    auc_valid = roc_auc_score(t_valid_d.detach().numpy(), y_valid_d.detach().numpy())\n",
    "    \n",
    "    print('epoch: {}, train:[loss={:.3f}, AUC={:.3f}], test:[loss={:.3f}, AUC={:.3f}]'.  format(epoch, loss_train, auc_train, loss_valid, auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "87b23f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/769717037.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = t_pred_d\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/769717037.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = proba\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/769717037.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['pred'] = x['proba'].map(lambda x: 0 if x <= 0.7 else 1)\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/769717037.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['win_ratio'] = [(p / sum1.loc[i])[0] for i, p in t_pred_d.items()]\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/769717037.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['expected'] = x['odds'] * x['win_ratio']\n"
     ]
    }
   ],
   "source": [
    "x = valid1[['h_num', 'odds', 'popular']]\n",
    "t_pred_d = pd.Series(np.around(torch.flatten(y_valid_d).detach().numpy(), decimals=5), index=x.index)\n",
    "sum1 = pd.DataFrame(t_pred_d.groupby(level=0).sum())\n",
    "\n",
    "x['proba'] = t_pred_d\n",
    "proba = x[['proba']]\n",
    "standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "\n",
    "x['proba'] = proba\n",
    "x['pred'] = x['proba'].map(lambda x: 0 if x <= 0.7 else 1)\n",
    "x['win_ratio'] = [(p / sum1.loc[i])[0] for i, p in t_pred_d.items()]\n",
    "x['expected'] = x['odds'] * x['win_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "029a4f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>popular</th>\n",
       "      <th>proba</th>\n",
       "      <th>pred</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020121645110309</th>\n",
       "      <td>2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>0.457213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020121650000003</th>\n",
       "      <td>4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758078</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206049</td>\n",
       "      <td>0.679962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020121645110307</th>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807324</td>\n",
       "      <td>1</td>\n",
       "      <td>0.264423</td>\n",
       "      <td>0.555287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020121647140302</th>\n",
       "      <td>6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753260</td>\n",
       "      <td>1</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.422059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020121647140304</th>\n",
       "      <td>6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.763505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427313</td>\n",
       "      <td>0.769164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022041348010410</th>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843135</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333341</td>\n",
       "      <td>0.466677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022041343010302</th>\n",
       "      <td>7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.711983</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150113</td>\n",
       "      <td>0.645487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022041343010302</th>\n",
       "      <td>1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.712035</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150125</td>\n",
       "      <td>0.615513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022041343010303</th>\n",
       "      <td>5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.713045</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225828</td>\n",
       "      <td>0.767817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022041348010411</th>\n",
       "      <td>8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880785</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425563</td>\n",
       "      <td>0.638344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8741 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  h_num  odds  popular     proba  pred  win_ratio  expected\n",
       "race_id                                                                    \n",
       "2020121645110309      2   1.7        1  0.781984     1   0.268949  0.457213\n",
       "2020121650000003      4   3.3        1  0.758078     1   0.206049  0.679962\n",
       "2020121645110307      1   2.1        1  0.807324     1   0.264423  0.555287\n",
       "2020121647140302      6   1.2        1  0.753260     1   0.351716  0.422059\n",
       "2020121647140304      6   1.8        1  0.763505     1   0.427313  0.769164\n",
       "...                 ...   ...      ...       ...   ...        ...       ...\n",
       "2022041348010410      3   1.4        1  0.843135     1   0.333341  0.466677\n",
       "2022041343010302      7   4.3        3  0.711983     1   0.150113  0.645487\n",
       "2022041343010302      1   4.1        2  0.712035     1   0.150125  0.615513\n",
       "2022041343010303      5   3.4        2  0.713045     1   0.225828  0.767817\n",
       "2022041348010411      8   1.5        1  0.880785     1   0.425563  0.638344\n",
       "\n",
       "[8741 rows x 7 columns]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt1 = x[\n",
    "    (x['pred'] == 1)\n",
    "    &\n",
    "    (x['expected'] < 1)\n",
    "]\n",
    "bt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "8d9a2095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：8741 レース出現確率:52.3% 賭金:874,100円 配当合計:671,050円 最高配当:650円 的中数:3785 的中率:43.3% 回収率:76.8%\n"
     ]
    }
   ],
   "source": [
    "bh1 = bt1.merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} レース出現確率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    len(bt1),\\\n",
    "    len(bt1.groupby(level=0)) / len(X_valid1_d.groupby(level=0)),\\\n",
    "    len(bt1) * 100,\\\n",
    "    int(bh1[bh1['h_num'] == bh1['1着馬番']]['単勝'].sum()),\\\n",
    "    int(bh1[bh1['h_num'] == bh1['1着馬番']]['単勝'].max()),\\\n",
    "    len(bh1[bh1['h_num'] == bh1['1着馬番']]),\\\n",
    "    len(bh1[bh1['h_num'] == bh1['1着馬番']]) / (len(bt1)),\\\n",
    "    (bh1[bh1['h_num'] == bh1['1着馬番']]['単勝'].sum() / (len(bt1) * 100)), \\\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "b9aead80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：5145 レース出現確率:31.3% 賭金:514,500円 配当合計:414,110円 最高配当:570円 的中数:2550 的中率:49.6% 回収率:80.5%\n"
     ]
    }
   ],
   "source": [
    "bt2 = bt1.reset_index()\n",
    "bt3 = bt.reset_index()\n",
    "bt4 = pd.concat([bt3[['race_id', 'h_num', 'odds']], bt2[['race_id', 'h_num', 'odds']]])\n",
    "bt5 = bt4[bt4.duplicated()]\n",
    "\n",
    "# bt5 = bt5[bt5['odds'] < 3]\n",
    "bh4 = bt5.merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} レース出現確率:{:.1%} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    len(bh4),\\\n",
    "    len(bh4.groupby(level=0)) / len(X_valid1_d.groupby(level=0)),\\\n",
    "    len(bh4) * 100,\\\n",
    "    int(bh4[bh4['h_num'] == bh4['1着馬番']]['単勝'].sum()),\\\n",
    "    int(bh4[bh4['h_num'] == bh4['1着馬番']]['単勝'].max()),\\\n",
    "    len(bh4[bh4['h_num'] == bh4['1着馬番']]),\\\n",
    "    len(bh4[bh4['h_num'] == bh4['1着馬番']]) / (len(bh4)),\\\n",
    "    (bh4[bh4['h_num'] == bh4['1着馬番']]['単勝'].sum() / (len(bh4) * 100)), \\\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "41b636e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "賭金:514,500円 配当合計:457,390円 的中数:4030 的中率:78.3% 回収率:88.9%\n"
     ]
    }
   ],
   "source": [
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(bh4[bh4['h_num'] == bh4[s + '着馬番']]['複勝' + s])\n",
    "    money += bh4[bh4['h_num'] == bh4[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "print(\"賭金:{:,}円 配当合計:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    (len(bt5) * 100),\\\n",
    "    int(money),\\\n",
    "    f_c,\\\n",
    "    f_c / len(bt5),\\\n",
    "    (money / (len(bt5) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "07d73688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rr = df.set_index('race_id')\n",
    "all_rr = all_rr.fillna({\n",
    "'1走前騎手ID': 0, '1走前距離': 0, '1走前場所': 0,'1走前コース': 0,\n",
    "'2走前騎手ID': 0, '2走前距離': 0, '2走前場所': 0,'2走前コース': 0,\n",
    "'3走前騎手ID': 0, '3走前距離': 0, '3走前場所': 0,'3走前コース': 0,\n",
    "'4走前騎手ID': 0, '4走前距離': 0, '4走前場所': 0,'4走前コース': 0,\n",
    "'5走前騎手ID': 0, '5走前距離': 0, '5走前場所': 0,'5走前コース': 0,\n",
    "})\n",
    "\n",
    "all_rr['result'] = all_rr['result'].map(lambda x: 1 if x < 3 else 0)\n",
    "\n",
    "r_categorical = process_categorical(all_rr, [\n",
    "    'owner',  'course',\n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    'grade', 'age', 'place_id',\n",
    "    'stallion_id', 'affiliation_id',\n",
    "    '1走前騎手ID', '1走前場所', '1走前距離', '1走前コース',\n",
    "    '2走前騎手ID', '2走前場所', '2走前距離', '2走前コース',\n",
    "    '3走前騎手ID', '3走前場所', '3走前距離', '3走前コース',\n",
    "    '4走前騎手ID', '4走前場所', '4走前距離', '4走前コース',\n",
    "    '5走前騎手ID', '5走前場所', '5走前距離', '5走前コース',\n",
    "])\n",
    "\n",
    "r_categorical = r_categorical.reset_index()\n",
    "r_categorical = r_categorical.merge(vec, on='horse_id')\n",
    "r_categorical = r_categorical.set_index('race_id')\n",
    "\n",
    "r_train1, r_valid1 = split_data(r_categorical)\n",
    "r_valid1, r_test1 = train_valid_split_data(r_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "681b34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r_train1 = r_train1.drop(['id', 'date', 'body_weight','body_weight_in_de','result', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "y_r_train1 = r_train1['result']\n",
    "X_r_valid1 = r_valid1.drop(['date', 'body_weight','body_weight_in_de','result', 'popular',  'horse_id'], axis=1)\n",
    "y_r_valid1 = r_valid1['result']\n",
    "X_r_test1 = r_test1.drop(['date','body_weight','body_weight_in_de', 'result', 'popular',  'horse_id'], axis=1)\n",
    "y_r_test1 = r_test1['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "10dbba4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-04 09:24:29,257]\u001b[0m A new study created in memory with name: no-name-9b90569e-ed30-4b3c-a4dd-13521e364c1b\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.817422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  14%|#########                                                      | 1/7 [00:20<02:05, 20.94s/it]\u001b[32m[I 2022-11-04 09:24:50,232]\u001b[0m Trial 0 finished with value: 0.4930812681715393 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  14%|#########                                                      | 1/7 [00:20<02:05, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.449162\tvalid_1's binary_logloss: 0.493081\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.528975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  29%|##################                                             | 2/7 [00:35<01:24, 16.99s/it]\u001b[32m[I 2022-11-04 09:25:04,461]\u001b[0m Trial 1 finished with value: 0.4941311705527971 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  29%|##################                                             | 2/7 [00:35<01:24, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.461997\tvalid_1's binary_logloss: 0.494131\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  43%|###########################                                    | 3/7 [00:46<00:58, 14.63s/it]\u001b[32m[I 2022-11-04 09:25:16,292]\u001b[0m Trial 2 finished with value: 0.49329224194834087 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  43%|###########################                                    | 3/7 [00:47<00:58, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.454747\tvalid_1's binary_logloss: 0.493292\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  57%|####################################                           | 4/7 [01:01<00:44, 14.70s/it]\u001b[32m[I 2022-11-04 09:25:31,080]\u001b[0m Trial 3 finished with value: 0.4938162059135084 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  57%|####################################                           | 4/7 [01:01<00:44, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.437075\tvalid_1's binary_logloss: 0.493816\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.563830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  71%|#############################################                  | 5/7 [01:15<00:28, 14.32s/it]\u001b[32m[I 2022-11-04 09:25:44,753]\u001b[0m Trial 4 finished with value: 0.49318853682754743 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  71%|#############################################                  | 5/7 [01:15<00:28, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.449266\tvalid_1's binary_logloss: 0.493189\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.316025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.493081:  86%|######################################################         | 6/7 [01:33<00:15, 15.46s/it]\u001b[32m[I 2022-11-04 09:26:02,430]\u001b[0m Trial 5 finished with value: 0.4943013966702089 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.4930812681715393.\u001b[0m\n",
      "feature_fraction, val_score: 0.493081:  86%|######################################################         | 6/7 [01:33<00:15, 15.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.454752\tvalid_1's binary_logloss: 0.494301\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.482273: 100%|###############################################################| 7/7 [01:48<00:00, 15.48s/it]\u001b[32m[I 2022-11-04 09:26:17,935]\u001b[0m Trial 6 finished with value: 0.4822729069324955 and parameters: {'feature_fraction': 0.4}. Best is trial 6 with value: 0.4822729069324955.\u001b[0m\n",
      "feature_fraction, val_score: 0.482273: 100%|###############################################################| 7/7 [01:48<00:00, 15.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.426464\tvalid_1's binary_logloss: 0.482273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.482273:   0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.482273:   5%|###4                                                                | 1/20 [00:13<04:23, 13.87s/it]\u001b[32m[I 2022-11-04 09:26:31,839]\u001b[0m Trial 7 finished with value: 0.49012515563667264 and parameters: {'num_leaves': 18}. Best is trial 7 with value: 0.49012515563667264.\u001b[0m\n",
      "num_leaves, val_score: 0.482273:   5%|###4                                                                | 1/20 [00:13<04:23, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.43398\tvalid_1's binary_logloss: 0.490125\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  10%|######8                                                             | 2/20 [00:36<05:47, 19.31s/it]\u001b[32m[I 2022-11-04 09:26:54,960]\u001b[0m Trial 8 finished with value: 0.4555680871302051 and parameters: {'num_leaves': 181}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  10%|######8                                                             | 2/20 [00:37<05:47, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.400244\tvalid_1's binary_logloss: 0.455568\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  15%|##########2                                                         | 3/20 [01:01<06:06, 21.57s/it]\u001b[32m[I 2022-11-04 09:27:19,219]\u001b[0m Trial 9 finished with value: 0.45703638440549865 and parameters: {'num_leaves': 172}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  15%|##########2                                                         | 3/20 [01:01<06:06, 21.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's binary_logloss: 0.401749\tvalid_1's binary_logloss: 0.457036\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  20%|#############6                                                      | 4/20 [01:22<05:44, 21.56s/it]\u001b[32m[I 2022-11-04 09:27:40,769]\u001b[0m Trial 10 finished with value: 0.47543979989060997 and parameters: {'num_leaves': 202}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  20%|#############6                                                      | 4/20 [01:22<05:44, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.406937\tvalid_1's binary_logloss: 0.47544\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  25%|#################                                                   | 5/20 [01:41<05:07, 20.51s/it]\u001b[32m[I 2022-11-04 09:27:59,412]\u001b[0m Trial 11 finished with value: 0.47543979989060997 and parameters: {'num_leaves': 202}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  25%|#################                                                   | 5/20 [01:41<05:07, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's binary_logloss: 0.406937\tvalid_1's binary_logloss: 0.47544\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  30%|####################4                                               | 6/20 [02:01<04:46, 20.47s/it]\u001b[32m[I 2022-11-04 09:28:19,808]\u001b[0m Trial 12 finished with value: 0.4806126879215442 and parameters: {'num_leaves': 162}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  30%|####################4                                               | 6/20 [02:01<04:46, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.415633\tvalid_1's binary_logloss: 0.480613\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.455568:  35%|#######################7                                            | 7/20 [02:22<04:25, 20.39s/it]\u001b[32m[I 2022-11-04 09:28:40,013]\u001b[0m Trial 13 finished with value: 0.46320349132510036 and parameters: {'num_leaves': 112}. Best is trial 8 with value: 0.4555680871302051.\u001b[0m\n",
      "num_leaves, val_score: 0.455568:  35%|#######################7                                            | 7/20 [02:22<04:25, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's binary_logloss: 0.409153\tvalid_1's binary_logloss: 0.463203\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  40%|###########################2                                        | 8/20 [02:49<04:32, 22.73s/it]\u001b[32m[I 2022-11-04 09:29:07,783]\u001b[0m Trial 14 finished with value: 0.4509596350230751 and parameters: {'num_leaves': 196}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  40%|###########################2                                        | 8/20 [02:49<04:32, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.45096\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  45%|##############################6                                     | 9/20 [03:06<03:48, 20.81s/it]\u001b[32m[I 2022-11-04 09:29:24,365]\u001b[0m Trial 15 finished with value: 0.4818442130044487 and parameters: {'num_leaves': 256}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  45%|##############################6                                     | 9/20 [03:06<03:48, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.413186\tvalid_1's binary_logloss: 0.481844\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  50%|#################################5                                 | 10/20 [03:30<03:37, 21.73s/it]\u001b[32m[I 2022-11-04 09:29:48,146]\u001b[0m Trial 16 finished with value: 0.46249571704266046 and parameters: {'num_leaves': 89}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  50%|#################################5                                 | 10/20 [03:30<03:37, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's binary_logloss: 0.409126\tvalid_1's binary_logloss: 0.462496\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  55%|####################################8                              | 11/20 [03:48<03:06, 20.76s/it]\u001b[32m[I 2022-11-04 09:30:06,723]\u001b[0m Trial 17 finished with value: 0.4818442130044487 and parameters: {'num_leaves': 256}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  55%|####################################8                              | 11/20 [03:48<03:06, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.413186\tvalid_1's binary_logloss: 0.481844\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  60%|########################################1                          | 12/20 [04:21<03:14, 24.32s/it]\u001b[32m[I 2022-11-04 09:30:39,150]\u001b[0m Trial 18 finished with value: 0.4515118332358949 and parameters: {'num_leaves': 205}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  60%|########################################1                          | 12/20 [04:21<03:14, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's binary_logloss: 0.387739\tvalid_1's binary_logloss: 0.451512\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  65%|###########################################5                       | 13/20 [04:32<02:23, 20.49s/it]\u001b[32m[I 2022-11-04 09:30:50,857]\u001b[0m Trial 19 finished with value: 0.4948307600788385 and parameters: {'num_leaves': 222}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  65%|###########################################5                       | 13/20 [04:32<02:23, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.485906\tvalid_1's binary_logloss: 0.494831\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  70%|##############################################9                    | 14/20 [04:55<02:05, 21.00s/it]\u001b[32m[I 2022-11-04 09:31:13,017]\u001b[0m Trial 20 finished with value: 0.463130454590852 and parameters: {'num_leaves': 138}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  70%|##############################################9                    | 14/20 [04:55<02:05, 21.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.405627\tvalid_1's binary_logloss: 0.46313\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  75%|##################################################2                | 15/20 [05:13<01:41, 20.26s/it]\u001b[32m[I 2022-11-04 09:31:31,578]\u001b[0m Trial 21 finished with value: 0.4771424706580835 and parameters: {'num_leaves': 72}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  75%|##################################################2                | 15/20 [05:13<01:41, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.42012\tvalid_1's binary_logloss: 0.477142\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  80%|#####################################################6             | 16/20 [05:25<01:11, 17.88s/it]\u001b[32m[I 2022-11-04 09:31:43,920]\u001b[0m Trial 22 finished with value: 0.4948307600788385 and parameters: {'num_leaves': 225}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  80%|#####################################################6             | 16/20 [05:25<01:11, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.485891\tvalid_1's binary_logloss: 0.494831\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  85%|########################################################9          | 17/20 [05:44<00:54, 18.17s/it]\u001b[32m[I 2022-11-04 09:32:02,759]\u001b[0m Trial 23 finished with value: 0.48155096987044327 and parameters: {'num_leaves': 141}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  85%|########################################################9          | 17/20 [05:44<00:54, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.420283\tvalid_1's binary_logloss: 0.481551\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  90%|############################################################3      | 18/20 [05:57<00:33, 16.65s/it]\u001b[32m[I 2022-11-04 09:32:15,877]\u001b[0m Trial 24 finished with value: 0.4948307600788385 and parameters: {'num_leaves': 224}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  90%|############################################################3      | 18/20 [05:57<00:33, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.485896\tvalid_1's binary_logloss: 0.494831\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960:  95%|###############################################################6   | 19/20 [06:25<00:19, 19.80s/it]\u001b[32m[I 2022-11-04 09:32:43,004]\u001b[0m Trial 25 finished with value: 0.4509596350230751 and parameters: {'num_leaves': 196}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960:  95%|###############################################################6   | 19/20 [06:25<00:19, 19.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.45096\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.450960: 100%|###################################################################| 20/20 [06:39<00:00, 18.15s/it]\u001b[32m[I 2022-11-04 09:32:57,321]\u001b[0m Trial 26 finished with value: 0.4902526873568268 and parameters: {'num_leaves': 24}. Best is trial 14 with value: 0.4509596350230751.\u001b[0m\n",
      "num_leaves, val_score: 0.450960: 100%|###################################################################| 20/20 [06:39<00:00, 19.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.432664\tvalid_1's binary_logloss: 0.490253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  10%|#######1                                                               | 1/10 [00:11<01:40, 11.17s/it]\u001b[32m[I 2022-11-04 09:33:08,533]\u001b[0m Trial 27 finished with value: 0.4947562065988081 and parameters: {'bagging_fraction': 0.5979058885443466, 'bagging_freq': 4}. Best is trial 27 with value: 0.4947562065988081.\u001b[0m\n",
      "bagging, val_score: 0.450960:  10%|#######1                                                               | 1/10 [00:11<01:40, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477154\tvalid_1's binary_logloss: 0.494756\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  20%|##############2                                                        | 2/10 [00:29<02:03, 15.41s/it]\u001b[32m[I 2022-11-04 09:33:26,896]\u001b[0m Trial 28 finished with value: 0.47096940011179245 and parameters: {'bagging_fraction': 0.5612915636186714, 'bagging_freq': 6}. Best is trial 28 with value: 0.47096940011179245.\u001b[0m\n",
      "bagging, val_score: 0.450960:  20%|##############2                                                        | 2/10 [00:29<02:03, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.40774\tvalid_1's binary_logloss: 0.470969\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  30%|#####################3                                                 | 3/10 [00:55<02:22, 20.35s/it]\u001b[32m[I 2022-11-04 09:33:53,124]\u001b[0m Trial 29 finished with value: 0.4537163692685771 and parameters: {'bagging_fraction': 0.8858080535639756, 'bagging_freq': 6}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  30%|#####################3                                                 | 3/10 [00:55<02:22, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.397703\tvalid_1's binary_logloss: 0.453716\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  40%|############################4                                          | 4/10 [01:11<01:51, 18.54s/it]\u001b[32m[I 2022-11-04 09:34:08,903]\u001b[0m Trial 30 finished with value: 0.4964179887912122 and parameters: {'bagging_fraction': 0.4604043642613606, 'bagging_freq': 5}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  40%|############################4                                          | 4/10 [01:11<01:51, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486065\tvalid_1's binary_logloss: 0.496418\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  50%|###################################5                                   | 5/10 [01:25<01:25, 17.04s/it]\u001b[32m[I 2022-11-04 09:34:23,281]\u001b[0m Trial 31 finished with value: 0.49038036314831845 and parameters: {'bagging_fraction': 0.6812157323692081, 'bagging_freq': 7}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  50%|###################################5                                   | 5/10 [01:25<01:25, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.439605\tvalid_1's binary_logloss: 0.49038\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  60%|##########################################6                            | 6/10 [01:36<00:59, 14.77s/it]\u001b[32m[I 2022-11-04 09:34:33,631]\u001b[0m Trial 32 finished with value: 0.49495062140418533 and parameters: {'bagging_fraction': 0.4329337448782027, 'bagging_freq': 2}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  60%|##########################################6                            | 6/10 [01:36<00:59, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486098\tvalid_1's binary_logloss: 0.494951\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  70%|#################################################6                     | 7/10 [01:46<00:40, 13.33s/it]\u001b[32m[I 2022-11-04 09:34:44,015]\u001b[0m Trial 33 finished with value: 0.49416852981060344 and parameters: {'bagging_fraction': 0.8366335009973871, 'bagging_freq': 5}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  70%|#################################################6                     | 7/10 [01:46<00:40, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486097\tvalid_1's binary_logloss: 0.494169\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  80%|########################################################8              | 8/10 [02:03<00:28, 14.48s/it]\u001b[32m[I 2022-11-04 09:35:00,948]\u001b[0m Trial 34 finished with value: 0.4695219119755655 and parameters: {'bagging_fraction': 0.5924101341477451, 'bagging_freq': 1}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  80%|########################################################8              | 8/10 [02:03<00:28, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.407565\tvalid_1's binary_logloss: 0.469522\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960:  90%|###############################################################9       | 9/10 [02:13<00:12, 12.98s/it]\u001b[32m[I 2022-11-04 09:35:10,637]\u001b[0m Trial 35 finished with value: 0.49467973301356566 and parameters: {'bagging_fraction': 0.4234330319545699, 'bagging_freq': 7}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960:  90%|###############################################################9       | 9/10 [02:13<00:12, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486104\tvalid_1's binary_logloss: 0.49468\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.450960: 100%|######################################################################| 10/10 [02:23<00:00, 12.13s/it]\u001b[32m[I 2022-11-04 09:35:20,868]\u001b[0m Trial 36 finished with value: 0.4944089376379044 and parameters: {'bagging_fraction': 0.8795931391239296, 'bagging_freq': 2}. Best is trial 29 with value: 0.4537163692685771.\u001b[0m\n",
      "bagging, val_score: 0.450960: 100%|######################################################################| 10/10 [02:23<00:00, 14.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477154\tvalid_1's binary_logloss: 0.494409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.450960:   0%|                                                                | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.450960:  33%|##################6                                     | 1/3 [00:11<00:22, 11.01s/it]\u001b[32m[I 2022-11-04 09:35:31,910]\u001b[0m Trial 37 finished with value: 0.4969974767878087 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 37 with value: 0.4969974767878087.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.450960:  33%|##################6                                     | 1/3 [00:11<00:22, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486059\tvalid_1's binary_logloss: 0.496997\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.450960:  67%|#####################################3                  | 2/3 [00:25<00:12, 12.89s/it]\u001b[32m[I 2022-11-04 09:35:46,122]\u001b[0m Trial 38 finished with value: 0.4906163048207774 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 38 with value: 0.4906163048207774.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.450960:  67%|#####################################3                  | 2/3 [00:25<00:12, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.434631\tvalid_1's binary_logloss: 0.490616\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.450960: 100%|########################################################| 3/3 [00:36<00:00, 11.97s/it]\u001b[32m[I 2022-11-04 09:35:56,987]\u001b[0m Trial 39 finished with value: 0.49706619478613495 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 38 with value: 0.4906163048207774.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.450960: 100%|########################################################| 3/3 [00:36<00:00, 12.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.486051\tvalid_1's binary_logloss: 0.497066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.450960:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.450960:   5%|##8                                                     | 1/20 [00:22<07:02, 22.25s/it]\u001b[32m[I 2022-11-04 09:36:19,263]\u001b[0m Trial 40 finished with value: 0.4601791317582196 and parameters: {'lambda_l1': 0.009032491001406113, 'lambda_l2': 0.3665201619488542}. Best is trial 40 with value: 0.4601791317582196.\u001b[0m\n",
      "regularization_factors, val_score: 0.450960:   5%|##8                                                     | 1/20 [00:22<07:02, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.394038\tvalid_1's binary_logloss: 0.460179\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.450960:  10%|#####6                                                  | 2/20 [00:44<06:42, 22.37s/it]\u001b[32m[I 2022-11-04 09:36:41,728]\u001b[0m Trial 41 finished with value: 0.4558706217961636 and parameters: {'lambda_l1': 3.517963343163258e-05, 'lambda_l2': 0.46137865837278313}. Best is trial 41 with value: 0.4558706217961636.\u001b[0m\n",
      "regularization_factors, val_score: 0.450960:  10%|#####6                                                  | 2/20 [00:44<06:42, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.394062\tvalid_1's binary_logloss: 0.455871\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.450960:  15%|########4                                               | 3/20 [01:07<06:26, 22.73s/it]\u001b[32m[I 2022-11-04 09:37:04,884]\u001b[0m Trial 42 finished with value: 0.4509974420431004 and parameters: {'lambda_l1': 1.1589124220122786e-06, 'lambda_l2': 1.3064673042521652e-06}. Best is trial 42 with value: 0.4509974420431004.\u001b[0m\n",
      "regularization_factors, val_score: 0.450960:  15%|########4                                               | 3/20 [01:07<06:26, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.450997\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  20%|###########2                                            | 4/20 [01:29<05:56, 22.31s/it]\u001b[32m[I 2022-11-04 09:37:26,547]\u001b[0m Trial 43 finished with value: 0.4463946820048444 and parameters: {'lambda_l1': 1.670012959564408e-05, 'lambda_l2': 0.07300648623169456}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  20%|###########2                                            | 4/20 [01:29<05:56, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's binary_logloss: 0.394489\tvalid_1's binary_logloss: 0.446395\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  25%|##############                                          | 5/20 [01:45<04:58, 19.89s/it]\u001b[32m[I 2022-11-04 09:37:42,158]\u001b[0m Trial 44 finished with value: 0.48157859996375135 and parameters: {'lambda_l1': 0.0021723596708245205, 'lambda_l2': 1.8854755810334696}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  25%|##############                                          | 5/20 [01:45<04:58, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.41769\tvalid_1's binary_logloss: 0.481579\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  30%|################8                                       | 6/20 [02:02<04:26, 19.07s/it]\u001b[32m[I 2022-11-04 09:37:59,633]\u001b[0m Trial 45 finished with value: 0.48322206500420756 and parameters: {'lambda_l1': 8.711503033136818, 'lambda_l2': 0.007768233491930022}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  30%|################8                                       | 6/20 [02:02<04:26, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.41152\tvalid_1's binary_logloss: 0.483222\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  35%|###################5                                    | 7/20 [02:17<03:49, 17.65s/it]\u001b[32m[I 2022-11-04 09:38:14,369]\u001b[0m Trial 46 finished with value: 0.48550610024457796 and parameters: {'lambda_l1': 1.2816475143547728, 'lambda_l2': 2.5062226451256074e-08}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  35%|###################5                                    | 7/20 [02:17<03:49, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.418474\tvalid_1's binary_logloss: 0.485506\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  40%|######################4                                 | 8/20 [02:41<03:55, 19.64s/it]\u001b[32m[I 2022-11-04 09:38:38,265]\u001b[0m Trial 47 finished with value: 0.46074987800784145 and parameters: {'lambda_l1': 0.0019331346962833226, 'lambda_l2': 0.00011553093020533417}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  40%|######################4                                 | 8/20 [02:41<03:55, 19.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.387917\tvalid_1's binary_logloss: 0.46075\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  45%|#########################2                              | 9/20 [02:58<03:26, 18.76s/it]\u001b[32m[I 2022-11-04 09:38:55,119]\u001b[0m Trial 48 finished with value: 0.4782527235179081 and parameters: {'lambda_l1': 0.00019137238968861307, 'lambda_l2': 0.0011143609804475782}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  45%|#########################2                              | 9/20 [02:58<03:26, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's binary_logloss: 0.41645\tvalid_1's binary_logloss: 0.478253\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  50%|###########################5                           | 10/20 [03:18<03:11, 19.12s/it]\u001b[32m[I 2022-11-04 09:39:15,035]\u001b[0m Trial 49 finished with value: 0.4813660949592826 and parameters: {'lambda_l1': 1.091335768262205e-05, 'lambda_l2': 0.0001412450149282092}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  50%|###########################5                           | 10/20 [03:18<03:11, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.414288\tvalid_1's binary_logloss: 0.481366\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  55%|##############################2                        | 11/20 [03:40<03:01, 20.19s/it]\u001b[32m[I 2022-11-04 09:39:37,644]\u001b[0m Trial 50 finished with value: 0.4616285604689383 and parameters: {'lambda_l1': 1.331289012290375e-08, 'lambda_l2': 0.02169910125033279}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  55%|##############################2                        | 11/20 [03:40<03:01, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.398217\tvalid_1's binary_logloss: 0.461629\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  60%|#################################                      | 12/20 [04:03<02:48, 21.11s/it]\u001b[32m[I 2022-11-04 09:40:00,858]\u001b[0m Trial 51 finished with value: 0.4509636004101037 and parameters: {'lambda_l1': 2.573712844298389e-07, 'lambda_l2': 9.176486592703995e-07}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  60%|#################################                      | 12/20 [04:03<02:48, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.450964\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  65%|###################################7                   | 13/20 [04:26<02:32, 21.73s/it]\u001b[32m[I 2022-11-04 09:40:24,005]\u001b[0m Trial 52 finished with value: 0.45096492292041324 and parameters: {'lambda_l1': 1.7710470297713848e-07, 'lambda_l2': 2.0516866559272992e-06}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  65%|###################################7                   | 13/20 [04:27<02:32, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.450965\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  70%|######################################5                | 14/20 [04:50<02:12, 22.12s/it]\u001b[32m[I 2022-11-04 09:40:47,027]\u001b[0m Trial 53 finished with value: 0.4510034950461787 and parameters: {'lambda_l1': 1.2360926528082727e-06, 'lambda_l2': 2.0087030201694077e-06}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  70%|######################################5                | 14/20 [04:50<02:12, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.451003\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  75%|#########################################2             | 15/20 [05:13<01:52, 22.42s/it]\u001b[32m[I 2022-11-04 09:41:10,145]\u001b[0m Trial 54 finished with value: 0.4509973312317263 and parameters: {'lambda_l1': 1.7369230017030675e-08, 'lambda_l2': 5.7242829267589606e-08}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  75%|#########################################2             | 15/20 [05:13<01:52, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's binary_logloss: 0.388534\tvalid_1's binary_logloss: 0.450997\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  80%|############################################           | 16/20 [05:32<01:25, 21.40s/it]\u001b[32m[I 2022-11-04 09:41:29,193]\u001b[0m Trial 55 finished with value: 0.4748276224141343 and parameters: {'lambda_l1': 7.5722511641609394e-06, 'lambda_l2': 3.218962459129022e-05}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  80%|############################################           | 16/20 [05:32<01:25, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.401369\tvalid_1's binary_logloss: 0.474828\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  85%|##############################################7        | 17/20 [05:52<01:03, 21.22s/it]\u001b[32m[I 2022-11-04 09:41:49,981]\u001b[0m Trial 56 finished with value: 0.4730346873018601 and parameters: {'lambda_l1': 4.394167781116005e-07, 'lambda_l2': 0.005039643712922152}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  85%|##############################################7        | 17/20 [05:52<01:03, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.400346\tvalid_1's binary_logloss: 0.473035\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  90%|#################################################5     | 18/20 [06:05<00:37, 18.74s/it]\u001b[32m[I 2022-11-04 09:42:02,964]\u001b[0m Trial 57 finished with value: 0.49241376476084636 and parameters: {'lambda_l1': 0.08421179665130478, 'lambda_l2': 0.05450805367137949}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  90%|#################################################5     | 18/20 [06:05<00:37, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.429971\tvalid_1's binary_logloss: 0.492414\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395:  95%|####################################################2  | 19/20 [06:25<00:19, 19.09s/it]\u001b[32m[I 2022-11-04 09:42:22,841]\u001b[0m Trial 58 finished with value: 0.47483074790058 and parameters: {'lambda_l1': 0.0001302665991271549, 'lambda_l2': 1.7245383093280079e-07}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395:  95%|####################################################2  | 19/20 [06:25<00:19, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.401369\tvalid_1's binary_logloss: 0.474831\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.446395: 100%|#######################################################| 20/20 [06:46<00:00, 19.62s/it]\u001b[32m[I 2022-11-04 09:42:43,720]\u001b[0m Trial 59 finished with value: 0.47482797689109707 and parameters: {'lambda_l1': 7.935137995105376e-08, 'lambda_l2': 2.4346577797622712e-05}. Best is trial 43 with value: 0.4463946820048444.\u001b[0m\n",
      "regularization_factors, val_score: 0.446395: 100%|#######################################################| 20/20 [06:46<00:00, 20.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.401369\tvalid_1's binary_logloss: 0.474828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395:   0%|                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395:  20%|############6                                                  | 1/5 [00:14<00:59, 14.97s/it]\u001b[32m[I 2022-11-04 09:42:58,728]\u001b[0m Trial 60 finished with value: 0.4708014482183718 and parameters: {'min_child_samples': 10}. Best is trial 60 with value: 0.4708014482183718.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.446395:  20%|############6                                                  | 1/5 [00:15<00:59, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.417118\tvalid_1's binary_logloss: 0.470801\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395:  40%|#########################2                                     | 2/5 [00:30<00:46, 15.37s/it]\u001b[32m[I 2022-11-04 09:43:14,380]\u001b[0m Trial 61 finished with value: 0.4821595313506966 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 0.4708014482183718.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.446395:  40%|#########################2                                     | 2/5 [00:30<00:46, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.414146\tvalid_1's binary_logloss: 0.48216\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395:  60%|#####################################8                         | 3/5 [00:46<00:31, 15.55s/it]\u001b[32m[I 2022-11-04 09:43:30,138]\u001b[0m Trial 62 finished with value: 0.4814114270960426 and parameters: {'min_child_samples': 25}. Best is trial 60 with value: 0.4708014482183718.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.446395:  60%|#####################################8                         | 3/5 [00:46<00:31, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's binary_logloss: 0.414468\tvalid_1's binary_logloss: 0.481411\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395:  80%|##################################################4            | 4/5 [01:06<00:17, 17.35s/it]\u001b[32m[I 2022-11-04 09:43:50,243]\u001b[0m Trial 63 finished with value: 0.46464264745052297 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.46464264745052297.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.446395:  80%|##################################################4            | 4/5 [01:06<00:17, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.402512\tvalid_1's binary_logloss: 0.464643\n",
      "[LightGBM] [Info] Number of positive: 109473, number of negative: 443211\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 34962\n",
      "[LightGBM] [Info] Number of data points in the train set: 552684, number of used features: 288\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198075 -> initscore=-1.398368\n",
      "[LightGBM] [Info] Start training from score -1.398368\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.446395: 100%|###############################################################| 5/5 [01:28<00:00, 18.99s/it]\u001b[32m[I 2022-11-04 09:44:12,155]\u001b[0m Trial 64 finished with value: 0.45465692999140567 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 0.45465692999140567.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.446395: 100%|###############################################################| 5/5 [01:28<00:00, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's binary_logloss: 0.399933\tvalid_1's binary_logloss: 0.454657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "lgb_r_train = lgb_o.Dataset(X_r_train1.values, y_r_train1.values)\n",
    "lgb_r_valid = lgb_o.Dataset(X_r_valid1.values, y_r_valid1.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_r_train, lgb_r_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c07608b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 1.670012959564408e-05,\n",
       " 'lambda_l2': 0.07300648623169456,\n",
       " 'num_leaves': 196,\n",
       " 'feature_fraction': 0.4,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "ae79cfdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.07300648623169456, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07300648623169456\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.670012959564408e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.670012959564408e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=1.0, bagging_freq=0, feature_fraction=0.4,\n",
       "               feature_pre_filter=False, lambda_l1=1.670012959564408e-05,\n",
       "               lambda_l2=0.07300648623169456, num_iterations=1000,\n",
       "               num_leaves=196, objective='binary', random_state=100)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "'objective': 'binary',\n",
    "'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 1.670012959564408e-05,\n",
    " 'lambda_l2': 0.07300648623169456,\n",
    " 'num_leaves': 196,\n",
    " 'feature_fraction': 0.4,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "r_lgb_clf = lgb.LGBMClassifier(**params)\n",
    "r_lgb_clf.fit(X_train1.values, y_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "32d6cce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766566930987426"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_me_valid = ModelEvaluator(r_lgb_clf, haitou, std=True)\n",
    "f_me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "b658d2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_me_valid = ModelEvaluator(r_lgb_clf, haitou, std=True)\n",
    "wrr = f_me_valid.pred_table(X_valid1, 0.65, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "841c65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_f = wrr.reset_index()\n",
    "rt = bt5[['race_id', 'h_num']].merge(wr_f[['race_id', 'h_num']], on='race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "1a90663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt = rt[\n",
    "    rt['h_num_x'] != rt['h_num_y']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4b562627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1121 的中数:229 的中率:20.4%\n",
      "馬連賭金:112,100円 馬連配当合計:94,830円 馬連最高配当:3,070円 馬連回収率:84.6%\n",
      "馬単賭金:224,200円 馬単配当合計:179,460円 馬単最高配当:7,710円 馬単回収率:80.0%\n"
     ]
    }
   ],
   "source": [
    "uma_haito = rtt.merge(haitou[['1着馬番', '2着馬番', '馬連', '馬単']], on='race_id')\n",
    "\n",
    "f_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "    len(uma_haito),\\\n",
    "    len(f_bt),\\\n",
    "    len(f_bt) / len(uma_haito)\n",
    "))\n",
    "\n",
    "print(\"馬連賭金:{:,}円 馬連配当合計:{:,}円 馬連最高配当:{:,}円 馬連回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 100),\\\n",
    "    int(f_bt['馬連'].sum()),\\\n",
    "    int(f_bt['馬連'].max()),\\\n",
    "    (f_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "))\n",
    "\n",
    "print(\"馬単賭金:{:,}円 馬単配当合計:{:,}円 馬単最高配当:{:,}円 馬単回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 200),\\\n",
    "    int(f_bt['馬単'].sum()),\\\n",
    "    int(f_bt['馬単'].max()),\\\n",
    "    (f_bt['馬単'].sum() / (len(uma_haito) * 200))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "efcac70c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数:15107 賭金:1,510,700円 配当合計:1,284,450円 的中数:9963 的中率:65.9% 回収率:85.0%\n"
     ]
    }
   ],
   "source": [
    "h = wrr.merge(haitou, on='race_id')\n",
    "\n",
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(h[h['h_num'] == h[s + '着馬番']]['複勝' + s])\n",
    "    money += h[h['h_num'] == h[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "print(\"点数:{} 賭金:{:,}円 配当合計:{:,}円 的中数:{} 的中率:{:.1%} 回収率:{:.1%}\". format(\n",
    "    len(wrr),\\\n",
    "    (len(wrr) * 100),\\\n",
    "    int(money),\\\n",
    "    f_c,\\\n",
    "    f_c / len(wrr),\\\n",
    "    (money / (len(wrr) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6cdc8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "cc = r_categorical.copy()\n",
    "result_r = cc.fillna(0)\n",
    "\n",
    "train1_r, valid1_r  = split_data(result_r)\n",
    "valid1_r, test1_r  = train_valid_split_data(valid1_r)\n",
    "\n",
    "X_train1_r  = train1_r.drop(['id', 'date', 'result', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "t_train1_r  = train1_r['result']\n",
    "X_valid1_r  = valid1_r.drop(['id', 'date', 'result', 'horse_id'], axis=1)\n",
    "t_valid1_r  = valid1_r['result']\n",
    "\n",
    "X_train_r = torch.Tensor(X_train1_r.values)\n",
    "t_train_r = torch.Tensor(t_train1_r.values)\n",
    "X_valid_r = torch.Tensor(X_valid1_r.drop(['odds', 'popular'], axis=1).values)\n",
    "t_valid_r = torch.Tensor(t_valid1_r.values)\n",
    "\n",
    "t_train_r = t_train_r.reshape([-1, 1])\n",
    "t_valid_r = t_valid_r.reshape([-1, 1])\n",
    "\n",
    "X_test1_r = test1.drop(['id', 'date', 'result', 'horse_id'], axis=1)\n",
    "t_test1_r = test1['result']\n",
    "X_test_r = torch.Tensor(X_test1_r.drop(['odds', 'popular'], axis=1).values)\n",
    "t_test_r = torch.Tensor(t_test1_r.values)\n",
    "\n",
    "t_test_r = t_test_r.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b447d57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train:[loss=0.158, AUC=0.715], test:[loss=0.159, AUC=0.711]\n",
      "epoch: 1, train:[loss=0.142, AUC=0.725], test:[loss=0.141, AUC=0.726]\n",
      "epoch: 2, train:[loss=0.143, AUC=0.731], test:[loss=0.143, AUC=0.730]\n",
      "epoch: 3, train:[loss=0.140, AUC=0.737], test:[loss=0.139, AUC=0.738]\n",
      "epoch: 4, train:[loss=0.145, AUC=0.736], test:[loss=0.144, AUC=0.735]\n",
      "epoch: 5, train:[loss=0.143, AUC=0.732], test:[loss=0.142, AUC=0.734]\n",
      "epoch: 6, train:[loss=0.142, AUC=0.740], test:[loss=0.142, AUC=0.740]\n",
      "epoch: 7, train:[loss=0.140, AUC=0.739], test:[loss=0.139, AUC=0.741]\n",
      "epoch: 8, train:[loss=0.139, AUC=0.742], test:[loss=0.139, AUC=0.742]\n",
      "epoch: 9, train:[loss=0.139, AUC=0.743], test:[loss=0.138, AUC=0.743]\n",
      "epoch: 10, train:[loss=0.139, AUC=0.743], test:[loss=0.138, AUC=0.742]\n",
      "epoch: 11, train:[loss=0.142, AUC=0.740], test:[loss=0.142, AUC=0.740]\n",
      "epoch: 12, train:[loss=0.139, AUC=0.742], test:[loss=0.139, AUC=0.743]\n",
      "epoch: 13, train:[loss=0.139, AUC=0.743], test:[loss=0.138, AUC=0.743]\n",
      "epoch: 14, train:[loss=0.139, AUC=0.745], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 15, train:[loss=0.143, AUC=0.743], test:[loss=0.142, AUC=0.744]\n",
      "epoch: 16, train:[loss=0.139, AUC=0.744], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 17, train:[loss=0.140, AUC=0.743], test:[loss=0.140, AUC=0.742]\n",
      "epoch: 18, train:[loss=0.142, AUC=0.742], test:[loss=0.142, AUC=0.742]\n",
      "epoch: 19, train:[loss=0.138, AUC=0.744], test:[loss=0.138, AUC=0.743]\n",
      "epoch: 20, train:[loss=0.139, AUC=0.745], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 21, train:[loss=0.141, AUC=0.743], test:[loss=0.141, AUC=0.742]\n",
      "epoch: 22, train:[loss=0.139, AUC=0.746], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 23, train:[loss=0.138, AUC=0.744], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 24, train:[loss=0.139, AUC=0.743], test:[loss=0.138, AUC=0.743]\n",
      "epoch: 25, train:[loss=0.139, AUC=0.741], test:[loss=0.139, AUC=0.741]\n",
      "epoch: 26, train:[loss=0.139, AUC=0.746], test:[loss=0.139, AUC=0.745]\n",
      "epoch: 27, train:[loss=0.139, AUC=0.745], test:[loss=0.138, AUC=0.744]\n",
      "epoch: 28, train:[loss=0.139, AUC=0.744], test:[loss=0.138, AUC=0.744]\n",
      "epoch: 29, train:[loss=0.139, AUC=0.746], test:[loss=0.138, AUC=0.747]\n",
      "epoch: 30, train:[loss=0.138, AUC=0.746], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 31, train:[loss=0.138, AUC=0.746], test:[loss=0.138, AUC=0.747]\n",
      "epoch: 32, train:[loss=0.138, AUC=0.747], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 33, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.747]\n",
      "epoch: 34, train:[loss=0.139, AUC=0.742], test:[loss=0.139, AUC=0.741]\n",
      "epoch: 35, train:[loss=0.139, AUC=0.746], test:[loss=0.138, AUC=0.744]\n",
      "epoch: 36, train:[loss=0.138, AUC=0.747], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 37, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.748]\n",
      "epoch: 38, train:[loss=0.139, AUC=0.743], test:[loss=0.139, AUC=0.742]\n",
      "epoch: 39, train:[loss=0.140, AUC=0.741], test:[loss=0.139, AUC=0.740]\n",
      "epoch: 40, train:[loss=0.138, AUC=0.745], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 41, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.748]\n",
      "epoch: 42, train:[loss=0.138, AUC=0.747], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 43, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.747]\n",
      "epoch: 44, train:[loss=0.138, AUC=0.745], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 45, train:[loss=0.141, AUC=0.746], test:[loss=0.140, AUC=0.746]\n",
      "epoch: 46, train:[loss=0.139, AUC=0.744], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 47, train:[loss=0.140, AUC=0.748], test:[loss=0.139, AUC=0.748]\n",
      "epoch: 48, train:[loss=0.139, AUC=0.747], test:[loss=0.138, AUC=0.747]\n",
      "epoch: 49, train:[loss=0.140, AUC=0.747], test:[loss=0.139, AUC=0.748]\n",
      "epoch: 50, train:[loss=0.138, AUC=0.748], test:[loss=0.138, AUC=0.748]\n",
      "epoch: 51, train:[loss=0.140, AUC=0.747], test:[loss=0.139, AUC=0.747]\n",
      "epoch: 52, train:[loss=0.138, AUC=0.748], test:[loss=0.138, AUC=0.748]\n",
      "epoch: 53, train:[loss=0.138, AUC=0.744], test:[loss=0.138, AUC=0.743]\n",
      "epoch: 54, train:[loss=0.139, AUC=0.747], test:[loss=0.138, AUC=0.748]\n",
      "epoch: 55, train:[loss=0.139, AUC=0.748], test:[loss=0.139, AUC=0.748]\n",
      "epoch: 56, train:[loss=0.140, AUC=0.748], test:[loss=0.139, AUC=0.749]\n",
      "epoch: 57, train:[loss=0.138, AUC=0.746], test:[loss=0.139, AUC=0.744]\n",
      "epoch: 58, train:[loss=0.138, AUC=0.746], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 59, train:[loss=0.138, AUC=0.745], test:[loss=0.138, AUC=0.744]\n",
      "epoch: 60, train:[loss=0.139, AUC=0.748], test:[loss=0.138, AUC=0.749]\n",
      "epoch: 61, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.749]\n",
      "epoch: 62, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.748]\n",
      "epoch: 63, train:[loss=0.140, AUC=0.747], test:[loss=0.139, AUC=0.747]\n",
      "epoch: 64, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.747]\n",
      "epoch: 65, train:[loss=0.137, AUC=0.748], test:[loss=0.137, AUC=0.749]\n",
      "epoch: 66, train:[loss=0.139, AUC=0.746], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 67, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.749]\n",
      "epoch: 68, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.748]\n",
      "epoch: 69, train:[loss=0.139, AUC=0.747], test:[loss=0.139, AUC=0.747]\n",
      "epoch: 70, train:[loss=0.138, AUC=0.747], test:[loss=0.138, AUC=0.748]\n",
      "epoch: 71, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.748]\n",
      "epoch: 72, train:[loss=0.138, AUC=0.747], test:[loss=0.138, AUC=0.746]\n",
      "epoch: 73, train:[loss=0.139, AUC=0.747], test:[loss=0.138, AUC=0.747]\n",
      "epoch: 74, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.749]\n",
      "epoch: 75, train:[loss=0.138, AUC=0.746], test:[loss=0.138, AUC=0.745]\n",
      "epoch: 76, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.747]\n",
      "epoch: 77, train:[loss=0.138, AUC=0.749], test:[loss=0.138, AUC=0.748]\n",
      "epoch: 78, train:[loss=0.138, AUC=0.747], test:[loss=0.137, AUC=0.747]\n",
      "epoch: 79, train:[loss=0.138, AUC=0.748], test:[loss=0.137, AUC=0.749]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_train_r, t_train_r)\n",
    "loader = DataLoader(dataset, batch_size=290, shuffle=True)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(290, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(80):\n",
    "    model.train()\n",
    "\n",
    "    for X, t in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(X)\n",
    "        loss = loss_fn(y, t)\n",
    "        # 傾きの計算\n",
    "        loss.backward()\n",
    "        # optimizerの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_train_r = model(X_train_r)\n",
    "    y_valid_r = model(X_valid_r)\n",
    "   #  平均二乗誤差 予測値と正解値の誤差の計算\n",
    "    loss_train = loss_fn(y_train_r, t_train_r)\n",
    "    loss_valid = loss_fn(y_valid_r, t_valid_r)\n",
    "    auc_train = roc_auc_score(t_train_r.detach().numpy(), y_train_r.detach().numpy())\n",
    "    auc_valid = roc_auc_score(t_valid_r.detach().numpy(), y_valid_r.detach().numpy())\n",
    "    \n",
    "    print('epoch: {}, train:[loss={:.3f}, AUC={:.3f}], test:[loss={:.3f}, AUC={:.3f}]'.  format(epoch, loss_train, auc_train, loss_valid, auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "94f0426d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/986927126.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xd['proba'] = t_pred_d\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/986927126.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xd['proba'] = proba\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_39183/986927126.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xd['pred'] = xd['proba'].map(lambda xd: 0 if xd <= 0.65 else 1)\n"
     ]
    }
   ],
   "source": [
    "xd = valid1[['h_num', 'odds', 'popular']]\n",
    "t_pred_d = pd.Series(np.around(torch.flatten(y_valid_d).detach().numpy(), decimals=5), index=xd.index)\n",
    "sum1 = pd.DataFrame(t_pred_d.groupby(level=0).sum())\n",
    "\n",
    "xd['proba'] = t_pred_d\n",
    "proba = xd[['proba']]\n",
    "standard_scaler = lambda xd: (xd - xd.mean()) / xd.std()\n",
    "proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "\n",
    "xd['proba'] = proba\n",
    "xd['pred'] = xd['proba'].map(lambda xd: 0 if xd <= 0.65 else 1)\n",
    "xd = xd[xd['pred'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ff80d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：619 的中数:127 的中率:20.5%\n",
      "馬連賭金:61,900円 馬連配当合計:42,510円 馬連最高配当:910円 馬連回収率:68.7%\n",
      "馬単賭金:123,800円 馬単配当合計:83,530円 馬単最高配当:3,200円 馬単回収率:67.5%\n"
     ]
    }
   ],
   "source": [
    "wr_df = xd.reset_index()\n",
    "rt = bt5[['race_id', 'h_num']].merge(wr_df[['race_id', 'h_num']], on='race_id')\n",
    "rtt = rt[\n",
    "    rt['h_num_x'] != rt['h_num_y']\n",
    "]\n",
    "\n",
    "uma_haito = rtt.merge(haitou[['1着馬番', '2着馬番', '馬連', '馬単']], on='race_id')\n",
    "\n",
    "f_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "    len(uma_haito),\\\n",
    "    len(f_bt),\\\n",
    "    len(f_bt) / len(uma_haito)\n",
    "))\n",
    "\n",
    "print(\"馬連賭金:{:,}円 馬連配当合計:{:,}円 馬連最高配当:{:,}円 馬連回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 100),\\\n",
    "    int(f_bt['馬連'].sum()),\\\n",
    "    int(f_bt['馬連'].max()),\\\n",
    "    (f_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "))\n",
    "\n",
    "print(\"馬単賭金:{:,}円 馬単配当合計:{:,}円 馬単最高配当:{:,}円 馬単回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 200),\\\n",
    "    int(f_bt['馬単'].sum()),\\\n",
    "    int(f_bt['馬単'].max()),\\\n",
    "    (f_bt['馬単'].sum() / (len(uma_haito) * 200))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "e5b98591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1511 的中数:301 的中率:19.9%\n",
      "馬連賭金:151,100円 馬連配当合計:120,320円 馬連最高配当:3,070円 馬連回収率:79.6%\n",
      "馬単賭金:302,200円 馬単配当合計:231,970円 馬単最高配当:7,710円 馬単回収率:76.8%\n"
     ]
    }
   ],
   "source": [
    "rbt2 = wr_f.reset_index()\n",
    "rbt3 = wr_df.reset_index()\n",
    "rbt4 = pd.concat([rbt3[['race_id', 'h_num', 'odds']], rbt2[['race_id', 'h_num', 'odds']]])\n",
    "rbt5 = rbt4.drop_duplicates()\n",
    "\n",
    "# bt5 = bt5[bt5['odds'] < 3]\n",
    "rbh4 = rbt5.merge(haitou, on='race_id')\n",
    "rt = bt5[['race_id', 'h_num']].merge(rbh4[['race_id', 'h_num']], on='race_id')\n",
    "rtt = rt[\n",
    "    rt['h_num_x'] != rt['h_num_y']\n",
    "]\n",
    "\n",
    "uma_haito = rtt.merge(haitou[['1着馬番', '2着馬番', '馬連', '馬単']], on='race_id')\n",
    "\n",
    "f_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "    len(uma_haito),\\\n",
    "    len(f_bt),\\\n",
    "    len(f_bt) / len(uma_haito)\n",
    "))\n",
    "\n",
    "print(\"馬連賭金:{:,}円 馬連配当合計:{:,}円 馬連最高配当:{:,}円 馬連回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 100),\\\n",
    "    int(f_bt['馬連'].sum()),\\\n",
    "    int(f_bt['馬連'].max()),\\\n",
    "    (f_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "))\n",
    "\n",
    "print(\"馬単賭金:{:,}円 馬単配当合計:{:,}円 馬単最高配当:{:,}円 馬単回収率:{:.1%}\". format(\n",
    "    (len(uma_haito) * 200),\\\n",
    "    int(f_bt['馬単'].sum()),\\\n",
    "    int(f_bt['馬単'].max()),\\\n",
    "    (f_bt['馬単'].sum() / (len(uma_haito) * 200))\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
