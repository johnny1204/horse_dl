{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312267c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "# from google.colab import drive\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "\n",
    "    df.drop([\n",
    "        'compi',\n",
    "        'compi_num', \n",
    "        \"speed\", \n",
    "        'rank',\n",
    "#         'result',\n",
    "#         'course',\n",
    "        'born',\n",
    "        '1走前走破タイム', '2走前走破タイム', '3走前走破タイム',\n",
    "        '4走前走破タイム', '5走前走破タイム',\n",
    "        '1走前補正タイム', '2走前補正タイム', '3走前補正タイム',\n",
    "        '4走前補正タイム', '5走前補正タイム',\n",
    "        '1走前結果', '2走前結果', '3走前結果',\n",
    "        '4走前結果', '5走前結果',\n",
    "        '1走前オッズ', '2走前オッズ', '3走前オッズ',\n",
    "        '4走前オッズ', '5走前オッズ',\n",
    "        '1走前コンピ指数', '2走前コンピ指数', '3走前コンピ指数',\n",
    "        '4走前コンピ指数', '5走前コンピ指数',\n",
    "        'horse_race_id',  'body_weight','body_weight_in_de',\n",
    "            '騎手全体勝率','騎手全体連対率','騎手全体複勝率','騎手競馬場別騎乗回数','騎手競馬場別勝率','騎手競馬場別連対率','騎手競馬場別複勝率',\n",
    "    '騎手コース別騎乗回数','騎手コース別勝率','騎手コース別連対率','騎手コース別複勝率','騎手距離別騎乗回数','騎手距離別勝率','騎手距離別連対率',\n",
    "    '騎手距離別複勝率','騎手同コース同距離別騎乗回数','騎手同コース同距離別勝率','騎手同コース同距離別連対率','騎手同コース同距離別複勝率',\n",
    "    '調教師全体勝率','調教師全体連対率','調教師全体複勝率','調教師競馬場別騎乗回数','調教師競馬場別勝率','調教師競馬場別連対率',\n",
    "    '調教師競馬場別複勝率','調教師コース別騎乗回数','調教師コース別勝率','調教師コース別連対率','調教師コース別複勝率','調教師距離別騎乗回数',\n",
    "    '調教師距離別勝率','調教師距離別連対率','調教師距離別複勝率','調教師同コース同距離別騎乗回数','調教師同コース同距離別勝率',\n",
    "    '調教師年齢別年間勝率', '調教師年齢別年間連対率', '調教師年齢別年間複勝率','調教師年齢別勝率', '調教師年齢別連対率', '調教師年齢別複勝率',\n",
    "    '調教師同コース同距離別連対率','調教師同コース同距離別複勝率','種牡馬全体勝率','種牡馬全体連対率','種牡馬全体複勝率',\n",
    "    '種牡馬競馬場別出走頭数','種牡馬競馬場別勝率','種牡馬競馬場別連対率','種牡馬競馬場別複勝率','種牡馬コース別出走頭数','種牡馬コース別勝率',\n",
    "    '種牡馬コース別連対率','種牡馬コース別複勝率','種牡馬距離別出走頭数','種牡馬距離別勝率','種牡馬距離別連対率','種牡馬距離別複勝率',\n",
    "    '種牡馬同コース同距離別出走頭数','種牡馬同コース同距離別勝率','種牡馬同コース同距離別連対率','種牡馬同コース同距離別複勝率',\n",
    "    '種牡馬同周り勝率', '種牡馬同周り連対率', '種牡馬同周り複勝率',  '種牡馬同枠勝率', '種牡馬同枠連対率', '種牡馬同枠複勝率',\n",
    "    '父系統出走頭数','父系統全体勝率','父系統全体連対率','父系統全体複勝率','父系統競馬場別出走頭数','父系統競馬場別勝率','父系統競馬場別連対率','父系統競馬場別複勝率','父系統コース別出走頭数','父系統コース別勝率',\n",
    "    '父系統コース別連対率','父系統コース別複勝率','父系統距離別出走頭数','父系統距離別勝率','父系統距離別連対率','父系統距離別複勝率',\n",
    "    '父系統同コース同距離別出走頭数','父系統同コース同距離別勝率','父系統同コース同距離別連対率','父系統同コース同距離別複勝率',\n",
    "    '季節勝率', '季節連対率', '季節複勝率', '逃げ率','先行率','中団率','追込率','マクリ率','上がり3F平均',\n",
    "    '勝率','同競馬場勝率','同距離勝率','同競馬場同距離勝率','同騎手騎乗勝率','コースタイプ勝率','同距離同クラス勝率','同枠タイプ生涯勝率',\n",
    "    '連対率','同競馬場連対率','同距離連対率','同競馬場同距離連対率','同騎手騎乗連対率','コースタイプ連対率','同距離同クラス連対率','同枠タイプ生涯連対率',\n",
    "    '複勝率','同競馬場複勝率','同距離複勝率','同競馬場同距離複勝率','同騎手騎乗複勝率','コースタイプ複勝率','同距離同クラス複勝率','同枠タイプ生涯複勝率',\n",
    "    '生涯出遅れ率','騎乗騎手年間出遅れ率', '同周り勝率', '同周り連対率', '同周り複勝率',\n",
    "\n",
    "    ], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    # df2 = pd.get_dummies(df2, sparse=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds', 'time_odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds', 'time_odds']]\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds', 'win_ratio']]\n",
    "#             pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "#         std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "#         .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "        \n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # 閾値を増やす        \n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490e7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "haitou = pd.read_csv('./csv_new2/race_detail.csv')\n",
    "haitou = haitou.set_index('race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed20588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrace = pd.read_pickle('./pickle_new/base_race_20220813_5.pickle')\n",
    "allrace = pd.read_pickle('./pickle_new/base_race_20220813_2.pickle')\n",
    "time = pd.read_csv('./csv_new2/base/race_time.csv')\n",
    "allrace = allrace.merge(time, how='left', on='id')\n",
    "\n",
    "time_odds_base = pd.read_csv('./csv_new2/time_odds.csv')\n",
    "allrace = allrace.merge(time_odds_base, how='left', on='id')\n",
    "\n",
    "df = allrace.query('(course == 2 | course == 1)')\n",
    "all_r = preprocessing(df)\n",
    "# all_r = preprocessing(allrace)\n",
    "# all_r['popular'] = all_r['popular'].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "all_r.drop([\n",
    "  '気温', '風速', '風向',\n",
    "  '1走前着差', '2走前着差', '3走前着差',\n",
    "    '4走前着差', '5走前着差',\n",
    "  '1走前スピードZI','2走前スピードZI', '3走前スピードZI',\n",
    "    '4走前スピードZI', '5走前スピードZI',\n",
    "  '1走前スピード指数','2走前スピード指数', '3走前スピード指数',\n",
    "    '4走前スピード指数', '5走前スピード指数',\n",
    "    '1走前相対着順', '2走前相対着順','3走前相対着順','4走前相対着順','5走前相対着順',\n",
    "    '1走前相対人気', '2走前相対人気','3均走前相対人気','4走前相対人気','5走前相対人気',\n",
    "      '1走前スピード指数偏差','2走前スピード指数偏差', '3走前スピード指数偏差',\n",
    "    '4走前スピード指数偏差', '5走前スピード指数偏差',\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "categorical = process_categorical(all_r, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "categorical = categorical.reset_index()\n",
    "vec = pd.read_pickle('./pickle_new/peds_vec.pickle')\n",
    "categorical = categorical.merge(vec.drop(['name'], axis=1), on='horse_id')\n",
    "\n",
    "categorical = categorical.set_index('race_id')\n",
    "\n",
    "# target = pd.read_pickle('./pickle_new/new_race_20220904.pickle')\n",
    "# time_odds = pd.read_csv('./csv_new2/20220904/time_odds.csv')\n",
    "# target = target.merge(time_odds, how='left', on='id')\n",
    "# target = target[target['date'].notnull()]\n",
    "\n",
    "# target = target.query('(course == 2 | course == 1)')\n",
    "# target = preprocessing(target)\n",
    "# target['result'] = target['result'].map(lambda x: 1 if x == 1 else 0)\n",
    "# target.drop([\n",
    "#   '気温', '風速', '風向',\n",
    "#   '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI','4走前スピードZI', '5走前スピードZI',\n",
    "#   '1走前スピード指数','2走前スピード指数', '3走前スピード指数','4走前スピード指数', '5走前スピード指数',\n",
    "# #   '先行指数', 'ペース指数', '上がり指数', 'スピード指数'\n",
    "# ], axis=1, inplace=True)\n",
    "# for i in range(1, 63):\n",
    "#     target.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "# test1 = process_categorical(target,  [\n",
    "#     'producer', 'owner', 'training_course', \n",
    "#     'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "#     '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "#     'color_id', 'stallion_id', 'affiliation_id'\n",
    "# ])\n",
    "c = categorical.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "3e15611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = c.copy()\n",
    "cc['result'] = cc['result'].map(lambda x: 1 if x <= 3 else 0)\n",
    "result_d = cc.fillna(0)\n",
    "\n",
    "train1, valid1  = split_data(result_d)\n",
    "valid1, test1  = train_valid_split_data(valid1)\n",
    "\n",
    "X_train1_d  = train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_train1_d  = train1['result']\n",
    "X_valid1_d  = valid1.drop(['id', 'date', 'result',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_valid1_d  = valid1['result']\n",
    "\n",
    "X_train_d = torch.Tensor(X_train1_d.values)\n",
    "t_train_d = torch.Tensor(t_train1_d.values)\n",
    "X_valid_d = torch.Tensor(X_valid1_d.drop(['odds', 'popular', 'time_odds'], axis=1).values)\n",
    "t_valid_d = torch.Tensor(t_valid1_d.values)\n",
    "\n",
    "t_train_d = t_train_d.reshape([-1, 1])\n",
    "t_valid_d = t_valid_d.reshape([-1, 1])\n",
    "\n",
    "X_test1_d = test1.drop(['id', 'date', 'result',  'time_popular', 'correct_time', 'horse_id'], axis=1)\n",
    "t_test1_d = test1['result']\n",
    "X_test_d = torch.Tensor(X_test1_d.drop(['odds', 'popular', 'time_odds'], axis=1).values)\n",
    "t_test_d = torch.Tensor(t_test1_d.values)\n",
    "\n",
    "t_test_d = t_test_d.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a576d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train:[loss=0.143, AUC=0.756], test:[loss=0.150, AUC=0.748]\n",
      "epoch: 1, train:[loss=0.142, AUC=0.766], test:[loss=0.147, AUC=0.761]\n",
      "epoch: 2, train:[loss=0.140, AUC=0.767], test:[loss=0.145, AUC=0.763]\n",
      "epoch: 3, train:[loss=0.139, AUC=0.771], test:[loss=0.143, AUC=0.767]\n",
      "epoch: 4, train:[loss=0.142, AUC=0.772], test:[loss=0.147, AUC=0.763]\n",
      "epoch: 5, train:[loss=0.138, AUC=0.771], test:[loss=0.143, AUC=0.765]\n",
      "epoch: 6, train:[loss=0.138, AUC=0.773], test:[loss=0.144, AUC=0.763]\n",
      "epoch: 7, train:[loss=0.140, AUC=0.775], test:[loss=0.144, AUC=0.771]\n",
      "epoch: 8, train:[loss=0.139, AUC=0.774], test:[loss=0.144, AUC=0.769]\n",
      "epoch: 9, train:[loss=0.143, AUC=0.767], test:[loss=0.151, AUC=0.758]\n",
      "epoch: 10, train:[loss=0.139, AUC=0.771], test:[loss=0.145, AUC=0.761]\n",
      "epoch: 11, train:[loss=0.139, AUC=0.771], test:[loss=0.143, AUC=0.765]\n",
      "epoch: 12, train:[loss=0.139, AUC=0.777], test:[loss=0.143, AUC=0.772]\n",
      "epoch: 13, train:[loss=0.138, AUC=0.772], test:[loss=0.143, AUC=0.764]\n",
      "epoch: 14, train:[loss=0.141, AUC=0.778], test:[loss=0.145, AUC=0.773]\n",
      "epoch: 15, train:[loss=0.138, AUC=0.777], test:[loss=0.142, AUC=0.771]\n",
      "epoch: 16, train:[loss=0.138, AUC=0.775], test:[loss=0.142, AUC=0.769]\n",
      "epoch: 17, train:[loss=0.138, AUC=0.776], test:[loss=0.143, AUC=0.771]\n",
      "epoch: 18, train:[loss=0.139, AUC=0.776], test:[loss=0.143, AUC=0.771]\n",
      "epoch: 19, train:[loss=0.138, AUC=0.775], test:[loss=0.143, AUC=0.768]\n",
      "epoch: 20, train:[loss=0.138, AUC=0.779], test:[loss=0.142, AUC=0.774]\n",
      "epoch: 21, train:[loss=0.137, AUC=0.778], test:[loss=0.141, AUC=0.773]\n",
      "epoch: 22, train:[loss=0.139, AUC=0.777], test:[loss=0.144, AUC=0.771]\n",
      "epoch: 23, train:[loss=0.139, AUC=0.775], test:[loss=0.143, AUC=0.770]\n",
      "epoch: 24, train:[loss=0.138, AUC=0.775], test:[loss=0.144, AUC=0.764]\n",
      "epoch: 25, train:[loss=0.138, AUC=0.777], test:[loss=0.142, AUC=0.772]\n",
      "epoch: 26, train:[loss=0.138, AUC=0.778], test:[loss=0.142, AUC=0.771]\n",
      "epoch: 27, train:[loss=0.138, AUC=0.775], test:[loss=0.142, AUC=0.770]\n",
      "epoch: 28, train:[loss=0.137, AUC=0.779], test:[loss=0.141, AUC=0.774]\n",
      "epoch: 29, train:[loss=0.137, AUC=0.777], test:[loss=0.141, AUC=0.771]\n",
      "epoch: 30, train:[loss=0.139, AUC=0.776], test:[loss=0.143, AUC=0.770]\n",
      "epoch: 31, train:[loss=0.137, AUC=0.777], test:[loss=0.142, AUC=0.770]\n",
      "epoch: 32, train:[loss=0.138, AUC=0.777], test:[loss=0.142, AUC=0.773]\n",
      "epoch: 33, train:[loss=0.139, AUC=0.774], test:[loss=0.144, AUC=0.768]\n",
      "epoch: 34, train:[loss=0.137, AUC=0.778], test:[loss=0.142, AUC=0.772]\n",
      "epoch: 35, train:[loss=0.138, AUC=0.778], test:[loss=0.142, AUC=0.773]\n",
      "epoch: 36, train:[loss=0.137, AUC=0.777], test:[loss=0.142, AUC=0.769]\n",
      "epoch: 37, train:[loss=0.137, AUC=0.779], test:[loss=0.141, AUC=0.773]\n",
      "epoch: 38, train:[loss=0.138, AUC=0.778], test:[loss=0.142, AUC=0.772]\n",
      "epoch: 39, train:[loss=0.138, AUC=0.776], test:[loss=0.143, AUC=0.768]\n",
      "epoch: 40, train:[loss=0.137, AUC=0.777], test:[loss=0.142, AUC=0.771]\n",
      "epoch: 41, train:[loss=0.137, AUC=0.777], test:[loss=0.142, AUC=0.771]\n",
      "epoch: 42, train:[loss=0.137, AUC=0.779], test:[loss=0.142, AUC=0.772]\n",
      "epoch: 43, train:[loss=0.138, AUC=0.779], test:[loss=0.142, AUC=0.773]\n",
      "epoch: 44, train:[loss=0.139, AUC=0.779], test:[loss=0.143, AUC=0.774]\n",
      "epoch: 45, train:[loss=0.137, AUC=0.778], test:[loss=0.141, AUC=0.772]\n",
      "epoch: 46, train:[loss=0.137, AUC=0.778], test:[loss=0.142, AUC=0.773]\n",
      "epoch: 47, train:[loss=0.138, AUC=0.779], test:[loss=0.142, AUC=0.774]\n",
      "epoch: 48, train:[loss=0.137, AUC=0.778], test:[loss=0.142, AUC=0.772]\n",
      "epoch: 49, train:[loss=0.138, AUC=0.776], test:[loss=0.142, AUC=0.770]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_train_d, t_train_d)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(575, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "\n",
    "    for X, t in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(X)\n",
    "        loss = loss_fn(y, t)\n",
    "        # 傾きの計算\n",
    "        loss.backward()\n",
    "        # optimizerの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_train_d = model(X_train_d)\n",
    "    y_valid_d = model(X_valid_d)\n",
    "   #  平均二乗誤差 予測値と正解値の誤差の計算\n",
    "    loss_train = loss_fn(y_train_d, t_train_d)\n",
    "    loss_valid = loss_fn(y_valid_d, t_valid_d)\n",
    "    auc_train = roc_auc_score(t_train_d.detach().numpy(), y_train_d.detach().numpy())\n",
    "    auc_valid = roc_auc_score(t_valid_d.detach().numpy(), y_valid_d.detach().numpy())\n",
    "    \n",
    "    print('epoch: {}, train:[loss={:.3f}, AUC={:.3f}], test:[loss={:.3f}, AUC={:.3f}]'.  format(epoch, loss_train, auc_train, loss_valid, auc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64f9de1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7682880557954992"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_d = model(X_test_d)\n",
    "roc_auc_score(t_test_d.detach().numpy(), y_test_d.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "22429999",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = pd.read_csv('./csv_new2/races.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "8192a0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_36569/2652617224.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = t_pred_d\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_36569/2652617224.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['proba'] = proba\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_36569/2652617224.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x['pred'] = x['proba'].map(lambda x: 0 if x <= 0.5 else 1)\n"
     ]
    }
   ],
   "source": [
    "x = valid1[['h_num', 'odds', 'time_odds', 'popular']]\n",
    "t_pred_d = pd.Series(np.around(torch.flatten(y_valid_d).detach().numpy(), decimals=5), index=x.index)\n",
    "sum1 = pd.DataFrame(t_pred_d.groupby(level=0).sum())\n",
    "\n",
    "x['proba'] = t_pred_d\n",
    "proba = x[['proba']]\n",
    "standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "\n",
    "x['proba'] = proba\n",
    "x['pred'] = x['proba'].map(lambda x: 0 if x <= 0.5 else 1)\n",
    "\n",
    "shisuu = pd.read_csv('./shisuu_new.csv')\n",
    "v = valid1.reset_index()[['race_id', 'h_num', 'id']]\n",
    "v['horse_race_id'] = v['id']\n",
    "\n",
    "sv = v.drop(['id'], axis=1).merge(shisuu, on='horse_race_id')\n",
    "sv = sv.merge(x, on=['race_id', 'h_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "5bd4d46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>pred</th>\n",
       "      <th>proba</th>\n",
       "      <th>popular</th>\n",
       "      <th>score</th>\n",
       "      <th>s_proba</th>\n",
       "      <th>shisuu</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018041509020807</td>\n",
       "      <td>12</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547701</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018041509020807</td>\n",
       "      <td>5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018041506030805</td>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.589088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2018041503010407</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624692</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2018041503010407</td>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137462</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137476</th>\n",
       "      <td>2021042405020109</td>\n",
       "      <td>2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.710751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137488</th>\n",
       "      <td>2021042405020109</td>\n",
       "      <td>7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.811091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>81</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137503</th>\n",
       "      <td>2021042409020907</td>\n",
       "      <td>2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547634</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137510</th>\n",
       "      <td>2021042405020108</td>\n",
       "      <td>3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.531623</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12743 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 race_id  h_num  odds  time_odds  pred     proba  popular  \\\n",
       "33      2018041509020807     12   4.3        4.5     1  0.547701      2.0   \n",
       "38      2018041509020807      5   7.3        7.4     1  0.547039      4.0   \n",
       "50      2018041506030805     10   4.5        5.0     1  0.589088      2.0   \n",
       "77      2018041503010407      4   6.1        7.0     1  0.624692      3.0   \n",
       "78      2018041503010407      1   3.4        4.2     1  0.621195      1.0   \n",
       "...                  ...    ...   ...        ...   ...       ...      ...   \n",
       "137462  2021042405020111      2   3.1        2.9     1  0.816141      1.0   \n",
       "137476  2021042405020109      2   3.6        4.9     1  0.710751      2.0   \n",
       "137488  2021042405020109      7   2.1        2.5     1  0.811091      1.0   \n",
       "137503  2021042409020907      2   2.7        3.3     1  0.547634      2.0   \n",
       "137510  2021042405020108      3   3.9        4.2     1  0.531623      2.0   \n",
       "\n",
       "        score  s_proba  shisuu  grade  \n",
       "33          8       54      31      3  \n",
       "38         11       54      32      3  \n",
       "50          2       58      30      3  \n",
       "77          3       62      32      3  \n",
       "78          3       62      32      3  \n",
       "...       ...      ...     ...    ...  \n",
       "137462     17       81      54      7  \n",
       "137476      5       71      39      4  \n",
       "137488     11       81      49      4  \n",
       "137503      2       54      28      3  \n",
       "137510      7       53      30      3  \n",
       "\n",
       "[12743 rows x 11 columns]"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = sv[['race_id', 'h_num', 'odds', 'time_odds', 'pred', 'proba', 'popular','score']]\n",
    "x1['s_proba'] = (x1['proba'] * 100).astype(int)\n",
    "x1['shisuu'] = (((50 + x1['score']) / 100)  * x1['s_proba']).astype(int)\n",
    "x1 = x1.merge(grade, on='race_id')\n",
    "\n",
    "bt1 = x1[\n",
    "    (x1['pred'] == 1)\n",
    "    &\n",
    "#     ((x1['pred_rank'] == 1))\n",
    "#     &\n",
    "#      (x1['time_odds'] / x1['d_odds'] >= 1)\n",
    "#     &\n",
    "#     (x1['grade'] > 2)\n",
    "#     &\n",
    "    (x1['score'] > 0)\n",
    "]\n",
    "bt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "5e3bfe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：12743 レース数:10060 対象レース数:7692 出現頻度:76.5% 的中率:51.3% 的中数:6542 賭金:1,274,300円 配当合計:1,074,130円 回収率:84.3%\n"
     ]
    }
   ],
   "source": [
    "bh = bt1.merge(haitou, on='race_id')\n",
    "\n",
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(bh[bh['h_num'] == bh[s + '着馬番']]['複勝' + s])\n",
    "    money += bh[bh['h_num'] == bh[s + '着馬番']]['複勝' + s].sum()\n",
    "    \n",
    "print(\"点数：{} レース数:{} 対象レース数:{} 出現頻度:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(bt1),\\\n",
    "    len(valid1.groupby('race_id')),\\\n",
    "    len(bt1.groupby('race_id')),\\\n",
    "    len(bt1.groupby('race_id')) / len(valid1.groupby('race_id')),\\\n",
    "    f_c / len(bt1),\\\n",
    "    f_c,\\\n",
    "    len(bt1) * 100,\\\n",
    "    int(money),\\\n",
    "    (money / (len(bt1) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "46ce9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_umaren = bt1.merge(x1, on='race_id')\n",
    "b_umaren = b_umaren[\n",
    "    b_umaren['h_num_x'] != b_umaren['h_num_y']\n",
    "]\n",
    "\n",
    "umaren = b_umaren[\n",
    "#     (b_umaren['score_y'] >= 0)\n",
    "#     &\n",
    "    ((b_umaren['shisuu_x'] + b_umaren['shisuu_y']) > 75)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "7372a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：3077 レース数:1345 出現頻度:13.4% 的中率:12.1% 的中数:371 賭金:307,700円 配当合計:296,400円 最高配当:8,560円 回収率:96.3%\n"
     ]
    }
   ],
   "source": [
    "umaren_bets = umaren.merge(haitou, on='race_id')[['race_id', 'h_num_x', 'h_num_y', '1着馬番', '2着馬番', '馬連', '馬単']]\n",
    "tekichu = umaren_bets[\n",
    "    (\n",
    "        (umaren_bets['h_num_x'] == umaren_bets['1着馬番'])\n",
    "        &\n",
    "        (umaren_bets['h_num_y'] == umaren_bets['2着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (umaren_bets['h_num_x'] == umaren_bets['2着馬番'])\n",
    "        &\n",
    "        (umaren_bets['h_num_y'] == umaren_bets['1着馬番'])\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"点数：{} レース数:{} 出現頻度:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(umaren),\\\n",
    "    len(umaren.groupby('race_id')),\\\n",
    "    len(umaren.groupby('race_id')) / len(valid1.groupby('race_id')),\\\n",
    "    len(tekichu) / len(umaren),\\\n",
    "    len(tekichu),\\\n",
    "    len(umaren) * 100,\\\n",
    "    int(tekichu['馬連'].sum()),\\\n",
    "    int(tekichu['馬連'].max()),\\\n",
    "    (int(tekichu['馬連'].sum()) / (len(umaren) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "8baaea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：3077 レース数:1345 出現頻度:13.4% 的中率:12.1% 的中数:371 賭金:615,400円 配当合計:574,660円 最高配当:17,070円 回収率:93.4%\n"
     ]
    }
   ],
   "source": [
    "umatan = b_umaren[\n",
    "    ((b_umaren['shisuu_x'] + b_umaren['shisuu_y']) > 75)\n",
    "]\n",
    "umatan_bets = umatan.merge(haitou, on='race_id')[['race_id', 'h_num_x', 'h_num_y', '1着馬番', '2着馬番', '馬連', '馬単']]\n",
    "\n",
    "umatan_tekichu = umatan_bets[\n",
    "    (\n",
    "        (umatan_bets['h_num_x'] == umatan_bets['1着馬番'])\n",
    "        &\n",
    "        (umatan_bets['h_num_y'] == umatan_bets['2着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (umatan_bets['h_num_x'] == umatan_bets['2着馬番'])\n",
    "        &\n",
    "        (umatan_bets['h_num_y'] == umatan_bets['1着馬番'])\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"点数：{} レース数:{} 出現頻度:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(umatan),\\\n",
    "    len(umatan.groupby('race_id')),\\\n",
    "    len(umatan.groupby('race_id')) / len(valid1.groupby('race_id')),\\\n",
    "    len(umatan_tekichu) / len(umaren),\\\n",
    "    len(umatan_tekichu),\\\n",
    "    len(umatan) * 200,\\\n",
    "    int(umatan_tekichu['馬単'].sum()),\\\n",
    "    int(umatan_tekichu['馬単'].max()),\\\n",
    "    (int(umatan_tekichu['馬単'].sum()) / (len(umatan) * 200))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "807e0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sanren = b_umaren.merge(x1, on='race_id')\n",
    "b_sanren = b_sanren[\n",
    "    (b_sanren['h_num_x'] != b_sanren['h_num'])\n",
    "    &\n",
    "    (b_sanren['h_num_y'] != b_sanren['h_num'])\n",
    "]\n",
    "\n",
    "t_sanren = b_sanren[\n",
    "    (b_sanren['score'] > 0)\n",
    "    &\n",
    "    ((b_sanren['shisuu_x'] + b_sanren['shisuu_y'] + b_sanren['shisuu']) >= 95)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "0d90424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：20578 レース数:1808 出現頻度:18.0% 的中率:2.6% 的中数:529 賭金:2,057,800円 配当合計:1,534,250円 最高配当:35,650円 回収率:74.6%\n"
     ]
    }
   ],
   "source": [
    "sanren = t_sanren.merge(haitou, on='race_id')[['race_id', 'h_num_x', 'h_num_y', 'h_num', '1着馬番', '2着馬番', '3着馬番', '3連複', '3連単']]\n",
    "san_tekichu = sanren[\n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['1着馬番']) & (sanren['h_num_y'] == sanren['2着馬番']) & (sanren['h_num'] == sanren['3着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['1着馬番']) & (sanren['h_num_y'] == sanren['3着馬番']) & (sanren['h_num'] == sanren['2着馬番'])\n",
    "    )\n",
    "    |\n",
    "    \n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['2着馬番']) & (sanren['h_num_y'] == sanren['1着馬番']) & (sanren['h_num'] == sanren['3着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['2着馬番']) & (sanren['h_num_y'] == sanren['3着馬番']) & (sanren['h_num'] == sanren['1着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['3着馬番']) & (sanren['h_num_y'] == sanren['2着馬番']) & (sanren['h_num'] == sanren['1着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (sanren['h_num_x'] == sanren['3着馬番']) & (sanren['h_num_y'] == sanren['1着馬番']) & (sanren['h_num'] == sanren['2着馬番'])\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"点数：{} レース数:{} 出現頻度:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(sanren),\\\n",
    "    len(sanren.groupby('race_id')),\\\n",
    "    len(sanren.groupby('race_id')) / len(valid1.groupby('race_id')),\\\n",
    "    len(san_tekichu) / len(sanren),\\\n",
    "    len(san_tekichu),\\\n",
    "    len(sanren) * 100,\\\n",
    "    int(san_tekichu['3連複'].sum()),\\\n",
    "    int(san_tekichu['3連複'].max()),\\\n",
    "    (int(san_tekichu['3連複'].sum()) / (len(sanren) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "0d6e2c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：20578 レース数:1808 出現頻度:18.0% 的中率:2.6% 的中数:529 賭金:12,346,800円 配当合計:8,418,330円 最高配当:180,340円 回収率:68.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"点数：{} レース数:{} 出現頻度:{:.1%} 的中率:{:.1%} 的中数:{} 賭金:{:,}円 配当合計:{:,}円 最高配当:{:,}円 回収率:{:.1%}\". format(\n",
    "    len(sanren),\\\n",
    "    len(sanren.groupby('race_id')),\\\n",
    "    len(sanren.groupby('race_id')) / len(valid1.groupby('race_id')),\\\n",
    "    len(san_tekichu) / len(sanren),\\\n",
    "    len(san_tekichu),\\\n",
    "    len(sanren) * 600,\\\n",
    "    int(san_tekichu['3連単'].sum()),\\\n",
    "    int(san_tekichu['3連単'].max()),\\\n",
    "    (int(san_tekichu['3連単'].sum()) / (len(sanren) * 600))\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
