{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c395f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "import lightgbm as lgb\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def build_data(base_name, new_name, result, vec):\n",
    "    allrace = pd.read_pickle(base_name)\n",
    "\n",
    "    df = allrace.query('((course == 2 | course == 1) and (grade > 1))')\n",
    "    all_r = preprocessing(df)\n",
    "    all_r['result'] = all_r['result'].map(lambda x: 1 if x < result else 0)\n",
    "    \n",
    "    all_r = all_r.fillna({\n",
    "    '1走前騎手ID': 0,'1走前減量騎手か': 0,'1走前ブリンカー着用': 0,'1走前出遅れ': 0,'1走前脚質': 0,'1走前不利': 0, '1走前距離': 0, '1走前場所': 0,'1走前コース': 0,'1走前出走クラス': 0,'1走前1着脚質': 0,\n",
    "    '2走前騎手ID': 0,'2走前減量騎手か': 0,'2走前ブリンカー着用': 0,'2走前出遅れ': 0,'2走前脚質': 0,'2走前不利': 0, '2走前距離': 0, '2走前場所': 0,'2走前コース': 0,'2走前出走クラス': 0,'2走前1着脚質': 0,\n",
    "    '3走前騎手ID': 0,'3走前減量騎手か': 0,'3走前ブリンカー着用': 0,'3走前出遅れ': 0,'3走前脚質': 0,'3走前不利': 0, '3走前距離': 0, '3走前場所': 0,'3走前コース': 0,'3走前出走クラス': 0,'3走前1着脚質': 0,\n",
    "    '4走前騎手ID': 0,'4走前減量騎手か': 0,'4走前ブリンカー着用': 0,'4走前出遅れ': 0,'4走前脚質': 0,'4走前不利': 0, '4走前距離': 0, '4走前場所': 0,'4走前コース': 0,'4走前出走クラス': 0,'4走前1着脚質': 0,\n",
    "    '5走前騎手ID': 0,'5走前減量騎手か': 0,'5走前ブリンカー着用': 0,'5走前出遅れ': 0,'5走前脚質': 0,'5走前不利': 0, '5走前距離': 0, '5走前場所': 0,'5走前コース': 0,'5走前出走クラス': 0,'5走前1着脚質': 0,\n",
    "    })\n",
    "    categorical = process_categorical(all_r, [\n",
    "        'producer', 'owner', 'training_course',  'course',\n",
    "        'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "        '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "        'color_id', 'stallion_id', 'affiliation_id',\n",
    "        '1走前騎手ID','1走前減量騎手か','1走前ブリンカー着用','1走前出遅れ','1走前脚質','1走前不利','1走前場所', '1走前距離', '1走前コース','1走前出走クラス','1走前1着脚質',\n",
    "        '2走前騎手ID','2走前減量騎手か','2走前ブリンカー着用','2走前出遅れ','2走前脚質','2走前不利','2走前場所', '2走前距離', '2走前コース','2走前出走クラス','2走前1着脚質',\n",
    "        '3走前騎手ID','3走前減量騎手か','3走前ブリンカー着用','3走前出遅れ','3走前脚質','3走前不利','3走前場所', '3走前距離', '3走前コース','3走前出走クラス','3走前1着脚質',\n",
    "        '4走前騎手ID','4走前減量騎手か','4走前ブリンカー着用','4走前出遅れ','4走前脚質','4走前不利','4走前場所', '4走前距離', '4走前コース','4走前出走クラス','4走前1着脚質',\n",
    "        '5走前騎手ID','5走前減量騎手か','5走前ブリンカー着用','5走前出遅れ','5走前脚質','5走前不利','5走前場所', '5走前距離', '5走前コース','5走前出走クラス','5走前1着脚質',\n",
    "    ])\n",
    "\n",
    "    categorical = categorical.reset_index()\n",
    "    categorical = categorical.merge(vec, on='horse_id')\n",
    "    categorical = categorical.set_index('race_id')\n",
    "\n",
    "    train1, valid1 = split_data(categorical)\n",
    "    valid1, test2 = train_valid_split_data(valid1)\n",
    "\n",
    "    target = pd.read_pickle('./pickle_new/new_race_20221127.pickle')\n",
    "    target = target.query('((course == 2 | course == 1) and (grade > 1))')\n",
    "    target = preprocessing(target)\n",
    "    target['result'] = target['result'].map(lambda x: 1 if x < result else 0)\n",
    "    \n",
    "    target = target.fillna({\n",
    "    '1走前騎手ID': 0,'1走前減量騎手か': 0,'1走前ブリンカー着用': 0,'1走前出遅れ': 0,'1走前脚質': 0,'1走前不利': 0, '1走前距離': 0, '1走前場所': 0,'1走前コース': 0,'1走前出走クラス': 0,'1走前1着脚質': 0,\n",
    "    '2走前騎手ID': 0,'2走前減量騎手か': 0,'2走前ブリンカー着用': 0,'2走前出遅れ': 0,'2走前脚質': 0,'2走前不利': 0, '2走前距離': 0, '2走前場所': 0,'2走前コース': 0,'2走前出走クラス': 0,'2走前1着脚質': 0,\n",
    "    '3走前騎手ID': 0,'3走前減量騎手か': 0,'3走前ブリンカー着用': 0,'3走前出遅れ': 0,'3走前脚質': 0,'3走前不利': 0, '3走前距離': 0, '3走前場所': 0,'3走前コース': 0,'3走前出走クラス': 0,'3走前1着脚質': 0,\n",
    "    '4走前騎手ID': 0,'4走前減量騎手か': 0,'4走前ブリンカー着用': 0,'4走前出遅れ': 0,'4走前脚質': 0,'4走前不利': 0, '4走前距離': 0, '4走前場所': 0,'4走前コース': 0,'4走前出走クラス': 0,'4走前1着脚質': 0,\n",
    "    '5走前騎手ID': 0,'5走前減量騎手か': 0,'5走前ブリンカー着用': 0,'5走前出遅れ': 0,'5走前脚質': 0,'5走前不利': 0, '5走前距離': 0, '5走前場所': 0,'5走前コース': 0,'5走前出走クラス': 0,'5走前1着脚質': 0,\n",
    "    })\n",
    "    test1 = process_categorical(target,  [\n",
    "        'producer', 'owner', 'training_course',  'course',\n",
    "        'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "        '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "        'color_id', 'stallion_id', 'affiliation_id', 'distance',\n",
    "        '1走前騎手ID','1走前減量騎手か','1走前ブリンカー着用','1走前出遅れ','1走前脚質','1走前不利','1走前場所', '1走前距離', '1走前コース','1走前出走クラス','1走前1着脚質',\n",
    "        '2走前騎手ID','2走前減量騎手か','2走前ブリンカー着用','2走前出遅れ','2走前脚質','2走前不利','2走前場所', '2走前距離', '2走前コース','2走前出走クラス','2走前1着脚質',\n",
    "        '3走前騎手ID','3走前減量騎手か','3走前ブリンカー着用','3走前出遅れ','3走前脚質','3走前不利','3走前場所', '3走前距離', '3走前コース','3走前出走クラス','3走前1着脚質',\n",
    "        '4走前騎手ID','4走前減量騎手か','4走前ブリンカー着用','4走前出遅れ','4走前脚質','4走前不利','4走前場所', '4走前距離', '4走前コース','4走前出走クラス','4走前1着脚質',\n",
    "        '5走前騎手ID','5走前減量騎手か','5走前ブリンカー着用','5走前出遅れ','5走前脚質','5走前不利','5走前場所', '5走前距離', '5走前コース','5走前出走クラス','5走前1着脚質',\n",
    "    ])\n",
    "\n",
    "    test1 = test1.reset_index()\n",
    "    test1 = test1.merge(vec, on='horse_id')\n",
    "    test1 = test1.set_index('race_id')\n",
    "\n",
    "    X_train1 = train1.drop(['id', 'index', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_train1 = train1['result']\n",
    "    X_valid1 = valid1.drop(['date', 'index', 'result', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_valid1 = valid1['result']\n",
    "    X_test2 = test2.drop(['date', 'index', 'result', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_test2 = test2['result']\n",
    "    X_test1 = test1.drop(['date', 'result', 'popular', 'popular',  'time_popular', 'horse_id', 'owner', 'producer', '気温', '風速'], axis=1)\n",
    "    y_test1 = test1['result']\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train1,\n",
    "        'y_train': y_train1,\n",
    "        'X_valid': X_valid1,\n",
    "        'y_valid': y_valid1,\n",
    "        'X_test2': X_test2,\n",
    "        'y_test2': y_test2,\n",
    "        'X_test1': X_test1,\n",
    "        'y_test1': y_test1,\n",
    "    }\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "    df.drop(['rank', 'body_weight'], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    # df2 = pd.get_dummies(df2, sparse=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "def optuna_params(X_train, y_train, X_valid, y_valid):\n",
    "    lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n",
    "    lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'random_state': 100\n",
    "    }\n",
    "\n",
    "    lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)\n",
    "    \n",
    "    return lgb_clf_o.params\n",
    "\n",
    "def fit(params, X, y):\n",
    "    lgb_clf = lgb.LGBMClassifier(**params)\n",
    "    lgb_clf.fit(X.values, y.values)\n",
    "    \n",
    "    return lgb_clf\n",
    "\n",
    "def tansho_return_rate(X, wr, expected=True):\n",
    "    wr['expected'] = wr['win_ratio'] * wr['time_odds'] \n",
    "    wr['pred_rank'] = wr[['win_ratio']].groupby('race_id').rank(ascending=False)\n",
    "    \n",
    "    race_grade = pd.read_csv('./csv_new2/races.csv')\n",
    "    race_grade = race_grade.set_index('race_id')\n",
    "    race_grade[['grade']]\n",
    "\n",
    "    bets = wr.merge(race_grade, on='race_id')\n",
    "\n",
    "    weather = pd.read_csv('./csv_new2/weathers.csv')\n",
    "    bets = bets.merge(weather[['race_id', 'place_id']], on='race_id')\n",
    "\n",
    "    if expected:\n",
    "        # 賭ける馬\n",
    "        v_bt = bets[\n",
    "        #     (bets['pred_rank'] == 1)\n",
    "        #     &\n",
    "            (bets['expected'] >= 1)\n",
    "        ]\n",
    "    else:\n",
    "        v_bt = bets\n",
    "\n",
    "    kaime = v_bt.merge(haitou[['1着馬番', '単勝']], on='race_id')\n",
    "    \n",
    "    print(\"点数：{} レース数:{} 対象レース率:{:.1%} 単勝的中率:{:.1%} 的中数:{} 賭金:{:,}円 単勝配当合計:{:,}円 単勝最高配当:{:,}円 単勝回収率:{:.1%}\". format(\n",
    "        len(kaime),\\\n",
    "        len(X.groupby('race_id')),\\\n",
    "        len(kaime.groupby('race_id')) / len(X.groupby('race_id')),\\\n",
    "        len(kaime[kaime['h_num'] == kaime['1着馬番']]) / (len(kaime)),\\\n",
    "        len(kaime[kaime['h_num'] == kaime['1着馬番']]),\\\n",
    "        len(kaime) * 100,\\\n",
    "        kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum(),\\\n",
    "        kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].max(),\\\n",
    "        (kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum() / (len(kaime) * 100))\n",
    "    ))\n",
    "    \n",
    "    return kaime\n",
    "\n",
    "def fukusho_return_rate(bt):\n",
    "    h = bt.merge(haitou, on='race_id')\n",
    "\n",
    "    money = 0\n",
    "    f_c = 0\n",
    "    for i in range(1, 5):\n",
    "        s = str(i)\n",
    "        f_c += len(h[h['h_num'] == h[s + '着馬番']]['複勝' + s])\n",
    "        money += h[h['h_num'] == h[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "    print(\"点数：{} 賭金:{:,}円 複勝的中率:{:.1%} 複勝的中数:{} 複勝配当合計:{:,}円 複勝回収率:{:.1%}\". format(\n",
    "        len(bt),\\\n",
    "        (len(bt) * 100),\\\n",
    "        f_c / len(bt),\\\n",
    "        f_c,\\\n",
    "        int(money),\n",
    "        (money / (len(bt) * 100)),\\\n",
    "    ))\n",
    "    \n",
    "def umaren_return_rate(fmrt, haitou):\n",
    "    uma_haito = fmrt.merge(haitou[['1着馬番', '2着馬番', '馬連', '馬単']], on='race_id')\n",
    "\n",
    "    f_bt = uma_haito[\n",
    "        (\n",
    "            (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "            &\n",
    "            (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "        )\n",
    "        |\n",
    "        (\n",
    "            (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "            &\n",
    "            (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "        len(uma_haito),\\\n",
    "        len(f_bt),\\\n",
    "        len(f_bt) / len(uma_haito)\n",
    "    ))\n",
    "    \n",
    "    print(\"馬連賭金:{:,}円 馬連配当合計:{:,}円 馬連最高配当:{:,}円 馬連回収率:{:.1%}\". format(\n",
    "        (len(uma_haito) * 100),\\\n",
    "        f_bt['馬連'].sum(),\\\n",
    "        f_bt['馬連'].max(),\\\n",
    "        (f_bt['馬連'].sum() / (len(uma_haito) * 100))\n",
    "    ))\n",
    "    \n",
    "    print(\"馬単賭金:{:,}円 馬単配当合計:{:,}円 馬単最高配当:{:,}円 馬単回収率:{:.1%}\". format(\n",
    "        (len(uma_haito) * 200),\\\n",
    "        f_bt['馬単'].sum(),\\\n",
    "        f_bt['馬単'].max(),\\\n",
    "        (f_bt['馬単'].sum() / (len(uma_haito) * 200))\n",
    "    ))\n",
    "    \n",
    "    return uma_haito\n",
    "    \n",
    "def sanrenkei(kaime, haitou):\n",
    "    kaime = kaime.merge(haitou, on='race_id')\n",
    "    f_b3 = kaime[\n",
    "        ((kaime['h_num_x'] == kaime['1着馬番']) & (kaime['h_num_y'] == kaime['2着馬番']) & (kaime['h_num'] == kaime['3着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['1着馬番']) & (kaime['h_num_y'] == kaime['3着馬番']) & (kaime['h_num'] == kaime['2着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['2着馬番']) & (kaime['h_num_y'] == kaime['1着馬番']) & (kaime['h_num'] == kaime['3着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['2着馬番']) & (kaime['h_num_y'] == kaime['3着馬番']) & (kaime['h_num'] == kaime['1着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['3着馬番']) & (kaime['h_num_y'] == kaime['1着馬番']) & (kaime['h_num'] == kaime['2着馬番']))\n",
    "        |\n",
    "        ((kaime['h_num_x'] == kaime['3着馬番']) & (kaime['h_num_y'] == kaime['2着馬番']) & (kaime['h_num'] == kaime['1着馬番']))\n",
    "    ]\n",
    "    \n",
    "    print(\"点数：{} 的中数:{} 的中率:{:.1%}\". format(\n",
    "        len(kaime),\\\n",
    "        len(f_b3),\\\n",
    "        len(f_b3) / len(kaime)\n",
    "    ))\n",
    "\n",
    "    print(\"3連複賭金:{:,}円 3連複配当合計:{:,}円 3連複最高配当:{:,}円 3連複回収率:{:.1%}\". format(\n",
    "        len(kaime) * 100,\\\n",
    "        f_b3['3連複'].sum(),\\\n",
    "        f_b3['3連複'].max(),\\\n",
    "         (f_b3['3連複'].sum() / (len(kaime) * 100))\n",
    "    ))\n",
    "    \n",
    "    print(\"3連単賭金:{:,}円 3連単配当合計:{:,}円 3連単最高配当:{:,}円 3連単回収率:{:.1%}\". format(\n",
    "        len(kaime) * 600,\\\n",
    "        int(f_b3['3連単'].sum()),\\\n",
    "        int(f_b3['3連単'].max()),\\\n",
    "        (f_b3['3連単'].sum() / (len(kaime) * 600))\n",
    "    ))\n",
    "    \n",
    "def wide_rate(kaime, haitou):\n",
    "    s = kaime.reset_index()\n",
    "    s = s.merge(haitou, on='race_id')\n",
    "    s = s[['race_id', 'h_num_x', 'h_num_y', 'h_num']]\n",
    "\n",
    "    a = s[['race_id', 'h_num_x', 'h_num_y']]\n",
    "\n",
    "    b = s[['race_id', 'h_num_y', 'h_num']]\n",
    "    b['h_num_x'] = b['h_num_y']\n",
    "    b['h_num_y'] = b['h_num']\n",
    "\n",
    "    c = s[['race_id', 'h_num_x', 'h_num']]\n",
    "    c['h_num_y'] = c['h_num']\n",
    "\n",
    "    w = pd.concat([\n",
    "        a[['race_id', 'h_num_x', 'h_num_y']],\n",
    "        b[['race_id', 'h_num_x', 'h_num_y']],\n",
    "        c[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    ]).drop_duplicates()\n",
    "    wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "    w = w.merge(wide, on='race_id')\n",
    "    \n",
    "    wide = 0\n",
    "    wide_tekichu = 0\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        sm = str(i)\n",
    "        w_b3 = w[\n",
    "            (\n",
    "                ((w['h_num_x'] == w['wide' + sm + '_uma1']) | (w['h_num_x'] == w['wide' + sm + '_uma2']))\n",
    "                &\n",
    "                ((w['h_num_y'] == w['wide' + sm + '_uma1']) | (w['h_num_y'] == w['wide' + sm + '_uma2']))\n",
    "            )\n",
    "        ]\n",
    "        wide = wide + w_b3['wide_' + sm].sum()\n",
    "        wide_tekichu = wide_tekichu + len(w_b3)\n",
    "\n",
    "    print(\"ワイド点数：{} ワイド賭金:{:,}円 ワイド配当合計:{:,}円 ワイド的中数:{} ワイド的中率:{:.1%} ワイド回収率:{:.1%}\". format(\n",
    "        len(w),\\\n",
    "        len(w) * 100,\\\n",
    "        int(wide),\\\n",
    "        wide_tekichu,\\\n",
    "        wide_tekichu / len(w),\\\n",
    "        (wide / (len(w) * 100))\n",
    "    ))\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds', 'time_odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def proba(self, X):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [p for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds', 'time_odds']]\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        pred_table['proba'] = self.proba(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds', 'win_ratio', 'proba']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table[['h_num', 'odds', 'time_odds', 'win_ratio', 'proba']]\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "        std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "        .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "527cde32",
   "metadata": {},
   "outputs": [],
   "source": [
    "haitou = pd.read_csv('./csv_new2/race_detail.csv')\n",
    "wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "\n",
    "haitou = haitou.merge(wide, on='race_id')\n",
    "haitou = haitou.set_index('race_id')\n",
    "\n",
    "shisuu = pd.read_csv('./shisuu_new.csv')\n",
    "\n",
    "vec = pd.read_pickle('./pickle_new/peds_vec.pickle')\n",
    "# vec = vec[[\n",
    "#     'horse_id', \"peds_2\",\"peds_3\",\"peds_4\",\"peds_5\",\n",
    "#     \"peds_6\",\"peds_7\",\"peds_8\",\"peds_9\",\"peds_10\",\n",
    "#     \"peds_11\",\"peds_12\",\"peds_13\",\"peds_14\"\n",
    "# ]]\n",
    "vec.drop(['peds_1'], axis=1, inplace=True)\n",
    "\n",
    "places = { 1: \"札幌\", 2: \"函館\", 3: \"福島\", 4: \"新潟\", 5: \"東京\", 6: \"中山\", 7: \"中京\", 8: \"京都\", 9: \"阪神\", 10: \"小倉\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e296943f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = build_data('./pickle_new/base_race_20221016.pickle', './pickle_new/peds_vec.pickle', 2, vec)\n",
    "X_train1 = d['X_train']\n",
    "y_train1 = d['y_train']\n",
    "X_valid1 = d['X_valid']\n",
    "y_valid1 = d['y_valid']\n",
    "X_test2 = d['X_test2']\n",
    "y_test2 = d['y_test2']\n",
    "X_test1 = d['X_test1']\n",
    "y_test1 = d['y_test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74580db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 16:44:49,406]\u001b[0m A new study created in memory with name: no-name-3611aa6e-4080-40ec-9211-79c15a9cb928\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.530839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.261560:  14%|#########                                                      | 1/7 [00:23<02:18, 23.10s/it]\u001b[32m[I 2022-10-27 16:45:12,603]\u001b[0m Trial 0 finished with value: 0.26156001250825206 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.26156001250825206.\u001b[0m\n",
      "feature_fraction, val_score: 0.261560:  14%|#########                                                      | 1/7 [00:23<02:18, 23.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241423\tvalid_1's binary_logloss: 0.26156\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.255606:  29%|##################                                             | 2/7 [00:43<01:48, 21.76s/it]\u001b[32m[I 2022-10-27 16:45:33,403]\u001b[0m Trial 1 finished with value: 0.2556057604215257 and parameters: {'feature_fraction': 0.4}. Best is trial 1 with value: 0.2556057604215257.\u001b[0m\n",
      "feature_fraction, val_score: 0.255606:  29%|##################                                             | 2/7 [00:43<01:48, 21.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.219242\tvalid_1's binary_logloss: 0.255606\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.348277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.254464:  43%|###########################                                    | 3/7 [01:05<01:27, 21.85s/it]\u001b[32m[I 2022-10-27 16:45:55,383]\u001b[0m Trial 2 finished with value: 0.2544640102838614 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.2544640102838614.\u001b[0m\n",
      "feature_fraction, val_score: 0.254464:  43%|###########################                                    | 3/7 [01:05<01:27, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.216809\tvalid_1's binary_logloss: 0.254464\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.254464:  57%|####################################                           | 4/7 [01:28<01:06, 22.14s/it]\u001b[32m[I 2022-10-27 16:46:17,978]\u001b[0m Trial 3 finished with value: 0.254983112176578 and parameters: {'feature_fraction': 0.5}. Best is trial 2 with value: 0.2544640102838614.\u001b[0m\n",
      "feature_fraction, val_score: 0.254464:  57%|####################################                           | 4/7 [01:28<01:06, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.213637\tvalid_1's binary_logloss: 0.254983\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.366639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.254464:  71%|#############################################                  | 5/7 [01:49<00:43, 21.81s/it]\u001b[32m[I 2022-10-27 16:46:39,183]\u001b[0m Trial 4 finished with value: 0.26156001250825206 and parameters: {'feature_fraction': 1.0}. Best is trial 2 with value: 0.2544640102838614.\u001b[0m\n",
      "feature_fraction, val_score: 0.254464:  71%|#############################################                  | 5/7 [01:49<00:43, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241412\tvalid_1's binary_logloss: 0.26156\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.358624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.254464:  86%|######################################################         | 6/7 [02:10<00:21, 21.44s/it]\u001b[32m[I 2022-10-27 16:46:59,945]\u001b[0m Trial 5 finished with value: 0.26156001250825206 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 2 with value: 0.2544640102838614.\u001b[0m\n",
      "feature_fraction, val_score: 0.254464:  86%|######################################################         | 6/7 [02:10<00:21, 21.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241421\tvalid_1's binary_logloss: 0.26156\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.433319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.254389: 100%|###############################################################| 7/7 [02:31<00:00, 21.21s/it]\u001b[32m[I 2022-10-27 16:47:20,663]\u001b[0m Trial 6 finished with value: 0.25438916478518203 and parameters: {'feature_fraction': 0.7}. Best is trial 6 with value: 0.25438916478518203.\u001b[0m\n",
      "feature_fraction, val_score: 0.254389: 100%|###############################################################| 7/7 [02:31<00:00, 21.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.242273\tvalid_1's binary_logloss: 0.254389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.254389:   0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.387326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.254292:   5%|###4                                                                | 1/20 [00:22<07:02, 22.26s/it]\u001b[32m[I 2022-10-27 16:47:43,035]\u001b[0m Trial 7 finished with value: 0.2542923703185901 and parameters: {'num_leaves': 204}. Best is trial 7 with value: 0.2542923703185901.\u001b[0m\n",
      "num_leaves, val_score: 0.254292:   5%|###4                                                                | 1/20 [00:22<07:02, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240395\tvalid_1's binary_logloss: 0.254292\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.422907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.254292:  10%|######8                                                             | 2/20 [00:43<06:27, 21.50s/it]\u001b[32m[I 2022-10-27 16:48:03,978]\u001b[0m Trial 8 finished with value: 0.2546002879565273 and parameters: {'num_leaves': 65}. Best is trial 7 with value: 0.2542923703185901.\u001b[0m\n",
      "num_leaves, val_score: 0.254292:  10%|######8                                                             | 2/20 [00:43<06:27, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.24185\tvalid_1's binary_logloss: 0.2546\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.423550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.254292:  15%|##########2                                                         | 3/20 [01:04<06:01, 21.26s/it]\u001b[32m[I 2022-10-27 16:48:24,936]\u001b[0m Trial 9 finished with value: 0.2546037106722635 and parameters: {'num_leaves': 96}. Best is trial 7 with value: 0.2542923703185901.\u001b[0m\n",
      "num_leaves, val_score: 0.254292:  15%|##########2                                                         | 3/20 [01:04<06:01, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241502\tvalid_1's binary_logloss: 0.254604\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.494692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.254292:  20%|#############6                                                      | 4/20 [01:26<05:48, 21.77s/it]\u001b[32m[I 2022-10-27 16:48:47,525]\u001b[0m Trial 10 finished with value: 0.2542950797439164 and parameters: {'num_leaves': 214}. Best is trial 7 with value: 0.2542923703185901.\u001b[0m\n",
      "num_leaves, val_score: 0.254292:  20%|#############6                                                      | 4/20 [01:26<05:48, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240302\tvalid_1's binary_logloss: 0.254295\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.498937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  25%|#################                                                   | 5/20 [01:55<06:04, 24.27s/it]\u001b[32m[I 2022-10-27 16:49:16,190]\u001b[0m Trial 11 finished with value: 0.25361530788858433 and parameters: {'num_leaves': 245}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  25%|#################                                                   | 5/20 [01:55<06:04, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.20011\tvalid_1's binary_logloss: 0.253615\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.432505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  30%|####################4                                               | 6/20 [02:17<05:28, 23.49s/it]\u001b[32m[I 2022-10-27 16:49:38,172]\u001b[0m Trial 12 finished with value: 0.2542923703185901 and parameters: {'num_leaves': 197}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  30%|####################4                                               | 6/20 [02:17<05:28, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240465\tvalid_1's binary_logloss: 0.254292\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.431606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  35%|#######################7                                            | 7/20 [02:37<04:50, 22.32s/it]\u001b[32m[I 2022-10-27 16:49:58,070]\u001b[0m Trial 13 finished with value: 0.254725432442155 and parameters: {'num_leaves': 25}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  35%|#######################7                                            | 7/20 [02:37<04:50, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.242356\tvalid_1's binary_logloss: 0.254725\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.435406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  40%|###########################2                                        | 8/20 [02:59<04:28, 22.35s/it]\u001b[32m[I 2022-10-27 16:50:20,521]\u001b[0m Trial 14 finished with value: 0.2542923703185901 and parameters: {'num_leaves': 204}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  40%|###########################2                                        | 8/20 [02:59<04:28, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240395\tvalid_1's binary_logloss: 0.254292\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.489356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  45%|##############################6                                     | 9/20 [03:22<04:07, 22.50s/it]\u001b[32m[I 2022-10-27 16:50:43,329]\u001b[0m Trial 15 finished with value: 0.2542559390966072 and parameters: {'num_leaves': 216}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  45%|##############################6                                     | 9/20 [03:22<04:07, 22.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240284\tvalid_1's binary_logloss: 0.254256\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.540984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  50%|#################################5                                 | 10/20 [03:43<03:38, 21.88s/it]\u001b[32m[I 2022-10-27 16:51:03,807]\u001b[0m Trial 16 finished with value: 0.2542359098037317 and parameters: {'num_leaves': 55}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  50%|#################################5                                 | 10/20 [03:43<03:38, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241963\tvalid_1's binary_logloss: 0.254236\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.428102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  55%|####################################8                              | 11/20 [04:05<03:18, 22.06s/it]\u001b[32m[I 2022-10-27 16:51:26,329]\u001b[0m Trial 17 finished with value: 0.2543053298812615 and parameters: {'num_leaves': 146}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  55%|####################################8                              | 11/20 [04:05<03:18, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240967\tvalid_1's binary_logloss: 0.254305\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.448598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253615:  60%|########################################1                          | 12/20 [04:24<02:49, 21.14s/it]\u001b[32m[I 2022-10-27 16:51:45,348]\u001b[0m Trial 18 finished with value: 0.25517940748034534 and parameters: {'num_leaves': 8}. Best is trial 11 with value: 0.25361530788858433.\u001b[0m\n",
      "num_leaves, val_score: 0.253615:  60%|########################################1                          | 12/20 [04:24<02:49, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.242749\tvalid_1's binary_logloss: 0.255179\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.402266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  65%|###########################################5                       | 13/20 [04:57<02:53, 24.74s/it]\u001b[32m[I 2022-10-27 16:52:18,380]\u001b[0m Trial 19 finished with value: 0.2535928479492282 and parameters: {'num_leaves': 254}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  65%|###########################################5                       | 13/20 [04:57<02:53, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.199724\tvalid_1's binary_logloss: 0.253593\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.539942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  70%|##############################################9                    | 14/20 [05:24<02:31, 25.32s/it]\u001b[32m[I 2022-10-27 16:52:45,011]\u001b[0m Trial 20 finished with value: 0.25418653873263564 and parameters: {'num_leaves': 255}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  70%|##############################################9                    | 14/20 [05:24<02:31, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239892\tvalid_1's binary_logloss: 0.254187\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.402451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  75%|##################################################2                | 15/20 [05:54<02:14, 26.86s/it]\u001b[32m[I 2022-10-27 16:53:15,444]\u001b[0m Trial 21 finished with value: 0.2539078889287328 and parameters: {'num_leaves': 253}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  75%|##################################################2                | 15/20 [05:54<02:14, 26.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.199879\tvalid_1's binary_logloss: 0.253908\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.388813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  80%|#####################################################6             | 16/20 [06:17<01:42, 25.62s/it]\u001b[32m[I 2022-10-27 16:53:38,173]\u001b[0m Trial 22 finished with value: 0.2543053298812615 and parameters: {'num_leaves': 159}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  80%|#####################################################6             | 16/20 [06:17<01:42, 25.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240834\tvalid_1's binary_logloss: 0.254305\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.417970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  85%|########################################################9          | 17/20 [06:42<01:16, 25.40s/it]\u001b[32m[I 2022-10-27 16:54:03,077]\u001b[0m Trial 23 finished with value: 0.2543053298812615 and parameters: {'num_leaves': 165}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  85%|########################################################9          | 17/20 [06:42<01:16, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240776\tvalid_1's binary_logloss: 0.254305\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.930482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  90%|############################################################3      | 18/20 [07:09<00:51, 25.96s/it]\u001b[32m[I 2022-10-27 16:54:30,374]\u001b[0m Trial 24 finished with value: 0.25460921977146644 and parameters: {'num_leaves': 116}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  90%|############################################################3      | 18/20 [07:09<00:51, 25.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241284\tvalid_1's binary_logloss: 0.254609\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.414808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593:  95%|###############################################################6   | 19/20 [07:34<00:25, 25.78s/it]\u001b[32m[I 2022-10-27 16:54:55,695]\u001b[0m Trial 25 finished with value: 0.2541839282448098 and parameters: {'num_leaves': 234}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593:  95%|###############################################################6   | 19/20 [07:35<00:25, 25.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.2401\tvalid_1's binary_logloss: 0.254184\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.652207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.253593: 100%|###################################################################| 20/20 [07:58<00:00, 25.01s/it]\u001b[32m[I 2022-10-27 16:55:18,955]\u001b[0m Trial 26 finished with value: 0.25430287056332307 and parameters: {'num_leaves': 174}. Best is trial 19 with value: 0.2535928479492282.\u001b[0m\n",
      "num_leaves, val_score: 0.253593: 100%|###################################################################| 20/20 [07:58<00:00, 23.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240692\tvalid_1's binary_logloss: 0.254303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.253593:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.430704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.250521:  10%|#######1                                                               | 1/10 [00:23<03:34, 23.82s/it]\u001b[32m[I 2022-10-27 16:55:42,871]\u001b[0m Trial 27 finished with value: 0.25052115595148017 and parameters: {'bagging_fraction': 0.5179974421265621, 'bagging_freq': 2}. Best is trial 27 with value: 0.25052115595148017.\u001b[0m\n",
      "bagging, val_score: 0.250521:  10%|#######1                                                               | 1/10 [00:23<03:34, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.206436\tvalid_1's binary_logloss: 0.250521\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.401222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.250521:  20%|##############2                                                        | 2/10 [00:44<02:55, 21.90s/it]\u001b[32m[I 2022-10-27 16:56:03,415]\u001b[0m Trial 28 finished with value: 0.2539354086610345 and parameters: {'bagging_fraction': 0.4612988654590238, 'bagging_freq': 2}. Best is trial 27 with value: 0.25052115595148017.\u001b[0m\n",
      "bagging, val_score: 0.250521:  20%|##############2                                                        | 2/10 [00:44<02:55, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240307\tvalid_1's binary_logloss: 0.253935\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.423706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  30%|#####################3                                                 | 3/10 [01:12<02:51, 24.53s/it]\u001b[32m[I 2022-10-27 16:56:31,059]\u001b[0m Trial 29 finished with value: 0.24879404635466154 and parameters: {'bagging_fraction': 0.7909269449031665, 'bagging_freq': 4}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  30%|#####################3                                                 | 3/10 [01:12<02:51, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.431933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  40%|############################4                                          | 4/10 [01:45<02:49, 28.25s/it]\u001b[32m[I 2022-10-27 16:57:05,063]\u001b[0m Trial 30 finished with value: 0.2503025943986227 and parameters: {'bagging_fraction': 0.7141523160691811, 'bagging_freq': 3}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  40%|############################4                                          | 4/10 [01:46<02:49, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.179953\tvalid_1's binary_logloss: 0.250303\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.475654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  50%|###################################5                                   | 5/10 [02:09<02:12, 26.43s/it]\u001b[32m[I 2022-10-27 16:57:28,214]\u001b[0m Trial 31 finished with value: 0.2548845759294051 and parameters: {'bagging_fraction': 0.8135483013649841, 'bagging_freq': 1}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  50%|###################################5                                   | 5/10 [02:09<02:12, 26.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239978\tvalid_1's binary_logloss: 0.254885\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.417137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  60%|##########################################6                            | 6/10 [02:34<01:44, 26.11s/it]\u001b[32m[I 2022-10-27 16:57:53,749]\u001b[0m Trial 32 finished with value: 0.2491497532861977 and parameters: {'bagging_fraction': 0.661288473461435, 'bagging_freq': 3}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  60%|##########################################6                            | 6/10 [02:34<01:44, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.205916\tvalid_1's binary_logloss: 0.24915\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.425480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  70%|#################################################6                     | 7/10 [02:59<01:16, 25.57s/it]\u001b[32m[I 2022-10-27 16:58:18,164]\u001b[0m Trial 33 finished with value: 0.2520224445589621 and parameters: {'bagging_fraction': 0.47392953249436187, 'bagging_freq': 6}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  70%|#################################################6                     | 7/10 [02:59<01:16, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.202256\tvalid_1's binary_logloss: 0.252022\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.464451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  80%|########################################################8              | 8/10 [03:25<00:51, 25.73s/it]\u001b[32m[I 2022-10-27 16:58:44,265]\u001b[0m Trial 34 finished with value: 0.2530741585731505 and parameters: {'bagging_fraction': 0.6235975753558248, 'bagging_freq': 7}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  80%|########################################################8              | 8/10 [03:25<00:51, 25.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.201122\tvalid_1's binary_logloss: 0.253074\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.545867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794:  90%|###############################################################9       | 9/10 [03:51<00:25, 25.91s/it]\u001b[32m[I 2022-10-27 16:59:10,628]\u001b[0m Trial 35 finished with value: 0.2524403607529352 and parameters: {'bagging_fraction': 0.6344880286384051, 'bagging_freq': 5}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794:  90%|###############################################################9       | 9/10 [03:51<00:25, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.206072\tvalid_1's binary_logloss: 0.25244\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.438486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.248794: 100%|######################################################################| 10/10 [04:18<00:00, 26.37s/it]\u001b[32m[I 2022-10-27 16:59:37,983]\u001b[0m Trial 36 finished with value: 0.25064558891513017 and parameters: {'bagging_fraction': 0.657062006024016, 'bagging_freq': 1}. Best is trial 29 with value: 0.24879404635466154.\u001b[0m\n",
      "bagging, val_score: 0.248794: 100%|######################################################################| 10/10 [04:19<00:00, 25.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200304\tvalid_1's binary_logloss: 0.250646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:   0%|                                                                | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.418838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:  17%|#########3                                              | 1/6 [00:22<01:50, 22.16s/it]\u001b[32m[I 2022-10-27 17:00:00,228]\u001b[0m Trial 37 finished with value: 0.25866716755676633 and parameters: {'feature_fraction': 0.62}. Best is trial 37 with value: 0.25866716755676633.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794:  17%|#########3                                              | 1/6 [00:22<01:50, 22.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.243655\tvalid_1's binary_logloss: 0.258667\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.677098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:  33%|##################6                                     | 2/6 [00:45<01:30, 22.72s/it]\u001b[32m[I 2022-10-27 17:00:23,316]\u001b[0m Trial 38 finished with value: 0.25949805922311114 and parameters: {'feature_fraction': 0.6839999999999999}. Best is trial 37 with value: 0.25866716755676633.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794:  33%|##################6                                     | 2/6 [00:45<01:30, 22.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.243489\tvalid_1's binary_logloss: 0.259498\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.440817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:  50%|############################                            | 3/6 [01:07<01:08, 22.70s/it]\u001b[32m[I 2022-10-27 17:00:46,021]\u001b[0m Trial 39 finished with value: 0.25866716755676633 and parameters: {'feature_fraction': 0.652}. Best is trial 37 with value: 0.25866716755676633.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794:  50%|############################                            | 3/6 [01:08<01:08, 22.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.243627\tvalid_1's binary_logloss: 0.258667\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.465684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:  67%|#####################################3                  | 4/6 [01:31<00:45, 22.89s/it]\u001b[32m[I 2022-10-27 17:01:09,218]\u001b[0m Trial 40 finished with value: 0.25207243062472734 and parameters: {'feature_fraction': 0.748}. Best is trial 40 with value: 0.25207243062472734.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794:  67%|#####################################3                  | 4/6 [01:31<00:45, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239919\tvalid_1's binary_logloss: 0.252072\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.563150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794:  83%|##############################################6         | 5/6 [01:54<00:23, 23.08s/it]\u001b[32m[I 2022-10-27 17:01:32,594]\u001b[0m Trial 41 finished with value: 0.25222371454798476 and parameters: {'feature_fraction': 0.716}. Best is trial 40 with value: 0.25207243062472734.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794:  83%|##############################################6         | 5/6 [01:54<00:23, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239909\tvalid_1's binary_logloss: 0.252224\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.503527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.248794: 100%|########################################################| 6/6 [02:17<00:00, 23.17s/it]\u001b[32m[I 2022-10-27 17:01:55,984]\u001b[0m Trial 42 finished with value: 0.2615360844403466 and parameters: {'feature_fraction': 0.7799999999999999}. Best is trial 40 with value: 0.25207243062472734.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.248794: 100%|########################################################| 6/6 [02:17<00:00, 23.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239254\tvalid_1's binary_logloss: 0.261536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.570042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:   5%|##8                                                     | 1/20 [00:27<08:38, 27.28s/it]\u001b[32m[I 2022-10-27 17:02:23,355]\u001b[0m Trial 43 finished with value: 0.25186432835723704 and parameters: {'lambda_l1': 0.5763206083426712, 'lambda_l2': 0.00021968913770097274}. Best is trial 43 with value: 0.25186432835723704.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:   5%|##8                                                     | 1/20 [00:27<08:38, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200338\tvalid_1's binary_logloss: 0.251864\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.311326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  10%|#####6                                                  | 2/20 [00:48<07:09, 23.88s/it]\u001b[32m[I 2022-10-27 17:02:44,827]\u001b[0m Trial 44 finished with value: 0.25207632286844023 and parameters: {'lambda_l1': 1.229173238568292e-08, 'lambda_l2': 0.0004958059128977537}. Best is trial 43 with value: 0.25186432835723704.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  10%|#####6                                                  | 2/20 [00:48<07:09, 23.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239927\tvalid_1's binary_logloss: 0.252076\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.330944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  15%|########4                                               | 3/20 [01:15<07:06, 25.11s/it]\u001b[32m[I 2022-10-27 17:03:11,437]\u001b[0m Trial 45 finished with value: 0.24927313314913696 and parameters: {'lambda_l1': 1.9928978601609892e-07, 'lambda_l2': 0.010832445886286485}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  15%|########4                                               | 3/20 [01:15<07:06, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200157\tvalid_1's binary_logloss: 0.249273\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.361965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  20%|###########2                                            | 4/20 [01:37<06:23, 23.97s/it]\u001b[32m[I 2022-10-27 17:03:33,648]\u001b[0m Trial 46 finished with value: 0.2547867649973153 and parameters: {'lambda_l1': 0.18764407351064658, 'lambda_l2': 0.22592509641991637}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  20%|###########2                                            | 4/20 [01:37<06:23, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240303\tvalid_1's binary_logloss: 0.254787\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.311468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  25%|##############                                          | 5/20 [01:58<05:41, 22.75s/it]\u001b[32m[I 2022-10-27 17:03:54,256]\u001b[0m Trial 47 finished with value: 0.2570325103694132 and parameters: {'lambda_l1': 5.978001504183929, 'lambda_l2': 0.795788798613108}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  25%|##############                                          | 5/20 [01:58<05:41, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.241631\tvalid_1's binary_logloss: 0.257033\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.290685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  30%|################8                                       | 6/20 [02:20<05:14, 22.49s/it]\u001b[32m[I 2022-10-27 17:04:16,214]\u001b[0m Trial 48 finished with value: 0.25542280087027264 and parameters: {'lambda_l1': 1.332797306532111e-07, 'lambda_l2': 0.8252189348953044}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  30%|################8                                       | 6/20 [02:20<05:14, 22.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240762\tvalid_1's binary_logloss: 0.255423\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.297876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  35%|###################5                                    | 7/20 [02:47<05:13, 24.08s/it]\u001b[32m[I 2022-10-27 17:04:43,554]\u001b[0m Trial 49 finished with value: 0.24927327782079006 and parameters: {'lambda_l1': 1.4288569313487302e-08, 'lambda_l2': 0.010855699308306143}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  35%|###################5                                    | 7/20 [02:47<05:13, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200157\tvalid_1's binary_logloss: 0.249273\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.745894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  40%|######################4                                 | 8/20 [03:19<05:18, 26.54s/it]\u001b[32m[I 2022-10-27 17:05:15,410]\u001b[0m Trial 50 finished with value: 0.25551591456494643 and parameters: {'lambda_l1': 0.021682656807588633, 'lambda_l2': 1.0845426678922097}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  40%|######################4                                 | 8/20 [03:19<05:18, 26.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.24086\tvalid_1's binary_logloss: 0.255516\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.688032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  45%|#########################2                              | 9/20 [03:56<05:29, 29.95s/it]\u001b[32m[I 2022-10-27 17:05:52,806]\u001b[0m Trial 51 finished with value: 0.2519734550952024 and parameters: {'lambda_l1': 0.05244999775384274, 'lambda_l2': 3.415452042096414e-05}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  45%|#########################2                              | 9/20 [03:56<05:29, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200118\tvalid_1's binary_logloss: 0.251973\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.393702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  50%|###########################5                           | 10/20 [04:21<04:42, 28.24s/it]\u001b[32m[I 2022-10-27 17:06:17,234]\u001b[0m Trial 52 finished with value: 0.25336725516123426 and parameters: {'lambda_l1': 5.140337692639536e-06, 'lambda_l2': 0.10384120329294666}. Best is trial 45 with value: 0.24927313314913696.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  50%|###########################5                           | 10/20 [04:21<04:42, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240083\tvalid_1's binary_logloss: 0.253367\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.341264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  55%|##############################2                        | 11/20 [04:48<04:11, 27.94s/it]\u001b[32m[I 2022-10-27 17:06:44,514]\u001b[0m Trial 53 finished with value: 0.2487941700969266 and parameters: {'lambda_l1': 0.00010231597247892853, 'lambda_l2': 4.881948763110933e-08}. Best is trial 53 with value: 0.2487941700969266.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  55%|##############################2                        | 11/20 [04:48<04:11, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.435805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  60%|#################################                      | 12/20 [05:17<03:46, 28.29s/it]\u001b[32m[I 2022-10-27 17:07:13,588]\u001b[0m Trial 54 finished with value: 0.24879422590842984 and parameters: {'lambda_l1': 0.00014867210012245956, 'lambda_l2': 2.2332173585271073e-08}. Best is trial 53 with value: 0.2487941700969266.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  60%|#################################                      | 12/20 [05:17<03:46, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.442495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  65%|###################################7                   | 13/20 [05:45<03:18, 28.33s/it]\u001b[32m[I 2022-10-27 17:07:42,030]\u001b[0m Trial 55 finished with value: 0.24879350874812922 and parameters: {'lambda_l1': 0.0006182812183916449, 'lambda_l2': 1.9511200966226616e-08}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  65%|###################################7                   | 13/20 [05:46<03:18, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200209\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.410694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  70%|######################################5                | 14/20 [06:08<02:40, 26.70s/it]\u001b[32m[I 2022-10-27 17:08:04,974]\u001b[0m Trial 56 finished with value: 0.25207574309506814 and parameters: {'lambda_l1': 0.0013302652015376422, 'lambda_l2': 2.5666725365473107e-08}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  70%|######################################5                | 14/20 [06:08<02:40, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239926\tvalid_1's binary_logloss: 0.252076\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.517727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  75%|#########################################2             | 15/20 [06:38<02:17, 27.45s/it]\u001b[32m[I 2022-10-27 17:08:34,141]\u001b[0m Trial 57 finished with value: 0.24879416197007345 and parameters: {'lambda_l1': 8.876344962436318e-05, 'lambda_l2': 1.2616422186770512e-06}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  75%|#########################################2             | 15/20 [06:38<02:17, 27.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.447413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  80%|############################################           | 16/20 [07:01<01:45, 26.31s/it]\u001b[32m[I 2022-10-27 17:08:57,804]\u001b[0m Trial 58 finished with value: 0.2520761180902971 and parameters: {'lambda_l1': 0.0028198267847152846, 'lambda_l2': 9.180165957212996e-07}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  80%|############################################           | 16/20 [07:01<01:45, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239926\tvalid_1's binary_logloss: 0.252076\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.473057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  85%|##############################################7        | 17/20 [07:30<01:21, 27.20s/it]\u001b[32m[I 2022-10-27 17:09:27,062]\u001b[0m Trial 59 finished with value: 0.2487940758310999 and parameters: {'lambda_l1': 1.7562850566401432e-05, 'lambda_l2': 1.2228929269222481e-06}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  85%|##############################################7        | 17/20 [07:31<01:21, 27.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.500038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  90%|#################################################5     | 18/20 [08:00<00:55, 27.83s/it]\u001b[32m[I 2022-10-27 17:09:56,395]\u001b[0m Trial 60 finished with value: 0.24886615548223506 and parameters: {'lambda_l1': 7.216565610876222e-06, 'lambda_l2': 4.914752208617919e-07}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  90%|#################################################5     | 18/20 [08:00<00:55, 27.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248866\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.575508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794:  95%|####################################################2  | 19/20 [08:29<00:28, 28.29s/it]\u001b[32m[I 2022-10-27 17:10:25,733]\u001b[0m Trial 61 finished with value: 0.24879412112365792 and parameters: {'lambda_l1': 6.2819717617342155e-06, 'lambda_l2': 1.0457872680813492e-05}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794:  95%|####################################################2  | 19/20 [08:29<00:28, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200208\tvalid_1's binary_logloss: 0.248794\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.540701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.248794: 100%|#######################################################| 20/20 [08:58<00:00, 28.59s/it]\u001b[32m[I 2022-10-27 17:10:54,982]\u001b[0m Trial 62 finished with value: 0.2520273007699609 and parameters: {'lambda_l1': 0.006260270351311416, 'lambda_l2': 1.8990390282203758e-07}. Best is trial 55 with value: 0.24879350874812922.\u001b[0m\n",
      "regularization_factors, val_score: 0.248794: 100%|#######################################################| 20/20 [08:58<00:00, 26.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.200122\tvalid_1's binary_logloss: 0.252027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794:   0%|                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.468173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794:  20%|############6                                                  | 1/5 [00:24<01:37, 24.33s/it]\u001b[32m[I 2022-10-27 17:11:19,437]\u001b[0m Trial 63 finished with value: 0.2535382387541131 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.2535382387541131.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.248794:  20%|############6                                                  | 1/5 [00:24<01:37, 24.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240095\tvalid_1's binary_logloss: 0.253538\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.503545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794:  40%|#########################2                                     | 2/5 [00:47<01:11, 23.93s/it]\u001b[32m[I 2022-10-27 17:11:43,066]\u001b[0m Trial 64 finished with value: 0.253276414902437 and parameters: {'min_child_samples': 5}. Best is trial 64 with value: 0.253276414902437.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.248794:  40%|#########################2                                     | 2/5 [00:48<01:11, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.239882\tvalid_1's binary_logloss: 0.253276\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.550363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794:  60%|#####################################8                         | 3/5 [01:17<00:53, 26.58s/it]\u001b[32m[I 2022-10-27 17:12:12,799]\u001b[0m Trial 65 finished with value: 0.2523666809936395 and parameters: {'min_child_samples': 25}. Best is trial 65 with value: 0.2523666809936395.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.248794:  60%|#####################################8                         | 3/5 [01:17<00:53, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.20045\tvalid_1's binary_logloss: 0.252367\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.468054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794:  80%|##################################################4            | 4/5 [01:42<00:25, 25.91s/it]\u001b[32m[I 2022-10-27 17:12:37,681]\u001b[0m Trial 66 finished with value: 0.25399959143216516 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.2523666809936395.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.248794:  80%|##################################################4            | 4/5 [01:42<00:25, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.240341\tvalid_1's binary_logloss: 0.254\n",
      "[LightGBM] [Info] Number of positive: 30534, number of negative: 407502\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.616635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 106289\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069707 -> initscore=-2.591205\n",
      "[LightGBM] [Info] Start training from score -2.591205\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.248794: 100%|###############################################################| 5/5 [02:11<00:00, 26.98s/it]\u001b[32m[I 2022-10-27 17:13:06,545]\u001b[0m Trial 67 finished with value: 0.24922491911515807 and parameters: {'min_child_samples': 10}. Best is trial 67 with value: 0.24922491911515807.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.248794: 100%|###############################################################| 5/5 [02:11<00:00, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.199634\tvalid_1's binary_logloss: 0.249225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.0006182812183916449,\n",
       " 'lambda_l2': 1.9511200966226616e-08,\n",
       " 'num_leaves': 254,\n",
       " 'feature_fraction': 0.7,\n",
       " 'bagging_fraction': 0.7909269449031665,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_train1, y_train1, X_valid1, y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff605d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7909269449031665, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7909269449031665\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.9511200966226616e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9511200966226616e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0006182812183916449, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006182812183916449\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 0.0006182812183916449,\n",
    " 'lambda_l2': 1.9511200966226616e-08,\n",
    " 'num_leaves': 254,\n",
    " 'feature_fraction': 0.7,\n",
    " 'bagging_fraction': 0.7909269449031665,\n",
    " 'bagging_freq': 4,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "lgb_clf = fit(params, X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d38261a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764372263528572"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf, haitou, std=True)\n",
    "me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ac0ec168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyterthemes import jtplot\n",
    "\n",
    "y_pred = me_valid.predict_proba(X_valid1).values\n",
    "\n",
    "jtplot.style(theme='monokai')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid1, y_pred)\n",
    "pit.plot(fpr, tpr, marker='o')\n",
    "pit.xlabel(\"False positive rate\")\n",
    "pit.ylabel(\"True positive rate\")\n",
    "pit.grid()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "05bf9a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>逃げ率偏差</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>compi</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>調教師同コース同距離別騎乗回数偏差</td>\n",
       "      <td>1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>調教師距離別騎乗回数偏差</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>騎手同コース同距離別勝率偏差</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>furlong_5</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>騎手同コース同距離別騎乗回数偏差</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>騎乗騎手年間出遅れ率偏差</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>生涯出遅れ率偏差</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>調教師競馬場別勝率偏差</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>上がり3F平均偏差</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>調教師コース別騎乗回数偏差</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>騎手距離別騎乗回数偏差</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>調教師競馬場別騎乗回数偏差</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>コースタイプ複勝率偏差</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>同枠タイプ生涯複勝率偏差</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>騎手同コース同距離別複勝率偏差</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>調教師距離別勝率偏差</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>騎手同コース同距離別連対率偏差</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>種牡馬同コース同距離別勝率偏差</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>調教師距離別複勝率偏差</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>連対率偏差</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>騎乗騎手年間出遅れ率</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>調教師年齢別年間勝率偏差</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>furlong_5_4</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>中団率偏差</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>騎手競馬場別騎乗回数偏差</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>コースタイプ連対率偏差</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1走前先行指数</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>調教師競馬場別複勝率偏差</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>同枠タイプ生涯連対率偏差</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>先行率偏差</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>調教師距離別連対率偏差</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>騎手距離別複勝率偏差</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>種牡馬同コース同距離別連対率偏差</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>調教師同コース同距離別連対率偏差</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>騎手競馬場別勝率偏差</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1走前上がり指数</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1走前スピード指数</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>peds_2</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "644              逃げ率偏差        1109\n",
       "11               compi        1055\n",
       "586  調教師同コース同距離別騎乗回数偏差        1006\n",
       "582       調教師距離別騎乗回数偏差         942\n",
       "568     騎手同コース同距離別勝率偏差         927\n",
       "22           furlong_5         924\n",
       "567   騎手同コース同距離別騎乗回数偏差         917\n",
       "675       騎乗騎手年間出遅れ率偏差         916\n",
       "674           生涯出遅れ率偏差         911\n",
       "575        調教師競馬場別勝率偏差         903\n",
       "649          上がり3F平均偏差         900\n",
       "578      調教師コース別騎乗回数偏差         898\n",
       "563        騎手距離別騎乗回数偏差         889\n",
       "574      調教師競馬場別騎乗回数偏差         887\n",
       "671        コースタイプ複勝率偏差         878\n",
       "673       同枠タイプ生涯複勝率偏差         871\n",
       "570    騎手同コース同距離別複勝率偏差         861\n",
       "583         調教師距離別勝率偏差         860\n",
       "569    騎手同コース同距離別連対率偏差         852\n",
       "612    種牡馬同コース同距離別勝率偏差         844\n",
       "585        調教師距離別複勝率偏差         838\n",
       "658              連対率偏差         834\n",
       "548         騎乗騎手年間出遅れ率         833\n",
       "588       調教師年齢別年間勝率偏差         821\n",
       "27         furlong_5_4         818\n",
       "646              中団率偏差         810\n",
       "555       騎手競馬場別騎乗回数偏差         803\n",
       "663        コースタイプ連対率偏差         799\n",
       "66             1走前先行指数         795\n",
       "577       調教師競馬場別複勝率偏差         793\n",
       "665       同枠タイプ生涯連対率偏差         793\n",
       "645              先行率偏差         793\n",
       "584        調教師距離別連対率偏差         790\n",
       "566         騎手距離別複勝率偏差         788\n",
       "613   種牡馬同コース同距離別連対率偏差         785\n",
       "594   調教師同コース同距離別連対率偏差         785\n",
       "556         騎手競馬場別勝率偏差         779\n",
       "68            1走前上がり指数         778\n",
       "69           1走前スピード指数         778\n",
       "699             peds_2         773"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_valid.feature_importance(X_train1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "907c7aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1501 レース数:9206 対象レース率:16.3% 単勝的中率:16.7% 的中数:250 賭金:150,100円 単勝配当合計:135,970円 単勝最高配当:2,720円 単勝回収率:90.6%\n",
      "点数：1501 賭金:150,100円 複勝的中率:43.9% 複勝的中数:659 複勝配当合計:127,980円 複勝回収率:85.3%\n"
     ]
    }
   ],
   "source": [
    "wr = me_valid.pred_table(X_valid1, 0.75, True)\n",
    "bt = tansho_return_rate(X_valid1, wr)\n",
    "fukusho_return_rate(bt[['race_id', 'h_num']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c723c0c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：758 レース数:3946 対象レース率:19.2% 単勝的中率:12.5% 的中数:95 賭金:75,800円 単勝配当合計:54,440円 単勝最高配当:2,480円 単勝回収率:71.8%\n",
      "点数：758 賭金:75,800円 複勝的中率:38.4% 複勝的中数:291 複勝配当合計:58,940円 複勝回収率:77.8%\n"
     ]
    }
   ],
   "source": [
    "me_test = ModelEvaluator(lgb_clf, haitou, std=True)\n",
    "wrt = me_test.pred_table(X_test2, 0.75, True)\n",
    "btt = tansho_return_rate(X_test2, wrt)\n",
    "fukusho_return_rate(btt[['race_id', 'h_num']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3db07148",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('./csv_new2/base/training_prev.csv')\n",
    "t.to_pickle('./pickle_new/training_prev.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "55b6232c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./pickle_new/training_prev.pickle')\n",
    "df = df.reset_index()\n",
    "df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "df['id'] = df['horse_race_id']\n",
    "\n",
    "df = df.sort_values(by='date', ascending = False)\n",
    "t_allrace = df.set_index('race_id')\n",
    "t_allrace['result'] = t_allrace['result'].map(lambda x: 1 if x < 2 else 0)\n",
    "\n",
    "# t_allrace = t_allrace.drop([\n",
    "#     'rank', 'rank.1', 'rank.2', 'rank.3', 'rank.4',\n",
    "# #     'training_course', 'training_course.1', 'training_course.2', 'training_course.3', 'training_course.4'\n",
    "# ], axis=1)\n",
    "t_allrace = t_allrace.fillna({\n",
    "    'rank': '-', 'rank.1': '-', 'rank.2': '-', 'rank.3': '-', 'rank.4': '-',\n",
    "    'training_course': '', 'training_course.1': '', 'training_course.2': '', 'training_course.3': '', 'training_course.4': '',\n",
    "})\n",
    "t_categorical = process_categorical(t_allrace, [\n",
    "    'rank', 'rank.1', 'rank.2', 'rank.3', 'rank.4',\n",
    "    'training_course', 'training_course.1', 'training_course.2', 'training_course.3', 'training_course.4',\n",
    "    'trainer_id'\n",
    "])\n",
    "\n",
    "t_train1, t_valid1 = split_data(t_categorical)\n",
    "t_valid1, t_test2 = train_valid_split_data(t_valid1)\n",
    "\n",
    "tdf = pd.read_csv('./training.csv')\n",
    "tdf = tdf.reset_index()\n",
    "tdf['date'] = tdf['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "tdf['id'] = tdf['horse_race_id']\n",
    "\n",
    "tdf = tdf.sort_values(by='date', ascending = False)\n",
    "tt_allrace = tdf.set_index('race_id')\n",
    "tt_allrace['result'] = tt_allrace['result'].map(lambda x: 1 if x < 2 else 0)\n",
    "\n",
    "tt_allrace = tt_allrace.fillna({\n",
    "    'rank': '-', 'rank.1': '-', 'rank.2': '-', 'rank.3': '-', 'rank.4': '-',\n",
    "    'training_course': '', 'training_course.1': '', 'training_course.2': '', 'training_course.3': '', 'training_course.4': '',\n",
    "})\n",
    "t_test = process_categorical(tt_allrace, [\n",
    "    'rank', 'rank.1', 'rank.2', 'rank.3', 'rank.4',\n",
    "    'training_course', 'training_course.1', 'training_course.2', 'training_course.3', 'training_course.4',\n",
    "])\n",
    "\n",
    "X_training_train1 = t_train1.drop(['id', 'date', 'trainer_id', 'odds', 'index', 'result', 'time_odds', 'horse_race_id', 'horse_race_id.1', 'horse_race_id.2', 'horse_race_id.3', 'horse_race_id.4'], axis=1)\n",
    "y_training_train1 = t_train1['result']\n",
    "X_training_valid1 = t_valid1.drop(['date', 'trainer_id','index', 'horse_race_id','result', 'horse_race_id.1', 'horse_race_id.2', 'horse_race_id.3', 'horse_race_id.4'], axis=1)\n",
    "y_training_valid1 = t_valid1['result']\n",
    "X_training_test2 = t_test2.drop(['date','trainer_id', 'index', 'horse_race_id', 'result','horse_race_id.1', 'horse_race_id.2', 'horse_race_id.3', 'horse_race_id.4'], axis=1)\n",
    "y_training_test2 = t_test2['result']\n",
    "X_training_test1 = t_test.drop(['date', 'index', 'horse_race_id', 'result','horse_race_id.1', 'horse_race_id.2', 'horse_race_id.3', 'horse_race_id.4'], axis=1)\n",
    "y_training_test1 = t_test['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "2cc835a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-28 15:13:07,242]\u001b[0m A new study created in memory with name: no-name-1e360c1a-6e3b-4209-ac74-7956a502344e\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.260010:  14%|#########                                                      | 1/7 [00:02<00:12,  2.16s/it]\u001b[32m[I 2022-10-28 15:13:09,409]\u001b[0m Trial 0 finished with value: 0.26000984240076985 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.26000984240076985.\u001b[0m\n",
      "feature_fraction, val_score: 0.260010:  14%|#########                                                      | 1/7 [00:02<00:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249273\tvalid_1's binary_logloss: 0.26001\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733:  29%|##################                                             | 2/7 [00:04<00:10,  2.12s/it]\u001b[32m[I 2022-10-28 15:13:11,502]\u001b[0m Trial 1 finished with value: 0.2597327534372159 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733:  29%|##################                                             | 2/7 [00:04<00:10,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250641\tvalid_1's binary_logloss: 0.259733\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733:  43%|###########################                                    | 3/7 [00:06<00:08,  2.13s/it]\u001b[32m[I 2022-10-28 15:13:13,639]\u001b[0m Trial 2 finished with value: 0.25980768049164793 and parameters: {'feature_fraction': 0.7}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733:  43%|###########################                                    | 3/7 [00:06<00:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25064\tvalid_1's binary_logloss: 0.259808\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733:  57%|####################################                           | 4/7 [00:08<00:06,  2.06s/it]\u001b[32m[I 2022-10-28 15:13:15,602]\u001b[0m Trial 3 finished with value: 0.25979187841728746 and parameters: {'feature_fraction': 0.4}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733:  57%|####################################                           | 4/7 [00:08<00:06,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250643\tvalid_1's binary_logloss: 0.259792\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733:  71%|#############################################                  | 5/7 [00:10<00:04,  2.07s/it]\u001b[32m[I 2022-10-28 15:13:17,688]\u001b[0m Trial 4 finished with value: 0.25980768049164793 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733:  71%|#############################################                  | 5/7 [00:10<00:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25064\tvalid_1's binary_logloss: 0.259808\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733:  86%|######################################################         | 6/7 [00:12<00:02,  2.09s/it]\u001b[32m[I 2022-10-28 15:13:19,831]\u001b[0m Trial 5 finished with value: 0.25998589913278747 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733:  86%|######################################################         | 6/7 [00:12<00:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.248871\tvalid_1's binary_logloss: 0.259986\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.259733: 100%|###############################################################| 7/7 [00:14<00:00,  2.10s/it]\u001b[32m[I 2022-10-28 15:13:21,952]\u001b[0m Trial 6 finished with value: 0.26000976387924324 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.2597327534372159.\u001b[0m\n",
      "feature_fraction, val_score: 0.259733: 100%|###############################################################| 7/7 [00:14<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249273\tvalid_1's binary_logloss: 0.26001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259733:   0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259733:   5%|###4                                                                | 1/20 [00:02<00:41,  2.18s/it]\u001b[32m[I 2022-10-28 15:13:24,138]\u001b[0m Trial 7 finished with value: 0.2598646456851493 and parameters: {'num_leaves': 119}. Best is trial 7 with value: 0.2598646456851493.\u001b[0m\n",
      "num_leaves, val_score: 0.259733:   5%|###4                                                                | 1/20 [00:02<00:41,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250199\tvalid_1's binary_logloss: 0.259865\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259733:  10%|######8                                                             | 2/20 [00:04<00:39,  2.18s/it]\u001b[32m[I 2022-10-28 15:13:26,312]\u001b[0m Trial 8 finished with value: 0.2598353403086734 and parameters: {'num_leaves': 99}. Best is trial 8 with value: 0.2598353403086734.\u001b[0m\n",
      "num_leaves, val_score: 0.259733:  10%|######8                                                             | 2/20 [00:04<00:39,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25029\tvalid_1's binary_logloss: 0.259835\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259733:  15%|##########2                                                         | 3/20 [00:06<00:37,  2.19s/it]\u001b[32m[I 2022-10-28 15:13:28,514]\u001b[0m Trial 9 finished with value: 0.2598686202912561 and parameters: {'num_leaves': 146}. Best is trial 8 with value: 0.2598353403086734.\u001b[0m\n",
      "num_leaves, val_score: 0.259733:  15%|##########2                                                         | 3/20 [00:06<00:37,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250086\tvalid_1's binary_logloss: 0.259869\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259733:  20%|#############6                                                      | 4/20 [00:08<00:35,  2.20s/it]\u001b[32m[I 2022-10-28 15:13:30,730]\u001b[0m Trial 10 finished with value: 0.2598686202912561 and parameters: {'num_leaves': 145}. Best is trial 8 with value: 0.2598353403086734.\u001b[0m\n",
      "num_leaves, val_score: 0.259733:  20%|#############6                                                      | 4/20 [00:08<00:35,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25009\tvalid_1's binary_logloss: 0.259869\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  25%|#################                                                   | 5/20 [00:10<00:31,  2.08s/it]\u001b[32m[I 2022-10-28 15:13:32,595]\u001b[0m Trial 11 finished with value: 0.2597310043706941 and parameters: {'num_leaves': 7}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  25%|#################                                                   | 5/20 [00:10<00:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250954\tvalid_1's binary_logloss: 0.259731\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  30%|####################4                                               | 6/20 [00:12<00:29,  2.13s/it]\u001b[32m[I 2022-10-28 15:13:34,828]\u001b[0m Trial 12 finished with value: 0.25985752439027066 and parameters: {'num_leaves': 179}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  30%|####################4                                               | 6/20 [00:12<00:29,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249949\tvalid_1's binary_logloss: 0.259858\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  35%|#######################7                                            | 7/20 [00:14<00:27,  2.11s/it]\u001b[32m[I 2022-10-28 15:13:36,899]\u001b[0m Trial 13 finished with value: 0.25979901431533053 and parameters: {'num_leaves': 56}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  35%|#######################7                                            | 7/20 [00:14<00:27,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250489\tvalid_1's binary_logloss: 0.259799\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  40%|###########################2                                        | 8/20 [00:16<00:24,  2.05s/it]\u001b[32m[I 2022-10-28 15:13:38,824]\u001b[0m Trial 14 finished with value: 0.25977945297595334 and parameters: {'num_leaves': 12}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  40%|###########################2                                        | 8/20 [00:16<00:24,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250812\tvalid_1's binary_logloss: 0.259779\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  45%|##############################6                                     | 9/20 [00:18<00:22,  2.02s/it]\u001b[32m[I 2022-10-28 15:13:40,762]\u001b[0m Trial 15 finished with value: 0.25977945297595334 and parameters: {'num_leaves': 12}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  45%|##############################6                                     | 9/20 [00:18<00:22,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250812\tvalid_1's binary_logloss: 0.259779\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  50%|#################################5                                 | 10/20 [00:21<00:20,  2.08s/it]\u001b[32m[I 2022-10-28 15:13:42,982]\u001b[0m Trial 16 finished with value: 0.25985752439027066 and parameters: {'num_leaves': 175}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  50%|#################################5                                 | 10/20 [00:21<00:20,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249967\tvalid_1's binary_logloss: 0.259858\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  55%|####################################8                              | 11/20 [00:23<00:19,  2.15s/it]\u001b[32m[I 2022-10-28 15:13:45,298]\u001b[0m Trial 17 finished with value: 0.25986397380002463 and parameters: {'num_leaves': 237}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  55%|####################################8                              | 11/20 [00:23<00:19,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249708\tvalid_1's binary_logloss: 0.259864\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  60%|########################################1                          | 12/20 [00:25<00:16,  2.07s/it]\u001b[32m[I 2022-10-28 15:13:47,179]\u001b[0m Trial 18 finished with value: 0.2597310043706941 and parameters: {'num_leaves': 7}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  60%|########################################1                          | 12/20 [00:25<00:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250954\tvalid_1's binary_logloss: 0.259731\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  65%|###########################################5                       | 13/20 [00:27<00:14,  2.08s/it]\u001b[32m[I 2022-10-28 15:13:49,273]\u001b[0m Trial 19 finished with value: 0.25979901431533053 and parameters: {'num_leaves': 57}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  65%|###########################################5                       | 13/20 [00:27<00:14,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250484\tvalid_1's binary_logloss: 0.259799\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259731:  70%|##############################################9                    | 14/20 [00:29<00:12,  2.08s/it]\u001b[32m[I 2022-10-28 15:13:51,343]\u001b[0m Trial 20 finished with value: 0.25979901431533053 and parameters: {'num_leaves': 55}. Best is trial 11 with value: 0.2597310043706941.\u001b[0m\n",
      "num_leaves, val_score: 0.259731:  70%|##############################################9                    | 14/20 [00:29<00:12,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250496\tvalid_1's binary_logloss: 0.259799\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662:  75%|##################################################2                | 15/20 [00:31<00:10,  2.01s/it]\u001b[32m[I 2022-10-28 15:13:53,190]\u001b[0m Trial 21 finished with value: 0.25966216764225214 and parameters: {'num_leaves': 4}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662:  75%|##################################################2                | 15/20 [00:31<00:10,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251087\tvalid_1's binary_logloss: 0.259662\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662:  80%|#####################################################6             | 16/20 [00:33<00:08,  2.05s/it]\u001b[32m[I 2022-10-28 15:13:55,346]\u001b[0m Trial 22 finished with value: 0.25983663301057497 and parameters: {'num_leaves': 89}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662:  80%|#####################################################6             | 16/20 [00:33<00:08,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250334\tvalid_1's binary_logloss: 0.259837\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662:  85%|########################################################9          | 17/20 [00:35<00:06,  2.06s/it]\u001b[32m[I 2022-10-28 15:13:57,422]\u001b[0m Trial 23 finished with value: 0.2597327534372159 and parameters: {'num_leaves': 36}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662:  85%|########################################################9          | 17/20 [00:35<00:06,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250607\tvalid_1's binary_logloss: 0.259733\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662:  90%|############################################################3      | 18/20 [00:37<00:04,  2.14s/it]\u001b[32m[I 2022-10-28 15:13:59,759]\u001b[0m Trial 24 finished with value: 0.2598872486998787 and parameters: {'num_leaves': 245}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662:  90%|############################################################3      | 18/20 [00:37<00:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.249681\tvalid_1's binary_logloss: 0.259887\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662:  95%|###############################################################6   | 19/20 [00:39<00:02,  2.11s/it]\u001b[32m[I 2022-10-28 15:14:01,776]\u001b[0m Trial 25 finished with value: 0.2597327534372159 and parameters: {'num_leaves': 36}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662:  95%|###############################################################6   | 19/20 [00:39<00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250607\tvalid_1's binary_logloss: 0.259733\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.259662: 100%|###################################################################| 20/20 [00:41<00:00,  2.10s/it]\u001b[32m[I 2022-10-28 15:14:03,879]\u001b[0m Trial 26 finished with value: 0.25983496191051164 and parameters: {'num_leaves': 75}. Best is trial 21 with value: 0.25966216764225214.\u001b[0m\n",
      "num_leaves, val_score: 0.259662: 100%|###################################################################| 20/20 [00:41<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.250395\tvalid_1's binary_logloss: 0.259835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259662:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259662:  10%|#######1                                                               | 1/10 [00:01<00:17,  1.99s/it]\u001b[32m[I 2022-10-28 15:14:05,877]\u001b[0m Trial 27 finished with value: 0.2596635669569468 and parameters: {'bagging_fraction': 0.9771313057078342, 'bagging_freq': 4}. Best is trial 27 with value: 0.2596635669569468.\u001b[0m\n",
      "bagging, val_score: 0.259662:  10%|#######1                                                               | 1/10 [00:01<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251087\tvalid_1's binary_logloss: 0.259664\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  20%|##############2                                                        | 2/10 [00:03<00:15,  1.90s/it]\u001b[32m[I 2022-10-28 15:14:07,716]\u001b[0m Trial 28 finished with value: 0.2596611877827718 and parameters: {'bagging_fraction': 0.47628567079676276, 'bagging_freq': 3}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  20%|##############2                                                        | 2/10 [00:03<00:15,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  30%|#####################3                                                 | 3/10 [00:05<00:13,  1.95s/it]\u001b[32m[I 2022-10-28 15:14:09,723]\u001b[0m Trial 29 finished with value: 0.2596639298765894 and parameters: {'bagging_fraction': 0.8501401376456775, 'bagging_freq': 2}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  30%|#####################3                                                 | 3/10 [00:05<00:13,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251088\tvalid_1's binary_logloss: 0.259664\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  40%|############################4                                          | 4/10 [00:07<00:11,  1.94s/it]\u001b[32m[I 2022-10-28 15:14:11,643]\u001b[0m Trial 30 finished with value: 0.2596619554265972 and parameters: {'bagging_fraction': 0.5955166949215172, 'bagging_freq': 2}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  40%|############################4                                          | 4/10 [00:07<00:11,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251081\tvalid_1's binary_logloss: 0.259662\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  50%|###################################5                                   | 5/10 [00:09<00:09,  1.91s/it]\u001b[32m[I 2022-10-28 15:14:13,494]\u001b[0m Trial 31 finished with value: 0.259661199842527 and parameters: {'bagging_fraction': 0.47192042112015464, 'bagging_freq': 5}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  50%|###################################5                                   | 5/10 [00:09<00:09,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251077\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  60%|##########################################6                            | 6/10 [00:11<00:07,  1.90s/it]\u001b[32m[I 2022-10-28 15:14:15,386]\u001b[0m Trial 32 finished with value: 0.25966529990013054 and parameters: {'bagging_fraction': 0.7573706488823202, 'bagging_freq': 5}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  60%|##########################################6                            | 6/10 [00:11<00:07,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251086\tvalid_1's binary_logloss: 0.259665\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  70%|#################################################6                     | 7/10 [00:13<00:05,  1.93s/it]\u001b[32m[I 2022-10-28 15:14:17,375]\u001b[0m Trial 33 finished with value: 0.25966169551880963 and parameters: {'bagging_fraction': 0.4621260677122176, 'bagging_freq': 1}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  70%|#################################################6                     | 7/10 [00:13<00:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251082\tvalid_1's binary_logloss: 0.259662\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  80%|########################################################8              | 8/10 [00:15<00:03,  1.92s/it]\u001b[32m[I 2022-10-28 15:14:19,264]\u001b[0m Trial 34 finished with value: 0.2596656491123814 and parameters: {'bagging_fraction': 0.6653540680583979, 'bagging_freq': 4}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  80%|########################################################8              | 8/10 [00:15<00:03,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25108\tvalid_1's binary_logloss: 0.259666\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661:  90%|###############################################################9       | 9/10 [00:17<00:01,  1.88s/it]\u001b[32m[I 2022-10-28 15:14:21,067]\u001b[0m Trial 35 finished with value: 0.259665429807803 and parameters: {'bagging_fraction': 0.6500528698288199, 'bagging_freq': 1}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661:  90%|###############################################################9       | 9/10 [00:17<00:01,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251081\tvalid_1's binary_logloss: 0.259665\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.259661: 100%|######################################################################| 10/10 [00:19<00:00,  1.87s/it]\u001b[32m[I 2022-10-28 15:14:22,919]\u001b[0m Trial 36 finished with value: 0.2596642771358465 and parameters: {'bagging_fraction': 0.6017664892150882, 'bagging_freq': 7}. Best is trial 28 with value: 0.2596611877827718.\u001b[0m\n",
      "bagging, val_score: 0.259661: 100%|######################################################################| 10/10 [00:19<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.25108\tvalid_1's binary_logloss: 0.259664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:   0%|                                                                | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:  17%|#########3                                              | 1/6 [00:01<00:09,  1.83s/it]\u001b[32m[I 2022-10-28 15:14:24,761]\u001b[0m Trial 37 finished with value: 0.2596611877827718 and parameters: {'feature_fraction': 0.484}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661:  17%|#########3                                              | 1/6 [00:01<00:09,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:  33%|##################6                                     | 2/6 [00:03<00:07,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:26,642]\u001b[0m Trial 38 finished with value: 0.2596631977691134 and parameters: {'feature_fraction': 0.45199999999999996}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661:  33%|##################6                                     | 2/6 [00:03<00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259663\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:  50%|############################                            | 3/6 [00:05<00:05,  1.91s/it]\u001b[32m[I 2022-10-28 15:14:28,602]\u001b[0m Trial 39 finished with value: 0.2596666825970873 and parameters: {'feature_fraction': 0.58}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661:  50%|############################                            | 3/6 [00:05<00:05,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259667\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:  67%|#####################################3                  | 4/6 [00:07<00:03,  1.89s/it]\u001b[32m[I 2022-10-28 15:14:30,474]\u001b[0m Trial 40 finished with value: 0.2596611877827718 and parameters: {'feature_fraction': 0.516}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661:  67%|#####################################3                  | 4/6 [00:07<00:03,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661:  83%|##############################################6         | 5/6 [00:09<00:01,  1.90s/it]\u001b[32m[I 2022-10-28 15:14:32,383]\u001b[0m Trial 41 finished with value: 0.2596631977691134 and parameters: {'feature_fraction': 0.42}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661:  83%|##############################################6         | 5/6 [00:09<00:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259663\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.259661: 100%|########################################################| 6/6 [00:11<00:00,  1.89s/it]\u001b[32m[I 2022-10-28 15:14:34,261]\u001b[0m Trial 42 finished with value: 0.2596666825970873 and parameters: {'feature_fraction': 0.5479999999999999}. Best is trial 37 with value: 0.2596611877827718.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.259661: 100%|########################################################| 6/6 [00:11<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:   5%|##8                                                     | 1/20 [00:01<00:34,  1.83s/it]\u001b[32m[I 2022-10-28 15:14:36,100]\u001b[0m Trial 43 finished with value: 0.2596611876339716 and parameters: {'lambda_l1': 0.0014744093521765521, 'lambda_l2': 3.12506115124748e-05}. Best is trial 43 with value: 0.2596611876339716.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:   5%|##8                                                     | 1/20 [00:01<00:34,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  10%|#####6                                                  | 2/20 [00:03<00:33,  1.85s/it]\u001b[32m[I 2022-10-28 15:14:37,968]\u001b[0m Trial 44 finished with value: 0.25966117202640626 and parameters: {'lambda_l1': 0.14735425114234899, 'lambda_l2': 0.04897655078963681}. Best is trial 44 with value: 0.25966117202640626.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  10%|#####6                                                  | 2/20 [00:03<00:33,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  15%|########4                                               | 3/20 [00:05<00:31,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:39,827]\u001b[0m Trial 45 finished with value: 0.25966068964919076 and parameters: {'lambda_l1': 4.957585472019195, 'lambda_l2': 2.1987610837389894e-05}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  15%|########4                                               | 3/20 [00:05<00:31,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251077\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  20%|###########2                                            | 4/20 [00:07<00:29,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:41,698]\u001b[0m Trial 46 finished with value: 0.2596611877740573 and parameters: {'lambda_l1': 1.6130337132004091e-06, 'lambda_l2': 0.0004507136713584197}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  20%|###########2                                            | 4/20 [00:07<00:29,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  25%|##############                                          | 5/20 [00:09<00:27,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:43,543]\u001b[0m Trial 47 finished with value: 0.25966118721228815 and parameters: {'lambda_l1': 1.0472623594323042e-05, 'lambda_l2': 0.029684550781249423}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  25%|##############                                          | 5/20 [00:09<00:27,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  30%|################8                                       | 6/20 [00:11<00:26,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:45,410]\u001b[0m Trial 48 finished with value: 0.25966118778178443 and parameters: {'lambda_l1': 1.0538601709535741e-05, 'lambda_l2': 8.503600581337354e-07}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  30%|################8                                       | 6/20 [00:11<00:26,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  35%|###################5                                    | 7/20 [00:13<00:24,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:47,283]\u001b[0m Trial 49 finished with value: 0.2596609091715459 and parameters: {'lambda_l1': 2.7717666365777336, 'lambda_l2': 0.0005534847442338991}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  35%|###################5                                    | 7/20 [00:13<00:24,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251077\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  40%|######################4                                 | 8/20 [00:14<00:22,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:49,130]\u001b[0m Trial 50 finished with value: 0.25966118751143735 and parameters: {'lambda_l1': 0.002488491110788585, 'lambda_l2': 0.0010993860833120347}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  40%|######################4                                 | 8/20 [00:14<00:22,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  45%|#########################2                              | 9/20 [00:16<00:20,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:50,986]\u001b[0m Trial 51 finished with value: 0.25966109121412023 and parameters: {'lambda_l1': 0.9604450430657255, 'lambda_l2': 0.00016327495354428155}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  45%|#########################2                              | 9/20 [00:16<00:20,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  50%|###########################5                           | 10/20 [00:18<00:18,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:52,868]\u001b[0m Trial 52 finished with value: 0.25966118766635493 and parameters: {'lambda_l1': 0.0010560630012330242, 'lambda_l2': 0.0005379351389786181}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  50%|###########################5                           | 10/20 [00:18<00:18,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  55%|##############################2                        | 11/20 [00:20<00:16,  1.86s/it]\u001b[32m[I 2022-10-28 15:14:54,717]\u001b[0m Trial 53 finished with value: 0.25966118778276415 and parameters: {'lambda_l1': 1.1044231605164693e-08, 'lambda_l2': 1.3859823649264295e-08}. Best is trial 45 with value: 0.25966068964919076.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  55%|##############################2                        | 11/20 [00:20<00:16,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  60%|#################################                      | 12/20 [00:22<00:15,  1.88s/it]\u001b[32m[I 2022-10-28 15:14:56,630]\u001b[0m Trial 54 finished with value: 0.2596605682088111 and parameters: {'lambda_l1': 6.167384960706648, 'lambda_l2': 3.66894768678273e-06}. Best is trial 54 with value: 0.2596605682088111.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  60%|#################################                      | 12/20 [00:22<00:15,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251078\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259661:  65%|###################################7                   | 13/20 [00:24<00:13,  1.87s/it]\u001b[32m[I 2022-10-28 15:14:58,481]\u001b[0m Trial 55 finished with value: 0.2596611811172767 and parameters: {'lambda_l1': 0.0662845996785587, 'lambda_l2': 4.8458200222850014e-06}. Best is trial 54 with value: 0.2596605682088111.\u001b[0m\n",
      "regularization_factors, val_score: 0.259661:  65%|###################################7                   | 13/20 [00:24<00:13,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  70%|######################################5                | 14/20 [00:26<00:11,  1.87s/it]\u001b[32m[I 2022-10-28 15:15:00,336]\u001b[0m Trial 56 finished with value: 0.2596603079319639 and parameters: {'lambda_l1': 8.761843231077567, 'lambda_l2': 1.578450732409142e-07}. Best is trial 56 with value: 0.2596603079319639.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  70%|######################################5                | 14/20 [00:26<00:11,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251078\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  75%|#########################################2             | 15/20 [00:27<00:09,  1.86s/it]\u001b[32m[I 2022-10-28 15:15:02,204]\u001b[0m Trial 57 finished with value: 0.2596611846561753 and parameters: {'lambda_l1': 0.031093243634393017, 'lambda_l2': 6.636424456726728e-08}. Best is trial 56 with value: 0.2596603079319639.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  75%|#########################################2             | 15/20 [00:27<00:09,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  80%|############################################           | 16/20 [00:29<00:07,  1.87s/it]\u001b[32m[I 2022-10-28 15:15:04,074]\u001b[0m Trial 58 finished with value: 0.25966029111362765 and parameters: {'lambda_l1': 8.929563313486526, 'lambda_l2': 3.5723237911931426e-07}. Best is trial 58 with value: 0.25966029111362765.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  80%|############################################           | 16/20 [00:29<00:07,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251078\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  85%|##############################################7        | 17/20 [00:31<00:05,  1.87s/it]\u001b[32m[I 2022-10-28 15:15:05,964]\u001b[0m Trial 59 finished with value: 0.2596611595254801 and parameters: {'lambda_l1': 0.28101841733071853, 'lambda_l2': 3.3440008540318924e-07}. Best is trial 58 with value: 0.25966029111362765.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  85%|##############################################7        | 17/20 [00:31<00:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  90%|#################################################5     | 18/20 [00:33<00:03,  1.88s/it]\u001b[32m[I 2022-10-28 15:15:07,843]\u001b[0m Trial 60 finished with value: 0.2596602074697827 and parameters: {'lambda_l1': 9.763828704602075, 'lambda_l2': 1.7070416028157427e-08}. Best is trial 60 with value: 0.2596602074697827.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  90%|#################################################5     | 18/20 [00:33<00:03,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660:  95%|####################################################2  | 19/20 [00:35<00:01,  1.88s/it]\u001b[32m[I 2022-10-28 15:15:09,718]\u001b[0m Trial 61 finished with value: 0.25966114653811606 and parameters: {'lambda_l1': 0.016433920395476597, 'lambda_l2': 2.065466493712445}. Best is trial 60 with value: 0.2596602074697827.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660:  95%|####################################################2  | 19/20 [00:35<00:01,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.259660: 100%|#######################################################| 20/20 [00:37<00:00,  1.88s/it]\u001b[32m[I 2022-10-28 15:15:11,606]\u001b[0m Trial 62 finished with value: 0.2596611877736877 and parameters: {'lambda_l1': 9.00482978763336e-05, 'lambda_l2': 1.532253064280231e-08}. Best is trial 60 with value: 0.2596602074697827.\u001b[0m\n",
      "regularization_factors, val_score: 0.259660: 100%|#######################################################| 20/20 [00:37<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251076\tvalid_1's binary_logloss: 0.259661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660:   0%|                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660:  20%|############6                                                  | 1/5 [00:01<00:07,  1.86s/it]\u001b[32m[I 2022-10-28 15:15:13,470]\u001b[0m Trial 63 finished with value: 0.2596602074697827 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.2596602074697827.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.259660:  20%|############6                                                  | 1/5 [00:01<00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660:  40%|#########################2                                     | 2/5 [00:03<00:05,  1.87s/it]\u001b[32m[I 2022-10-28 15:15:15,345]\u001b[0m Trial 64 finished with value: 0.2596602074697827 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.2596602074697827.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.259660:  40%|#########################2                                     | 2/5 [00:03<00:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660:  60%|#####################################8                         | 3/5 [00:05<00:03,  1.86s/it]\u001b[32m[I 2022-10-28 15:15:17,199]\u001b[0m Trial 65 finished with value: 0.2596602074697827 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.2596602074697827.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.259660:  60%|#####################################8                         | 3/5 [00:05<00:03,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660:  80%|##################################################4            | 4/5 [00:07<00:01,  1.86s/it]\u001b[32m[I 2022-10-28 15:15:19,065]\u001b[0m Trial 66 finished with value: 0.2596602074697827 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.2596602074697827.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.259660:  80%|##################################################4            | 4/5 [00:07<00:01,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n",
      "[LightGBM] [Info] Number of positive: 30730, number of negative: 413487\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8710\n",
      "[LightGBM] [Info] Number of data points in the train set: 444217, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069178 -> initscore=-2.599387\n",
      "[LightGBM] [Info] Start training from score -2.599387\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.259660: 100%|###############################################################| 5/5 [00:09<00:00,  1.88s/it]\u001b[32m[I 2022-10-28 15:15:20,986]\u001b[0m Trial 67 finished with value: 0.2596602074697827 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.2596602074697827.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.259660: 100%|###############################################################| 5/5 [00:09<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.251079\tvalid_1's binary_logloss: 0.25966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 9.763828704602075,\n",
       " 'lambda_l2': 1.7070416028157427e-08,\n",
       " 'num_leaves': 4,\n",
       " 'feature_fraction': 0.5,\n",
       " 'bagging_fraction': 0.47628567079676276,\n",
       " 'bagging_freq': 3,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_training_train1, y_training_train1, X_training_valid1, y_training_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "5662162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.47628567079676276, subsample=1.0 will be ignored. Current value: bagging_fraction=0.47628567079676276\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.7070416028157427e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7070416028157427e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.763828704602075, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.763828704602075\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 9.763828704602075,\n",
    " 'lambda_l2': 1.7070416028157427e-08,\n",
    " 'num_leaves': 4,\n",
    " 'feature_fraction': 0.5,\n",
    " 'bagging_fraction': 0.47628567079676276,\n",
    " 'bagging_freq': 3,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "t_lgb_clf = fit(params, X_training_train1, y_training_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "eb5b27b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6952842808034848"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_me_valid = ModelEvaluator(t_lgb_clf, haitou, std=True)\n",
    "t_me_valid.score(y_training_valid1, X_training_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1376de88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>training_course</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>furlong_5.1</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rank</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furlong_5</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>furlong_5.3</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>training_course.1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>furlong_3_2</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h_num</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furlong_3</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>furlong_4_3.1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>furlong_3_2.1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>furlong_5_4.4</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rank.1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>furlong_3.1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>furlong_1.1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>furlong_5_4</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>furlong_4.1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>furlong_5_4.1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>furlong_4_3</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>furlong_3.4</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>furlong_1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>furlong_5.2</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furlong_4</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>furlong_5_4.3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>furlong_5.4</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>furlong_5_4.2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>furlong_3.3</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>furlong_4_3.4</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>furlong_4.3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>furlong_4.2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>furlong_3.2</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>furlong_2_1.1</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>furlong_2_1</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>furlong_3_2.3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>furlong_3_2.2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>furlong_1.3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>furlong_1.2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>run_place.2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>run_place</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>furlong_2_1.3</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             features  importance\n",
       "11    training_course          97\n",
       "17        furlong_5.1          80\n",
       "13               rank          80\n",
       "2           furlong_5          69\n",
       "47        furlong_5.3          63\n",
       "26  training_course.1          63\n",
       "9         furlong_3_2          63\n",
       "0               h_num          62\n",
       "4           furlong_3          62\n",
       "23      furlong_4_3.1          59\n",
       "24      furlong_3_2.1          57\n",
       "67      furlong_5_4.4          57\n",
       "28             rank.1          56\n",
       "19        furlong_3.1          56\n",
       "20        furlong_1.1          56\n",
       "7         furlong_5_4          56\n",
       "18        furlong_4.1          55\n",
       "22      furlong_5_4.1          55\n",
       "8         furlong_4_3          55\n",
       "64        furlong_3.4          52\n",
       "5           furlong_1          52\n",
       "32        furlong_5.2          51\n",
       "3           furlong_4          49\n",
       "52      furlong_5_4.3          48\n",
       "62        furlong_5.4          48\n",
       "37      furlong_5_4.2          46\n",
       "49        furlong_3.3          46\n",
       "68      furlong_4_3.4          46\n",
       "48        furlong_4.3          45\n",
       "33        furlong_4.2          45\n",
       "34        furlong_3.2          43\n",
       "25      furlong_2_1.1          42\n",
       "10        furlong_2_1          42\n",
       "54      furlong_3_2.3          41\n",
       "39      furlong_3_2.2          41\n",
       "50        furlong_1.3          41\n",
       "35        furlong_1.2          40\n",
       "45        run_place.2          40\n",
       "15          run_place          40\n",
       "55      furlong_2_1.3          39"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_me_valid.feature_importance(X_training_train1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "a7d1e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1501 レース数:9203 対象レース率:16.3% 単勝的中率:16.7% 的中数:250 賭金:150,100円 単勝配当合計:135,970円 単勝最高配当:2,720円 単勝回収率:90.6%\n"
     ]
    }
   ],
   "source": [
    "twr = t_me_valid.pred_table(X_training_valid1, 0.7, True)\n",
    "ttbt = tansho_return_rate(X_training_valid1, wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a5efa3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：2259 レース数:9203 対象レース率:24.5% 単勝的中率:15.3% 的中数:345 賭金:225,900円 単勝配当合計:190,410円 単勝最高配当:2,720円 単勝回収率:84.3%\n"
     ]
    }
   ],
   "source": [
    "X = X_training_valid1\n",
    "kaime = pd.concat([\n",
    "    bt[['race_id', 'h_num', 'win_ratio',  'time_odds', 'expected']],\n",
    "    btt[['race_id', 'h_num',  'win_ratio', 'time_odds', 'expected']]]\n",
    ").merge(haitou, on='race_id')\n",
    "\n",
    "print(\"点数：{} レース数:{} 対象レース率:{:.1%} 単勝的中率:{:.1%} 的中数:{} 賭金:{:,}円 単勝配当合計:{:,}円 単勝最高配当:{:,}円 単勝回収率:{:.1%}\". format(\n",
    "    len(kaime),\\\n",
    "    len(X.groupby('race_id')),\\\n",
    "    len(kaime.groupby('race_id')) / len(X.groupby('race_id')),\\\n",
    "    len(kaime[kaime['h_num'] == kaime['1着馬番']]) / (len(kaime)),\\\n",
    "    len(kaime[kaime['h_num'] == kaime['1着馬番']]),\\\n",
    "    len(kaime) * 100,\\\n",
    "    kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum(),\\\n",
    "    kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].max(),\\\n",
    "    (kaime[kaime['h_num'] == kaime['1着馬番']]['単勝'].sum() / (len(kaime) * 100))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "c5e3470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：1150 レース数:3944 対象レース率:27.6% 単勝的中率:6.3% 的中数:72 賭金:115,000円 単勝配当合計:129,080円 単勝最高配当:25,220円 単勝回収率:112.2%\n"
     ]
    }
   ],
   "source": [
    "t_me_test = ModelEvaluator(t_lgb_clf, haitou, std=True)\n",
    "ttwr = t_me_test.pred_table(X_training_test2, 0.7, True)\n",
    "tttbt = tansho_return_rate(X_training_test2, ttwr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07d73688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = build_data('./pickle_new/base_race_20221016.pickle', './pickle_new/peds_vec.pickle', 4, vec)\n",
    "X_fukusho_train1 = fd['X_train']\n",
    "y_fukusho_train1 = fd['y_train']\n",
    "X_fukusho_valid1 = fd['X_valid']\n",
    "y_fukusho_valid1 = fd['y_valid']\n",
    "X_fukusho_test2 = fd['X_test2']\n",
    "y_fukusho_test2 = fd['y_test2']\n",
    "X_fukusho_test1 = fd['X_test1']\n",
    "y_fukusho_test1 = fd['y_test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "10dbba4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-28 23:23:22,068]\u001b[0m A new study created in memory with name: no-name-3abd7912-385e-4449-a1ec-09a023b2adeb\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.523823:  14%|#########                                                      | 1/7 [00:28<02:48, 28.07s/it]\u001b[32m[I 2022-10-28 23:23:50,215]\u001b[0m Trial 0 finished with value: 0.5238225114837118 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.5238225114837118.\u001b[0m\n",
      "feature_fraction, val_score: 0.523823:  14%|#########                                                      | 1/7 [00:28<02:48, 28.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.500232\tvalid_1's binary_logloss: 0.523823\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.523823:  29%|##################                                             | 2/7 [00:52<02:08, 25.66s/it]\u001b[32m[I 2022-10-28 23:24:14,195]\u001b[0m Trial 1 finished with value: 0.5238225114837118 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.5238225114837118.\u001b[0m\n",
      "feature_fraction, val_score: 0.523823:  29%|##################                                             | 2/7 [00:52<02:08, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.500251\tvalid_1's binary_logloss: 0.523823\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.651549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.511271:  43%|###########################                                    | 3/7 [01:20<01:46, 26.73s/it]\u001b[32m[I 2022-10-28 23:24:42,193]\u001b[0m Trial 2 finished with value: 0.5112705189688999 and parameters: {'feature_fraction': 0.7}. Best is trial 2 with value: 0.5112705189688999.\u001b[0m\n",
      "feature_fraction, val_score: 0.511271:  43%|###########################                                    | 3/7 [01:20<01:46, 26.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493925\tvalid_1's binary_logloss: 0.511271\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.623941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.511271:  57%|####################################                           | 4/7 [01:51<01:25, 28.59s/it]\u001b[32m[I 2022-10-28 23:25:13,624]\u001b[0m Trial 3 finished with value: 0.5119518508661322 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 2 with value: 0.5112705189688999.\u001b[0m\n",
      "feature_fraction, val_score: 0.511271:  57%|####################################                           | 4/7 [01:51<01:25, 28.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493892\tvalid_1's binary_logloss: 0.511952\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.511271:  71%|#############################################                  | 5/7 [02:16<00:54, 27.46s/it]\u001b[32m[I 2022-10-28 23:25:39,106]\u001b[0m Trial 4 finished with value: 0.5182203547819659 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.5112705189688999.\u001b[0m\n",
      "feature_fraction, val_score: 0.511271:  71%|#############################################                  | 5/7 [02:17<00:54, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.490054\tvalid_1's binary_logloss: 0.51822\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.555067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.511271:  86%|######################################################         | 6/7 [02:43<00:27, 27.22s/it]\u001b[32m[I 2022-10-28 23:26:05,848]\u001b[0m Trial 5 finished with value: 0.5119518508661322 and parameters: {'feature_fraction': 0.8}. Best is trial 2 with value: 0.5112705189688999.\u001b[0m\n",
      "feature_fraction, val_score: 0.511271:  86%|######################################################         | 6/7 [02:43<00:27, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493892\tvalid_1's binary_logloss: 0.511952\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.483473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.511271: 100%|###############################################################| 7/7 [03:10<00:00, 27.07s/it]\u001b[32m[I 2022-10-28 23:26:32,592]\u001b[0m Trial 6 finished with value: 0.52753633230888 and parameters: {'feature_fraction': 1.0}. Best is trial 2 with value: 0.5112705189688999.\u001b[0m\n",
      "feature_fraction, val_score: 0.511271: 100%|###############################################################| 7/7 [03:10<00:00, 27.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493231\tvalid_1's binary_logloss: 0.527536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511271:   0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.446390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511271:   5%|###4                                                                | 1/20 [00:28<09:01, 28.52s/it]\u001b[32m[I 2022-10-28 23:27:01,168]\u001b[0m Trial 7 finished with value: 0.5115059733226327 and parameters: {'num_leaves': 143}. Best is trial 7 with value: 0.5115059733226327.\u001b[0m\n",
      "num_leaves, val_score: 0.511271:   5%|###4                                                                | 1/20 [00:28<09:01, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493125\tvalid_1's binary_logloss: 0.511506\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.447329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  10%|######8                                                             | 2/20 [00:57<08:38, 28.79s/it]\u001b[32m[I 2022-10-28 23:27:30,143]\u001b[0m Trial 8 finished with value: 0.511221991279834 and parameters: {'num_leaves': 179}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  10%|######8                                                             | 2/20 [00:57<08:38, 28.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492925\tvalid_1's binary_logloss: 0.511222\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.538660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  15%|##########2                                                         | 3/20 [01:27<08:16, 29.21s/it]\u001b[32m[I 2022-10-28 23:27:59,871]\u001b[0m Trial 9 finished with value: 0.5113154643611689 and parameters: {'num_leaves': 256}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  15%|##########2                                                         | 3/20 [01:27<08:16, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492529\tvalid_1's binary_logloss: 0.511315\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.529110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  20%|#############6                                                      | 4/20 [01:55<07:39, 28.69s/it]\u001b[32m[I 2022-10-28 23:28:27,768]\u001b[0m Trial 10 finished with value: 0.5119541075406588 and parameters: {'num_leaves': 102}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  20%|#############6                                                      | 4/20 [01:55<07:39, 28.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493358\tvalid_1's binary_logloss: 0.511954\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.495992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  25%|#################                                                   | 5/20 [02:23<07:09, 28.61s/it]\u001b[32m[I 2022-10-28 23:28:56,243]\u001b[0m Trial 11 finished with value: 0.5115588346245623 and parameters: {'num_leaves': 145}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  25%|#################                                                   | 5/20 [02:23<07:09, 28.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493114\tvalid_1's binary_logloss: 0.511559\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.547987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  30%|####################4                                               | 6/20 [02:53<06:46, 29.02s/it]\u001b[32m[I 2022-10-28 23:29:26,061]\u001b[0m Trial 12 finished with value: 0.511802425515807 and parameters: {'num_leaves': 113}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  30%|####################4                                               | 6/20 [02:53<06:46, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493294\tvalid_1's binary_logloss: 0.511802\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.517489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  35%|#######################7                                            | 7/20 [03:21<06:12, 28.67s/it]\u001b[32m[I 2022-10-28 23:29:53,992]\u001b[0m Trial 13 finished with value: 0.511980861338958 and parameters: {'num_leaves': 87}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  35%|#######################7                                            | 7/20 [03:21<06:12, 28.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493453\tvalid_1's binary_logloss: 0.511981\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.423055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  40%|###########################2                                        | 8/20 [03:49<05:43, 28.63s/it]\u001b[32m[I 2022-10-28 23:30:22,543]\u001b[0m Trial 14 finished with value: 0.511607136333371 and parameters: {'num_leaves': 124}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  40%|###########################2                                        | 8/20 [03:49<05:43, 28.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493231\tvalid_1's binary_logloss: 0.511607\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.539831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  45%|##############################6                                     | 9/20 [04:17<05:11, 28.35s/it]\u001b[32m[I 2022-10-28 23:30:50,274]\u001b[0m Trial 15 finished with value: 0.5117675333010921 and parameters: {'num_leaves': 96}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  45%|##############################6                                     | 9/20 [04:17<05:11, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493395\tvalid_1's binary_logloss: 0.511768\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.444823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  50%|#################################5                                 | 10/20 [04:47<04:48, 28.84s/it]\u001b[32m[I 2022-10-28 23:31:20,224]\u001b[0m Trial 16 finished with value: 0.5113154643611689 and parameters: {'num_leaves': 246}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  50%|#################################5                                 | 10/20 [04:47<04:48, 28.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492579\tvalid_1's binary_logloss: 0.511315\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.517408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  55%|####################################8                              | 11/20 [05:11<04:06, 27.34s/it]\u001b[32m[I 2022-10-28 23:31:44,131]\u001b[0m Trial 17 finished with value: 0.5140079514259118 and parameters: {'num_leaves': 5}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  55%|####################################8                              | 11/20 [05:11<04:06, 27.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.495084\tvalid_1's binary_logloss: 0.514008\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.513569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  60%|########################################1                          | 12/20 [05:41<03:45, 28.14s/it]\u001b[32m[I 2022-10-28 23:32:14,125]\u001b[0m Trial 18 finished with value: 0.5113154643611689 and parameters: {'num_leaves': 252}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  60%|########################################1                          | 12/20 [05:41<03:45, 28.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.49255\tvalid_1's binary_logloss: 0.511315\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.516446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  65%|###########################################5                       | 13/20 [06:10<03:19, 28.49s/it]\u001b[32m[I 2022-10-28 23:32:43,421]\u001b[0m Trial 19 finished with value: 0.511348961557752 and parameters: {'num_leaves': 201}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  65%|###########################################5                       | 13/20 [06:10<03:19, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492811\tvalid_1's binary_logloss: 0.511349\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.578385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  70%|##############################################9                    | 14/20 [06:40<02:52, 28.72s/it]\u001b[32m[I 2022-10-28 23:33:12,655]\u001b[0m Trial 20 finished with value: 0.511348961557752 and parameters: {'num_leaves': 199}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  70%|##############################################9                    | 14/20 [06:40<02:52, 28.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492821\tvalid_1's binary_logloss: 0.511349\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.496571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511222:  75%|##################################################2                | 15/20 [07:10<02:26, 29.28s/it]\u001b[32m[I 2022-10-28 23:33:43,249]\u001b[0m Trial 21 finished with value: 0.511348961557752 and parameters: {'num_leaves': 199}. Best is trial 8 with value: 0.511221991279834.\u001b[0m\n",
      "num_leaves, val_score: 0.511222:  75%|##################################################2                | 15/20 [07:10<02:26, 29.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492821\tvalid_1's binary_logloss: 0.511349\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.442225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511123:  80%|#####################################################6             | 16/20 [07:39<01:56, 29.20s/it]\u001b[32m[I 2022-10-28 23:34:12,259]\u001b[0m Trial 22 finished with value: 0.5111227917327655 and parameters: {'num_leaves': 171}. Best is trial 22 with value: 0.5111227917327655.\u001b[0m\n",
      "num_leaves, val_score: 0.511123:  80%|#####################################################6             | 16/20 [07:39<01:56, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492968\tvalid_1's binary_logloss: 0.511123\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.426152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511123:  85%|########################################################9          | 17/20 [08:08<01:27, 29.05s/it]\u001b[32m[I 2022-10-28 23:34:40,979]\u001b[0m Trial 23 finished with value: 0.511470438629425 and parameters: {'num_leaves': 168}. Best is trial 22 with value: 0.5111227917327655.\u001b[0m\n",
      "num_leaves, val_score: 0.511123:  85%|########################################################9          | 17/20 [08:08<01:27, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492984\tvalid_1's binary_logloss: 0.51147\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.505631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511123:  90%|############################################################3      | 18/20 [08:36<00:57, 28.77s/it]\u001b[32m[I 2022-10-28 23:35:09,109]\u001b[0m Trial 24 finished with value: 0.5111227917327655 and parameters: {'num_leaves': 175}. Best is trial 22 with value: 0.5111227917327655.\u001b[0m\n",
      "num_leaves, val_score: 0.511123:  90%|############################################################3      | 18/20 [08:36<00:57, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492946\tvalid_1's binary_logloss: 0.511123\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.440918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511123:  95%|###############################################################6   | 19/20 [09:01<00:27, 27.74s/it]\u001b[32m[I 2022-10-28 23:35:34,442]\u001b[0m Trial 25 finished with value: 0.5117238664744079 and parameters: {'num_leaves': 36}. Best is trial 22 with value: 0.5111227917327655.\u001b[0m\n",
      "num_leaves, val_score: 0.511123:  95%|###############################################################6   | 19/20 [09:01<00:27, 27.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493863\tvalid_1's binary_logloss: 0.511724\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.425597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.511123: 100%|###################################################################| 20/20 [09:28<00:00, 27.40s/it]\u001b[32m[I 2022-10-28 23:36:01,032]\u001b[0m Trial 26 finished with value: 0.5119717339431383 and parameters: {'num_leaves': 70}. Best is trial 22 with value: 0.5111227917327655.\u001b[0m\n",
      "num_leaves, val_score: 0.511123: 100%|###################################################################| 20/20 [09:28<00:00, 28.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493568\tvalid_1's binary_logloss: 0.511972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.511123:   0%|                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.458832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.511123:  10%|#######1                                                               | 1/10 [00:25<03:49, 25.50s/it]\u001b[32m[I 2022-10-28 23:36:26,588]\u001b[0m Trial 27 finished with value: 0.5130705954444258 and parameters: {'bagging_fraction': 0.5299153748533555, 'bagging_freq': 7}. Best is trial 27 with value: 0.5130705954444258.\u001b[0m\n",
      "bagging, val_score: 0.511123:  10%|#######1                                                               | 1/10 [00:25<03:49, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493125\tvalid_1's binary_logloss: 0.513071\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.452162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.511123:  20%|##############2                                                        | 2/10 [00:52<03:30, 26.35s/it]\u001b[32m[I 2022-10-28 23:36:53,543]\u001b[0m Trial 28 finished with value: 0.5143665504301745 and parameters: {'bagging_fraction': 0.7315141852409024, 'bagging_freq': 2}. Best is trial 27 with value: 0.5130705954444258.\u001b[0m\n",
      "bagging, val_score: 0.511123:  20%|##############2                                                        | 2/10 [00:52<03:30, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493021\tvalid_1's binary_logloss: 0.514367\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.434397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.511123:  30%|#####################3                                                 | 3/10 [01:21<03:12, 27.49s/it]\u001b[32m[I 2022-10-28 23:37:22,409]\u001b[0m Trial 29 finished with value: 0.5133364448135371 and parameters: {'bagging_fraction': 0.9642497429346645, 'bagging_freq': 2}. Best is trial 27 with value: 0.5130705954444258.\u001b[0m\n",
      "bagging, val_score: 0.511123:  30%|#####################3                                                 | 3/10 [01:21<03:12, 27.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492991\tvalid_1's binary_logloss: 0.513336\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.476547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.511123:  40%|############################4                                          | 4/10 [01:54<02:58, 29.72s/it]\u001b[32m[I 2022-10-28 23:37:55,561]\u001b[0m Trial 30 finished with value: 0.5141346932751693 and parameters: {'bagging_fraction': 0.7834845022473869, 'bagging_freq': 2}. Best is trial 27 with value: 0.5130705954444258.\u001b[0m\n",
      "bagging, val_score: 0.511123:  40%|############################4                                          | 4/10 [01:54<02:58, 29.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493011\tvalid_1's binary_logloss: 0.514135\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.528587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510809:  50%|###################################5                                   | 5/10 [02:21<02:23, 28.77s/it]\u001b[32m[I 2022-10-28 23:38:22,625]\u001b[0m Trial 31 finished with value: 0.5108086953056782 and parameters: {'bagging_fraction': 0.5422981201165818, 'bagging_freq': 7}. Best is trial 31 with value: 0.5108086953056782.\u001b[0m\n",
      "bagging, val_score: 0.510809:  50%|###################################5                                   | 5/10 [02:21<02:23, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.49312\tvalid_1's binary_logloss: 0.510809\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.528730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510809:  60%|##########################################6                            | 6/10 [02:47<01:50, 27.70s/it]\u001b[32m[I 2022-10-28 23:38:48,225]\u001b[0m Trial 32 finished with value: 0.5158231638706077 and parameters: {'bagging_fraction': 0.46011720870995226, 'bagging_freq': 4}. Best is trial 31 with value: 0.5108086953056782.\u001b[0m\n",
      "bagging, val_score: 0.510809:  60%|##########################################6                            | 6/10 [02:47<01:50, 27.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493116\tvalid_1's binary_logloss: 0.515823\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.579229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510809:  70%|#################################################6                     | 7/10 [03:15<01:24, 28.06s/it]\u001b[32m[I 2022-10-28 23:39:17,036]\u001b[0m Trial 33 finished with value: 0.5129695107216653 and parameters: {'bagging_fraction': 0.8367420248934543, 'bagging_freq': 3}. Best is trial 31 with value: 0.5108086953056782.\u001b[0m\n",
      "bagging, val_score: 0.510809:  70%|#################################################6                     | 7/10 [03:15<01:24, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.492974\tvalid_1's binary_logloss: 0.51297\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.789547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510809:  80%|########################################################8              | 8/10 [03:46<00:57, 28.79s/it]\u001b[32m[I 2022-10-28 23:39:47,383]\u001b[0m Trial 34 finished with value: 0.513263211028808 and parameters: {'bagging_fraction': 0.4433967228156532, 'bagging_freq': 5}. Best is trial 31 with value: 0.5108086953056782.\u001b[0m\n",
      "bagging, val_score: 0.510809:  80%|########################################################8              | 8/10 [03:46<00:57, 28.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493076\tvalid_1's binary_logloss: 0.513263\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.640477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510485:  90%|###############################################################9       | 9/10 [04:16<00:29, 29.36s/it]\u001b[32m[I 2022-10-28 23:40:18,011]\u001b[0m Trial 35 finished with value: 0.5104852113807868 and parameters: {'bagging_fraction': 0.4806708139231316, 'bagging_freq': 2}. Best is trial 35 with value: 0.5104852113807868.\u001b[0m\n",
      "bagging, val_score: 0.510485:  90%|###############################################################9       | 9/10 [04:16<00:29, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.481129\tvalid_1's binary_logloss: 0.510485\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.516656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.510485: 100%|######################################################################| 10/10 [04:45<00:00, 29.19s/it]\u001b[32m[I 2022-10-28 23:40:46,807]\u001b[0m Trial 36 finished with value: 0.5110279222040512 and parameters: {'bagging_fraction': 0.6063471306610588, 'bagging_freq': 6}. Best is trial 35 with value: 0.5104852113807868.\u001b[0m\n",
      "bagging, val_score: 0.510485: 100%|######################################################################| 10/10 [04:45<00:00, 28.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.481096\tvalid_1's binary_logloss: 0.511028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:   0%|                                                                | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.652547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:  17%|#########3                                              | 1/6 [00:31<02:39, 31.80s/it]\u001b[32m[I 2022-10-28 23:41:18,699]\u001b[0m Trial 37 finished with value: 0.5116743553442905 and parameters: {'feature_fraction': 0.7799999999999999}. Best is trial 37 with value: 0.5116743553442905.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.510485:  17%|#########3                                              | 1/6 [00:31<02:39, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493111\tvalid_1's binary_logloss: 0.511674\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.628590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:  33%|##################6                                     | 2/6 [01:02<02:04, 31.24s/it]\u001b[32m[I 2022-10-28 23:41:49,514]\u001b[0m Trial 38 finished with value: 0.5104903591358715 and parameters: {'feature_fraction': 0.6839999999999999}. Best is trial 38 with value: 0.5104903591358715.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.510485:  33%|##################6                                     | 2/6 [01:02<02:04, 31.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.481136\tvalid_1's binary_logloss: 0.51049\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.530157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:  50%|############################                            | 3/6 [01:29<01:27, 29.26s/it]\u001b[32m[I 2022-10-28 23:42:16,421]\u001b[0m Trial 39 finished with value: 0.511955409247174 and parameters: {'feature_fraction': 0.748}. Best is trial 38 with value: 0.5104903591358715.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.510485:  50%|############################                            | 3/6 [01:29<01:27, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493104\tvalid_1's binary_logloss: 0.511955\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.368205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:  67%|#####################################3                  | 4/6 [01:55<00:55, 27.93s/it]\u001b[32m[I 2022-10-28 23:42:42,293]\u001b[0m Trial 40 finished with value: 0.5112773248945922 and parameters: {'feature_fraction': 0.652}. Best is trial 38 with value: 0.5104903591358715.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.510485:  67%|#####################################3                  | 4/6 [01:55<00:55, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.481171\tvalid_1's binary_logloss: 0.511277\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.533359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.510485:  83%|##############################################6         | 5/6 [02:22<00:27, 27.53s/it]\u001b[32m[I 2022-10-28 23:43:09,119]\u001b[0m Trial 41 finished with value: 0.5134660253368201 and parameters: {'feature_fraction': 0.62}. Best is trial 38 with value: 0.5104903591358715.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.510485:  83%|##############################################6         | 5/6 [02:22<00:27, 27.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.493113\tvalid_1's binary_logloss: 0.513466\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.540368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.504555: 100%|########################################################| 6/6 [02:48<00:00, 27.12s/it]\u001b[32m[I 2022-10-28 23:43:35,469]\u001b[0m Trial 42 finished with value: 0.5045545679047074 and parameters: {'feature_fraction': 0.716}. Best is trial 42 with value: 0.5045545679047074.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.504555: 100%|########################################################| 6/6 [02:48<00:00, 28.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477972\tvalid_1's binary_logloss: 0.504555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.504555:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.484406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.504555:   5%|##8                                                     | 1/20 [00:26<08:22, 26.44s/it]\u001b[32m[I 2022-10-28 23:44:01,977]\u001b[0m Trial 43 finished with value: 0.5063310907214502 and parameters: {'lambda_l1': 2.3016992767388835, 'lambda_l2': 1.082124680820066e-06}. Best is trial 43 with value: 0.5063310907214502.\u001b[0m\n",
      "regularization_factors, val_score: 0.504555:   5%|##8                                                     | 1/20 [00:26<08:22, 26.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478207\tvalid_1's binary_logloss: 0.506331\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.510093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.504554:  10%|#####6                                                  | 2/20 [00:53<08:02, 26.81s/it]\u001b[32m[I 2022-10-28 23:44:29,039]\u001b[0m Trial 44 finished with value: 0.5045544775563066 and parameters: {'lambda_l1': 0.00019017268542006113, 'lambda_l2': 0.002407826287319413}. Best is trial 44 with value: 0.5045544775563066.\u001b[0m\n",
      "regularization_factors, val_score: 0.504554:  10%|#####6                                                  | 2/20 [00:53<08:02, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477974\tvalid_1's binary_logloss: 0.504554\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.479980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.504554:  15%|########4                                               | 3/20 [01:20<07:40, 27.07s/it]\u001b[32m[I 2022-10-28 23:44:56,425]\u001b[0m Trial 45 finished with value: 0.504554567805686 and parameters: {'lambda_l1': 0.00013076121124661893, 'lambda_l2': 6.5829378115846765e-06}. Best is trial 44 with value: 0.5045544775563066.\u001b[0m\n",
      "regularization_factors, val_score: 0.504554:  15%|########4                                               | 3/20 [01:20<07:40, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477972\tvalid_1's binary_logloss: 0.504555\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.521336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.503808:  20%|###########2                                            | 4/20 [01:48<07:13, 27.10s/it]\u001b[32m[I 2022-10-28 23:45:23,560]\u001b[0m Trial 46 finished with value: 0.5038084348229795 and parameters: {'lambda_l1': 0.4277443493967021, 'lambda_l2': 2.988937832365509e-05}. Best is trial 46 with value: 0.5038084348229795.\u001b[0m\n",
      "regularization_factors, val_score: 0.503808:  20%|###########2                                            | 4/20 [01:48<07:13, 27.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478022\tvalid_1's binary_logloss: 0.503808\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.523467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.503808:  25%|##############                                          | 5/20 [02:14<06:44, 26.96s/it]\u001b[32m[I 2022-10-28 23:45:50,304]\u001b[0m Trial 47 finished with value: 0.5045545681899525 and parameters: {'lambda_l1': 1.4436270666518998e-07, 'lambda_l2': 1.5770085258801928e-06}. Best is trial 46 with value: 0.5038084348229795.\u001b[0m\n",
      "regularization_factors, val_score: 0.503808:  25%|##############                                          | 5/20 [02:14<06:44, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477972\tvalid_1's binary_logloss: 0.504555\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.495392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  30%|################8                                       | 6/20 [02:42<06:23, 27.36s/it]\u001b[32m[I 2022-10-28 23:46:18,454]\u001b[0m Trial 48 finished with value: 0.5022085876343142 and parameters: {'lambda_l1': 1.4367016376486885, 'lambda_l2': 1.1166927044455953e-08}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  30%|################8                                       | 6/20 [02:42<06:23, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478075\tvalid_1's binary_logloss: 0.502209\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.463571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  35%|###################5                                    | 7/20 [03:12<06:06, 28.16s/it]\u001b[32m[I 2022-10-28 23:46:48,240]\u001b[0m Trial 49 finished with value: 0.5045595368259043 and parameters: {'lambda_l1': 2.361877288909538e-08, 'lambda_l2': 0.02753701464198671}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  35%|###################5                                    | 7/20 [03:12<06:06, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477985\tvalid_1's binary_logloss: 0.50456\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.722084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  40%|######################4                                 | 8/20 [03:41<05:39, 28.30s/it]\u001b[32m[I 2022-10-28 23:47:16,834]\u001b[0m Trial 50 finished with value: 0.5045297180838747 and parameters: {'lambda_l1': 2.762811376610515e-06, 'lambda_l2': 0.17254946646300304}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  40%|######################4                                 | 8/20 [03:41<05:39, 28.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478005\tvalid_1's binary_logloss: 0.50453\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.673164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  45%|#########################2                              | 9/20 [04:10<05:14, 28.57s/it]\u001b[32m[I 2022-10-28 23:47:45,999]\u001b[0m Trial 51 finished with value: 0.5045545978155276 and parameters: {'lambda_l1': 5.542592122467474e-05, 'lambda_l2': 0.00017028882027322388}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  45%|#########################2                              | 9/20 [04:10<05:14, 28.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477972\tvalid_1's binary_logloss: 0.504555\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.420020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  50%|###########################5                           | 10/20 [04:36<04:38, 27.88s/it]\u001b[32m[I 2022-10-28 23:48:12,319]\u001b[0m Trial 52 finished with value: 0.5042385464284922 and parameters: {'lambda_l1': 0.010682971776954183, 'lambda_l2': 0.1008668993840297}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  50%|###########################5                           | 10/20 [04:36<04:38, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477995\tvalid_1's binary_logloss: 0.504239\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.724480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  55%|##############################2                        | 11/20 [05:04<04:11, 27.92s/it]\u001b[32m[I 2022-10-28 23:48:40,329]\u001b[0m Trial 53 finished with value: 0.5038075840662968 and parameters: {'lambda_l1': 0.02379662702392217, 'lambda_l2': 1.1900116168508248e-08}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  55%|##############################2                        | 11/20 [05:04<04:11, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477979\tvalid_1's binary_logloss: 0.503808\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.495756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  60%|#################################                      | 12/20 [05:31<03:40, 27.53s/it]\u001b[32m[I 2022-10-28 23:49:06,973]\u001b[0m Trial 54 finished with value: 0.5038083378976579 and parameters: {'lambda_l1': 0.03444219809100315, 'lambda_l2': 1.1347468580386712e-08}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  60%|#################################                      | 12/20 [05:31<03:40, 27.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477981\tvalid_1's binary_logloss: 0.503808\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.606438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  65%|###################################7                   | 13/20 [05:57<03:09, 27.06s/it]\u001b[32m[I 2022-10-28 23:49:32,941]\u001b[0m Trial 55 finished with value: 0.5045539461586458 and parameters: {'lambda_l1': 0.01114323167899222, 'lambda_l2': 1.0829545118523066e-08}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  65%|###################################7                   | 13/20 [05:57<03:09, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477975\tvalid_1's binary_logloss: 0.504554\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.504453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  70%|######################################5                | 14/20 [06:22<02:38, 26.39s/it]\u001b[32m[I 2022-10-28 23:49:57,788]\u001b[0m Trial 56 finished with value: 0.5090575989299192 and parameters: {'lambda_l1': 5.877183756339948, 'lambda_l2': 1.3607500807528382e-07}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  70%|######################################5                | 14/20 [06:22<02:38, 26.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478618\tvalid_1's binary_logloss: 0.509058\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.641067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  75%|#########################################2             | 15/20 [06:49<02:12, 26.52s/it]\u001b[32m[I 2022-10-28 23:50:24,595]\u001b[0m Trial 57 finished with value: 0.5042233435152839 and parameters: {'lambda_l1': 0.13001362836639452, 'lambda_l2': 1.1875605832418074e-07}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  75%|#########################################2             | 15/20 [06:49<02:12, 26.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477987\tvalid_1's binary_logloss: 0.504223\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.527919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  80%|############################################           | 16/20 [07:15<01:46, 26.52s/it]\u001b[32m[I 2022-10-28 23:50:51,140]\u001b[0m Trial 58 finished with value: 0.5032694867330517 and parameters: {'lambda_l1': 0.0022024992846235458, 'lambda_l2': 1.5187677103908566}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  80%|############################################           | 16/20 [07:15<01:46, 26.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478167\tvalid_1's binary_logloss: 0.503269\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.564393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  85%|##############################################7        | 17/20 [07:42<01:19, 26.53s/it]\u001b[32m[I 2022-10-28 23:51:17,694]\u001b[0m Trial 59 finished with value: 0.5052730363976446 and parameters: {'lambda_l1': 0.0033038215858742226, 'lambda_l2': 3.214811242064448}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  85%|##############################################7        | 17/20 [07:42<01:19, 26.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478321\tvalid_1's binary_logloss: 0.505273\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.649881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  90%|#################################################5     | 18/20 [08:08<00:53, 26.58s/it]\u001b[32m[I 2022-10-28 23:51:44,399]\u001b[0m Trial 60 finished with value: 0.5045543176220164 and parameters: {'lambda_l1': 7.542656767674158e-06, 'lambda_l2': 0.0015040806608550752}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  90%|#################################################5     | 18/20 [08:08<00:53, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.477974\tvalid_1's binary_logloss: 0.504554\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.629393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209:  95%|####################################################2  | 19/20 [08:35<00:26, 26.69s/it]\u001b[32m[I 2022-10-28 23:52:11,347]\u001b[0m Trial 61 finished with value: 0.5087857041837941 and parameters: {'lambda_l1': 0.0011279232620322874, 'lambda_l2': 7.645683765754049}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209:  95%|####################################################2  | 19/20 [08:35<00:26, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478646\tvalid_1's binary_logloss: 0.508786\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.498081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502209: 100%|#######################################################| 20/20 [09:03<00:00, 26.95s/it]\u001b[32m[I 2022-10-28 23:52:38,906]\u001b[0m Trial 62 finished with value: 0.5049753285594438 and parameters: {'lambda_l1': 0.47661127300607975, 'lambda_l2': 0.010552113012799142}. Best is trial 48 with value: 0.5022085876343142.\u001b[0m\n",
      "regularization_factors, val_score: 0.502209: 100%|#######################################################| 20/20 [09:03<00:00, 27.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478027\tvalid_1's binary_logloss: 0.504975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502209:   0%|                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.500561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502209:  20%|############6                                                  | 1/5 [00:26<01:47, 26.98s/it]\u001b[32m[I 2022-10-28 23:53:05,949]\u001b[0m Trial 63 finished with value: 0.5024355146192956 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.5024355146192956.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502209:  20%|############6                                                  | 1/5 [00:27<01:47, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478058\tvalid_1's binary_logloss: 0.502436\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.617053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502209:  40%|#########################2                                     | 2/5 [00:53<01:20, 26.69s/it]\u001b[32m[I 2022-10-28 23:53:32,430]\u001b[0m Trial 64 finished with value: 0.5034280710835131 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.5024355146192956.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502209:  40%|#########################2                                     | 2/5 [00:53<01:20, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478088\tvalid_1's binary_logloss: 0.503428\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.463251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502209:  60%|#####################################8                         | 3/5 [01:20<00:53, 26.97s/it]\u001b[32m[I 2022-10-28 23:53:59,738]\u001b[0m Trial 65 finished with value: 0.5024508905779983 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.5024355146192956.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502209:  60%|#####################################8                         | 3/5 [01:20<00:53, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478056\tvalid_1's binary_logloss: 0.502451\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.557050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502196:  80%|##################################################4            | 4/5 [01:47<00:26, 26.84s/it]\u001b[32m[I 2022-10-28 23:54:26,369]\u001b[0m Trial 66 finished with value: 0.5021958455748818 and parameters: {'min_child_samples': 25}. Best is trial 66 with value: 0.5021958455748818.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502196:  80%|##################################################4            | 4/5 [01:47<00:26, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.478083\tvalid_1's binary_logloss: 0.502196\n",
      "[LightGBM] [Info] Number of positive: 91640, number of negative: 346396\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.449600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 107323\n",
      "[LightGBM] [Info] Number of data points in the train set: 438036, number of used features: 760\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209207 -> initscore=-1.329715\n",
      "[LightGBM] [Info] Start training from score -1.329715\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502196: 100%|###############################################################| 5/5 [02:13<00:00, 26.65s/it]\u001b[32m[I 2022-10-28 23:54:52,687]\u001b[0m Trial 67 finished with value: 0.5052315367365499 and parameters: {'min_child_samples': 100}. Best is trial 66 with value: 0.5021958455748818.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502196: 100%|###############################################################| 5/5 [02:13<00:00, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.47811\tvalid_1's binary_logloss: 0.505232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 1.4367016376486885,\n",
       " 'lambda_l2': 1.1166927044455953e-08,\n",
       " 'num_leaves': 171,\n",
       " 'feature_fraction': 0.716,\n",
       " 'bagging_fraction': 0.4806708139231316,\n",
       " 'bagging_freq': 2,\n",
       " 'min_child_samples': 25,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_params(X_fukusho_train1, y_fukusho_train1, X_fukusho_valid1, y_fukusho_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae79cfdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "'objective': 'binary',\n",
    "'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 1.4367016376486885,\n",
    " 'lambda_l2': 1.1166927044455953e-08,\n",
    " 'num_leaves': 171,\n",
    " 'feature_fraction': 0.716,\n",
    " 'bagging_fraction': 0.4806708139231316,\n",
    " 'bagging_freq': 2,\n",
    " 'min_child_samples': 25,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "f_lgb_clf = fit(params, X_fukusho_train1, y_fukusho_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "32d6cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7689010129091167"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_me_valid = ModelEvaluator(f_lgb_clf, haitou, std=True)\n",
    "f_me_valid.score(y_fukusho_valid1, X_fukusho_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a729c302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：695 的中数:50 的中率:7.2%\n",
      "馬連賭金:69,500円 馬連配当合計:67,960円 馬連最高配当:6,410円 馬連回収率:97.8%\n",
      "馬単賭金:139,000円 馬単配当合計:143,010円 馬単最高配当:18,650円 馬単回収率:102.9%\n"
     ]
    }
   ],
   "source": [
    "fm = f_me_valid.pred_table(X_fukusho_valid1, 0.7)\n",
    "fmr = bt[['race_id', 'place_id', 'h_num']].merge(fm, on='race_id')\n",
    "fmr = fmr[fmr['h_num_x'] != fmr['h_num_y']]\n",
    "uma_haito = umaren_return_rate(fmr, haitou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "286b1c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：379 的中数:28 的中率:7.4%\n",
      "馬連賭金:37,900円 馬連配当合計:37,470円 馬連最高配当:3,430円 馬連回収率:98.9%\n",
      "馬単賭金:75,800円 馬単配当合計:70,890円 馬単最高配当:5,480円 馬単回収率:93.5%\n"
     ]
    }
   ],
   "source": [
    "f_me_test = ModelEvaluator(f_lgb_clf, haitou, std=True)\n",
    "fmt = f_me_test.pred_table(X_fukusho_test2, 0.7)\n",
    "fmrt = btt[['race_id', 'place_id', 'h_num']].merge(fmt, on='race_id')\n",
    "fmrt = fmrt[fmrt['h_num_x'] != fmrt['h_num_y']]\n",
    "t_uma_haito = umaren_return_rate(fmrt, haitou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "5ae853d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ワイド点数：695 ワイド賭金:69,500円 ワイド配当合計:64,410円 ワイド的中数:126 ワイド的中率:18.1% ワイド回収率:92.7%\n"
     ]
    }
   ],
   "source": [
    "wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "w = uma_haito[['race_id', 'h_num_x', 'h_num_y']].merge(wide, on='race_id')\n",
    "\n",
    "wide = 0\n",
    "wide_tekichu = 0\n",
    "\n",
    "for i in range(1, 8):\n",
    "    sm = str(i)\n",
    "    w_b3 = w[\n",
    "        (\n",
    "            ((w['h_num_x'] == w['wide' + sm + '_uma1']) | (w['h_num_x'] == w['wide' + sm + '_uma2']))\n",
    "            &\n",
    "            ((w['h_num_y'] == w['wide' + sm + '_uma1']) | (w['h_num_y'] == w['wide' + sm + '_uma2']))\n",
    "        )\n",
    "    ]\n",
    "    wide = wide + w_b3['wide_' + sm].sum()\n",
    "    wide_tekichu = wide_tekichu + len(w_b3)\n",
    "\n",
    "print(\"ワイド点数：{} ワイド賭金:{:,}円 ワイド配当合計:{:,}円 ワイド的中数:{} ワイド的中率:{:.1%} ワイド回収率:{:.1%}\". format(\n",
    "    len(w),\\\n",
    "    len(w) * 100,\\\n",
    "    int(wide),\\\n",
    "    wide_tekichu,\\\n",
    "    wide_tekichu / len(w),\\\n",
    "    (wide / (len(w) * 100))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "452b70db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：678 的中数:18 的中率:2.7%\n",
      "3連複賭金:67,800円 3連複配当合計:38,220円 3連複最高配当:10,140円 3連複回収率:56.4%\n",
      "3連単賭金:406,800円 3連単配当合計:245,300円 3連単最高配当:68,720円 3連単回収率:60.3%\n",
      "ワイド点数：1815 ワイド賭金:181,500円 ワイド配当合計:154,230円 ワイド的中数:240 ワイド的中率:13.2% ワイド回収率:85.0%\n"
     ]
    }
   ],
   "source": [
    "fm3 = f_me_valid.pred_table(X_fukusho_valid1, 0.5)\n",
    "uma3 = uma_haito.merge(fm3[['h_num']], on='race_id')\n",
    "uma3 = uma3[\n",
    "    (\n",
    "        (uma3['h_num'] != uma3['h_num_y'])\n",
    "        &\n",
    "        (uma3['h_num'] != uma3['h_num_x'])\n",
    "    )\n",
    "]\n",
    "sanrenkei(uma3[['race_id', 'h_num_x', 'h_num_y', 'h_num']], haitou)\n",
    "wide_rate(uma3[['race_id', 'h_num_x', 'h_num_y', 'h_num']], haitou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a2e975b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "点数：350 的中数:9 的中率:2.6%\n",
      "3連複賭金:35,000円 3連複配当合計:24,440円 3連複最高配当:6,000円 3連複回収率:69.8%\n",
      "3連単賭金:210,000円 3連単配当合計:147,890円 3連単最高配当:40,970円 3連単回収率:70.4%\n",
      "ワイド点数：946 ワイド賭金:94,600円 ワイド配当合計:87,430円 ワイド的中数:125 ワイド的中率:13.2% ワイド回収率:92.4%\n"
     ]
    }
   ],
   "source": [
    "fmt3 = f_me_test.pred_table(X_fukusho_test2, 0.5)\n",
    "t_uma3 = t_uma_haito.merge(fmt3[['h_num']], on='race_id')\n",
    "t_uma3 = t_uma3[\n",
    "    (\n",
    "        (t_uma3['h_num'] != t_uma3['h_num_y'])\n",
    "        &\n",
    "        (t_uma3['h_num'] != t_uma3['h_num_x'])\n",
    "    )\n",
    "]\n",
    "sanrenkei(t_uma3[['race_id', 'h_num_x', 'h_num_y', 'h_num']], haitou)\n",
    "wide_rate(t_uma3[['race_id', 'h_num_x', 'h_num_y', 'h_num']], haitou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebd2a70d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/20221127/weathers.csv')\n",
    "\n",
    "me_test = ModelEvaluator(lgb_clf, haitou, std=True)\n",
    "wrm = me_test.pred_table(X_test1, 0.75, False)\n",
    "twt = wrm.reset_index().merge(weather, on='race_id')\n",
    "twt['pred_rank'] = twt[['race_id', 'win_ratio']].groupby('race_id').rank(ascending=False)\n",
    "twt = twt[\n",
    "    (\n",
    "        (twt['pred_rank'] == 1)\n",
    "        |\n",
    "        (twt['pred_rank'] == 2)\n",
    "        |\n",
    "        (twt['pred_rank'] == 3)\n",
    "    )\n",
    "]\n",
    "\n",
    "twt['place'] = twt['place_id'].map(lambda x: places[x])\n",
    "# twt['ratio'] = twt['win_ratio'].map(lambda x: math.floor(x * 100))\n",
    "twt['num'] = twt['race_id'].astype(str).map(lambda x: x[14:])\n",
    "lbets = twt.copy()\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/20221127/weathers.csv')\n",
    "lbets = lbets.merge(weather, on='race_id')\n",
    "\n",
    "lb = lbets[['race_id', 'place', 'num', 'h_num', 'pred_rank']].sort_values(by=['race_id', 'pred_rank'])\n",
    "\n",
    "lb[['place', 'num', 'h_num']].to_csv('1127.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4a8148a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_me_test = ModelEvaluator(f_lgb_clf, haitou, True)\n",
    "\n",
    "fm_test = f_me_test.pred_table(X_fukusho_test1, 0.5, True)\n",
    "fm_test['pred_rank'] = fm_test[['win_ratio']].groupby(level=0).rank(ascending=False)\n",
    "fm_test = fm_test.reset_index().merge(weather[['race_id', 'place_id']], on='race_id')\n",
    "\n",
    "t_local_umaren = lbets[['race_id', 'h_num']].merge(fm_test, on='race_id')\n",
    "t_local_umaren = t_local_umaren[t_local_umaren['h_num_x'] != t_local_umaren['h_num_y']]\n",
    "\n",
    "l_umaren = lbets[['race_id', 'h_num']].merge(fm_test, on='race_id')\n",
    "\n",
    "l_uma_haito = t_local_umaren[t_local_umaren['h_num_x'] != t_local_umaren['h_num_y']]\n",
    "\n",
    "lu = l_uma_haito[['race_id', 'place_id', 'h_num_y', 'win_ratio', 'pred_rank']].drop_duplicates()\n",
    "lu['place'] = lu['place_id'].map(lambda x: places[x])\n",
    "lu['num'] = lu['race_id'].astype(str).map(lambda x: x[14:])\n",
    "\n",
    "lub = lu[['race_id', 'place', 'num', 'h_num_y', 'pred_rank']].sort_values(by=['race_id', 'pred_rank'])\n",
    "lub[['place', 'num', 'h_num_y']].to_csv('1127_ren.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
