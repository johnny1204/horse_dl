{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c395f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "import matplotlib.pyplot as pit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "\n",
    "# exports\n",
    "def plot_calibration_curve(named_classifiers, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"完全な補正\")\n",
    "    for name, clf in named_classifiers.items():\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, prob_pos)\n",
    "        brier = brier_score_loss(y_test, prob_pos)\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tAUC  : %1.3f\" % auc)\n",
    "        print(\"\\tBrier: %1.3f\" % (brier))\n",
    "        print()\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_test,\n",
    "            prob_pos,\n",
    "            n_bins=10,\n",
    "        )\n",
    "\n",
    "        ax1.plot(\n",
    "            mean_predicted_value,\n",
    "            fraction_of_positives,\n",
    "            \"s-\",\n",
    "            label=\"%s (%1.3f)\" % (name, brier),\n",
    "        )\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name, histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"正例の比率\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title(\"信頼性曲線\")\n",
    "\n",
    "    ax2.set_xlabel(\"予測値の平均\")\n",
    "    ax2.set_ylabel(\"サンプル数\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def preprocessing(results, kako=5):\n",
    "    df = results.copy()\n",
    "\n",
    "    df.drop([\n",
    "        'compi',\n",
    "        'compi_num', \n",
    "        \"speed\", \n",
    "        'rank',\n",
    "#         'result',\n",
    "        'course',\n",
    "        'born',\n",
    "        '1走前走破タイム', '2走前走破タイム', '3走前走破タイム', '4走前走破タイム', '5走前走破タイム',\n",
    "        '1走前補正タイム', '2走前補正タイム', '3走前補正タイム', '4走前補正タイム', '5走前補正タイム',\n",
    "        '1走前結果', '2走前結果', '3走前結果', '4走前結果', '5走前結果',\n",
    "        '1走前オッズ', '2走前オッズ', '3走前オッズ', '4走前オッズ', '5走前オッズ',\n",
    "        'horse_race_id',  'body_weight','body_weight_in_de'\n",
    "#         '季節出走回数', '季節勝率偏差値', '季節連対率偏差値', '季節複勝率偏差値',\n",
    "\n",
    "    ], axis=1, inplace=True)\n",
    "    df['date'] = df['date'].astype(str).map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    \n",
    "    df = df.sort_values(by='date', ascending = False)\n",
    "    df = df.set_index('race_id')\n",
    "    return df\n",
    "\n",
    "def split_data(df, test_size=0.3, place=None):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "\n",
    "    train = df.loc[train_ids]\n",
    "    test = df.loc[test_ids]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_valid_split_data(df, test_size=0.3):\n",
    "    sorted_ids = df.sort_values('date').index.unique()\n",
    "    train_ids = sorted_ids[:round(len(sorted_ids) * (1-test_size))]\n",
    "    test_ids = sorted_ids[round(len(sorted_ids) * (1-test_size)):]\n",
    "    \n",
    "    train = df.loc[train_ids]\n",
    "    valid = df.loc[test_ids]\n",
    "    \n",
    "    return train, valid\n",
    "\n",
    "def process_categorical(df, target_columns):\n",
    "    df2 = df.copy()\n",
    "    for column in target_columns:\n",
    "        df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n",
    "    # df2 = pd.get_dummies(df2, sparse=True)\n",
    "    df2 = pd.get_dummies(df2)\n",
    "    for column in target_columns:\n",
    "        df2[column] = df2[column].astype('category')\n",
    "        \n",
    "    return df2\n",
    "\n",
    "class TimeModel:\n",
    "    def __init__(self, model, base_data):\n",
    "        self.model = model\n",
    "        self.base_data = base_data\n",
    "        \n",
    "    def pred_time(self, X):\n",
    "        pred_time = self.base_data.copy()[['id', 'popular']]\n",
    "        actual_table = X.copy()[['id', 'h_num', 'place_id']]\n",
    "\n",
    "        X = X.drop(['id'], axis=1)\n",
    "        actual_table['pred_time'] = model.predict(X)\n",
    "\n",
    "        actual_table = actual_table.reset_index()\n",
    "        pred_time = pred_time.reset_index()\n",
    "        actual = pred_time.merge(actual_table, left_index=True, right_index=True, how='right')\n",
    "        actual.drop(['id_x', 'id_y', 'race_id_y'], axis=1, inplace=True)\n",
    "\n",
    "        return actual\n",
    "    \n",
    "    def race_pred_time(self, X):\n",
    "        actual = self.pred_time(X)\n",
    "        groups = actual.groupby('race_id_x').groups\n",
    "        column_list = [\"h_num\", 'pred_time', 'popular']\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        max_length = 0\n",
    "        for group, indexes in groups.items():\n",
    "            # 最後に並び替えをさせるのに最大作成された項目数を記録\n",
    "            length = len(indexes)+1\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "\n",
    "            columns = list()\n",
    "            values = list()\n",
    "            columns += ['race_id', 'place_id']\n",
    "            values += [actual.iloc[indexes]['race_id_x'].T.tolist()[0], actual.iloc[indexes]['place_id'].T.tolist()[0]]\n",
    "\n",
    "            for target_column in column_list:\n",
    "                columns += [f'{target_column}_{x}' for x in range(1, length)]\n",
    "                sort_values = actual.iloc[indexes, :].sort_values(by='pred_time', ascending = False)\n",
    "                values += sort_values[target_column].T.tolist()\n",
    "\n",
    "            record_df = pd.DataFrame([values], columns=columns)\n",
    "            new_df = pd.concat([new_df, record_df], axis=0)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, haitou_table, std = True):\n",
    "        self.model = model\n",
    "        self.haitou = haitou_table\n",
    "        self.std = std\n",
    "        self.pp = None\n",
    "        \n",
    "    def predict_proba(self, X, std=True):\n",
    "#         proba = pd.Series(self.model.predict_proba(X)[:, 1], index=X.index)\n",
    "        if self.pp is not None:\n",
    "          return self.pp\n",
    "\n",
    "        proba = pd.Series(self.model.predict_proba(X.drop(['id', 'odds', 'time_odds'], axis=1))[:, 1], index=X.index)\n",
    "        if std:\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "            \n",
    "        self.pp = proba\n",
    "        return proba\n",
    "    \n",
    "    def prefict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def win_ratio(self, X):\n",
    "        sum1 = pd.DataFrame(self.predict_proba(X).groupby(level=0).sum())\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        return [(p / sum1.loc[i])[0] for i, p in y_pred.items()]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        proba = self.predict_proba(X, True)\n",
    "        n = lambda x: 0.0 if np.isnan(x) else x\n",
    "        proba = proba.map(n)\n",
    "        return roc_auc_score(y_true, proba)\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({'features': X.columns, 'importance': self.model.feature_importances_})\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['h_num', 'odds', 'time_odds']]\n",
    "        pred_table['pred'] = self.prefict(X, threshold)\n",
    "        pred_table['win_ratio'] = self.win_ratio(X)\n",
    "        if bet_only:\n",
    "            pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds', 'win_ratio']]\n",
    "#             pred_table = pred_table[pred_table['pred'] == 1][['h_num', 'odds', 'time_odds']]\n",
    "            return pred_table\n",
    "        else:\n",
    "            return pred_table\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        haitou = self.haitou.copy()\n",
    "        df = haitou.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "\n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']]) + len(df[df['2着馬番'] == df['h_num']]) + len(df[df['3着馬番'] == df['h_num']]) + len(df[df['4着馬番'] == df['h_num']])\n",
    "        for i in range(1, 5):\n",
    "            money += df[df[str(i) + '着馬番'] == df['h_num']]['複勝' + str(i)].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate,n_hits\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        df['単勝配当'] = df['単勝'].astype(int)\n",
    "        \n",
    "#         std = ((df['1着馬番'] ==  df['h_num']) * df['単勝配当'])\\\n",
    "#         .groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['1着馬番'] == df['h_num']])\n",
    "        \n",
    "        money += df[df['1着馬番'] == df['h_num']]['単勝配当'].sum()\n",
    "        return_rate =  (n_bets*100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        df = self.haitou.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1/pred_table['odds']).sum()\n",
    "        std = ((df['1着馬番'] == df['h_num']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        df['h_num'] = df['h_num'].astype(float)\n",
    "        df['馬番_1'] = df['1着馬番']\n",
    "        n_hits = len(df.query('馬番_1 == h_num'))\n",
    "        return_rate = n_hits/bet_money\n",
    "        return n_bets, return_rate, n_hits\n",
    "        \n",
    "    \n",
    "def gain(return_func, X, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # 閾値を増やす        \n",
    "        threshold = 1 * i /n_samples + min_threshold * (1 - i/n_samples)\n",
    "        n_bets, return_rate, n_hits = return_func(X, threshold)\n",
    "        if n_bets == 0:\n",
    "            break;\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = { 'return_rate': return_rate, 'n_hits': n_hits }\n",
    "    return pd.DataFrame(gain).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd60e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "allrace = pd.read_pickle('./pickle_new/base_race_20220813_6.pickle')\n",
    "time = pd.read_csv('./csv_new2/base/race_time.csv')\n",
    "df = allrace.merge(time, how='left', on='id')\n",
    "\n",
    "# df = allrace.query('course == 2')\n",
    "all_r = preprocessing(df)\n",
    "# grouped = all_r.groupby('race_id')\n",
    "# all_r['correct_time'] = grouped['correct_time'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "all_r.drop(['気温', '1走前着差', '2走前着差','3走前着差',\n",
    "#             '1走前スピードZI', '2走前スピードZI', '3走前スピードZI'\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "for i in range(1, 63):\n",
    "    all_r.drop(['peds' + str(i)], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9644b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = process_categorical(all_r, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "train, valid = split_data(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0878b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_pickle('./pickle_new/new_race_20220903.pickle')\n",
    "time = pd.read_csv('./csv_new2/base/race_time.csv')\n",
    "target = target.merge(time, how='left', on='id')\n",
    "target = target[target['date'].notnull()]\n",
    "all_target = preprocessing(target)\n",
    "# grouped_target = all_target.groupby('race_id')\n",
    "# all_target['time'] = grouped_target['time'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "for i in range(1, 63):\n",
    "    all_target.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "\n",
    "test = process_categorical(all_target, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c8be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "X_train = train.drop(['id', 'horse_id', 'date','correct_time', 'result', 'odds', 'popular',  'producer', 'owner'], axis=1)\n",
    "# X_train = train.drop(['date'], axis=1)\n",
    "y_train = train['correct_time']\n",
    "X_valid = valid.drop(['date', 'horse_id', 'correct_time', 'result', 'odds', 'popular',  'producer', 'owner'], axis=1)\n",
    "y_valid = valid['correct_time']\n",
    "X_test = test.drop(['date', 'horse_id', 'correct_time', 'result', 'odds', 'popular',  'producer', 'owner'], axis=1)\n",
    "y_test = test['correct_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8cb5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:156: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59596\n",
      "[LightGBM] [Info] Number of data points in the train set: 476633, number of used features: 531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 88.239849\n",
      "[1]\tTrain's rmse: 11.6069\tTest's rmse: 12.2275\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tTrain's rmse: 11.3531\tTest's rmse: 11.9943\n",
      "[3]\tTrain's rmse: 11.1253\tTest's rmse: 11.793\n",
      "[4]\tTrain's rmse: 10.95\tTest's rmse: 11.6284\n",
      "[5]\tTrain's rmse: 10.7982\tTest's rmse: 11.4965\n",
      "[6]\tTrain's rmse: 10.6397\tTest's rmse: 11.3596\n",
      "[7]\tTrain's rmse: 10.4962\tTest's rmse: 11.2458\n",
      "[8]\tTrain's rmse: 10.3918\tTest's rmse: 11.1637\n",
      "[9]\tTrain's rmse: 10.2955\tTest's rmse: 11.0835\n",
      "[10]\tTrain's rmse: 10.2098\tTest's rmse: 11.0101\n",
      "[11]\tTrain's rmse: 10.1266\tTest's rmse: 10.9525\n",
      "[12]\tTrain's rmse: 10.0463\tTest's rmse: 10.9008\n",
      "[13]\tTrain's rmse: 9.98424\tTest's rmse: 10.8551\n",
      "[14]\tTrain's rmse: 9.92412\tTest's rmse: 10.8207\n",
      "[15]\tTrain's rmse: 9.87382\tTest's rmse: 10.7857\n",
      "[16]\tTrain's rmse: 9.81597\tTest's rmse: 10.7446\n",
      "[17]\tTrain's rmse: 9.76077\tTest's rmse: 10.7047\n",
      "[18]\tTrain's rmse: 9.71685\tTest's rmse: 10.6772\n",
      "[19]\tTrain's rmse: 9.66235\tTest's rmse: 10.6508\n",
      "[20]\tTrain's rmse: 9.61906\tTest's rmse: 10.6339\n",
      "[21]\tTrain's rmse: 9.56992\tTest's rmse: 10.6094\n",
      "[22]\tTrain's rmse: 9.53577\tTest's rmse: 10.5985\n",
      "[23]\tTrain's rmse: 9.49552\tTest's rmse: 10.5848\n",
      "[24]\tTrain's rmse: 9.4559\tTest's rmse: 10.5667\n",
      "[25]\tTrain's rmse: 9.42716\tTest's rmse: 10.5473\n",
      "[26]\tTrain's rmse: 9.39957\tTest's rmse: 10.5365\n",
      "[27]\tTrain's rmse: 9.36947\tTest's rmse: 10.5282\n",
      "[28]\tTrain's rmse: 9.34639\tTest's rmse: 10.5195\n",
      "[29]\tTrain's rmse: 9.32097\tTest's rmse: 10.514\n",
      "[30]\tTrain's rmse: 9.29978\tTest's rmse: 10.5073\n",
      "[31]\tTrain's rmse: 9.27397\tTest's rmse: 10.4949\n",
      "[32]\tTrain's rmse: 9.25282\tTest's rmse: 10.4901\n",
      "[33]\tTrain's rmse: 9.21877\tTest's rmse: 10.4808\n",
      "[34]\tTrain's rmse: 9.20046\tTest's rmse: 10.4723\n",
      "[35]\tTrain's rmse: 9.17848\tTest's rmse: 10.4579\n",
      "[36]\tTrain's rmse: 9.1495\tTest's rmse: 10.45\n",
      "[37]\tTrain's rmse: 9.12542\tTest's rmse: 10.4451\n",
      "[38]\tTrain's rmse: 9.10808\tTest's rmse: 10.4417\n",
      "[39]\tTrain's rmse: 9.08872\tTest's rmse: 10.4374\n",
      "[40]\tTrain's rmse: 9.07051\tTest's rmse: 10.4336\n",
      "[41]\tTrain's rmse: 9.05055\tTest's rmse: 10.4275\n",
      "[42]\tTrain's rmse: 9.02612\tTest's rmse: 10.4224\n",
      "[43]\tTrain's rmse: 9.00018\tTest's rmse: 10.4163\n",
      "[44]\tTrain's rmse: 8.98123\tTest's rmse: 10.41\n",
      "[45]\tTrain's rmse: 8.95954\tTest's rmse: 10.4029\n",
      "[46]\tTrain's rmse: 8.93689\tTest's rmse: 10.3994\n",
      "[47]\tTrain's rmse: 8.91393\tTest's rmse: 10.3968\n",
      "[48]\tTrain's rmse: 8.89252\tTest's rmse: 10.3947\n",
      "[49]\tTrain's rmse: 8.87636\tTest's rmse: 10.3885\n",
      "[50]\tTrain's rmse: 8.8567\tTest's rmse: 10.3864\n",
      "[51]\tTrain's rmse: 8.84354\tTest's rmse: 10.3793\n",
      "[52]\tTrain's rmse: 8.82563\tTest's rmse: 10.3741\n",
      "[53]\tTrain's rmse: 8.81127\tTest's rmse: 10.3664\n",
      "[54]\tTrain's rmse: 8.7922\tTest's rmse: 10.3651\n",
      "[55]\tTrain's rmse: 8.77896\tTest's rmse: 10.3625\n",
      "[56]\tTrain's rmse: 8.7629\tTest's rmse: 10.3594\n",
      "[57]\tTrain's rmse: 8.7469\tTest's rmse: 10.3552\n",
      "[58]\tTrain's rmse: 8.73584\tTest's rmse: 10.3504\n",
      "[59]\tTrain's rmse: 8.71825\tTest's rmse: 10.35\n",
      "[60]\tTrain's rmse: 8.70515\tTest's rmse: 10.348\n",
      "[61]\tTrain's rmse: 8.69183\tTest's rmse: 10.3459\n",
      "[62]\tTrain's rmse: 8.67292\tTest's rmse: 10.3441\n",
      "[63]\tTrain's rmse: 8.65914\tTest's rmse: 10.3422\n",
      "[64]\tTrain's rmse: 8.64155\tTest's rmse: 10.3405\n",
      "[65]\tTrain's rmse: 8.62385\tTest's rmse: 10.3389\n",
      "[66]\tTrain's rmse: 8.61326\tTest's rmse: 10.3358\n",
      "[67]\tTrain's rmse: 8.59994\tTest's rmse: 10.3347\n",
      "[68]\tTrain's rmse: 8.5829\tTest's rmse: 10.3333\n",
      "[69]\tTrain's rmse: 8.57179\tTest's rmse: 10.3317\n",
      "[70]\tTrain's rmse: 8.55457\tTest's rmse: 10.3293\n",
      "[71]\tTrain's rmse: 8.5354\tTest's rmse: 10.3292\n",
      "[72]\tTrain's rmse: 8.52207\tTest's rmse: 10.3267\n",
      "[73]\tTrain's rmse: 8.51393\tTest's rmse: 10.3247\n",
      "[74]\tTrain's rmse: 8.49634\tTest's rmse: 10.324\n",
      "[75]\tTrain's rmse: 8.48523\tTest's rmse: 10.3221\n",
      "[76]\tTrain's rmse: 8.46939\tTest's rmse: 10.3214\n",
      "[77]\tTrain's rmse: 8.45737\tTest's rmse: 10.3197\n",
      "[78]\tTrain's rmse: 8.44507\tTest's rmse: 10.3188\n",
      "[79]\tTrain's rmse: 8.43278\tTest's rmse: 10.3162\n",
      "[80]\tTrain's rmse: 8.42135\tTest's rmse: 10.3137\n",
      "[81]\tTrain's rmse: 8.41087\tTest's rmse: 10.3121\n",
      "[82]\tTrain's rmse: 8.40085\tTest's rmse: 10.3103\n",
      "[83]\tTrain's rmse: 8.39088\tTest's rmse: 10.3097\n",
      "[84]\tTrain's rmse: 8.38085\tTest's rmse: 10.3091\n",
      "[85]\tTrain's rmse: 8.3693\tTest's rmse: 10.3077\n",
      "[86]\tTrain's rmse: 8.35195\tTest's rmse: 10.3082\n",
      "[87]\tTrain's rmse: 8.33828\tTest's rmse: 10.3078\n",
      "[88]\tTrain's rmse: 8.32559\tTest's rmse: 10.3068\n",
      "[89]\tTrain's rmse: 8.31751\tTest's rmse: 10.3048\n",
      "[90]\tTrain's rmse: 8.30288\tTest's rmse: 10.305\n",
      "[91]\tTrain's rmse: 8.28766\tTest's rmse: 10.3047\n",
      "[92]\tTrain's rmse: 8.27057\tTest's rmse: 10.304\n",
      "[93]\tTrain's rmse: 8.25994\tTest's rmse: 10.3038\n",
      "[94]\tTrain's rmse: 8.24926\tTest's rmse: 10.303\n",
      "[95]\tTrain's rmse: 8.24136\tTest's rmse: 10.3007\n",
      "[96]\tTrain's rmse: 8.23162\tTest's rmse: 10.3002\n",
      "[97]\tTrain's rmse: 8.22268\tTest's rmse: 10.2984\n",
      "[98]\tTrain's rmse: 8.20894\tTest's rmse: 10.2987\n",
      "[99]\tTrain's rmse: 8.20054\tTest's rmse: 10.2956\n",
      "[100]\tTrain's rmse: 8.18688\tTest's rmse: 10.2962\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's rmse: 8.18688\tTest's rmse: 10.2962\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "params = {\n",
    "#   'task': 'train',\n",
    "#   'boosting_type': 'gbdt',\n",
    "  'objective': 'regression',\n",
    "  'metric': 'rmse', \n",
    "#   'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "# model = lgb.LGBMRegressor()\n",
    "# model.fit(\n",
    "#     X_train, \n",
    "#     y_train,\n",
    "#     eval_set=[(X_test.drop(['id'], axis=1), y_test), (X_train, y_train)],\n",
    "#     verbose=-1\n",
    "# )\n",
    "\n",
    "# lgb.plot_metric(model)\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid.drop(['id'], axis=1), y_valid, reference=lgb_train)\n",
    "# lgb_test = lgb.Dataset(X_test.drop(['id'], axis=1), y_test, reference=lgb_train)\n",
    "\n",
    "lgb_results = {} \n",
    "\n",
    "params = {'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 1.7428030658502613e-06,\n",
    " 'lambda_l2': 9.047629861481605,\n",
    " 'num_leaves': 217,\n",
    " 'feature_fraction': 0.41600000000000004,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 100,\n",
    " 'num_iterations': 100,\n",
    " 'early_stopping_round': 50,\n",
    " 'categorical_column': [2, 4, 5, 6, 7, 232, 236, 237, 309, 310, 312, 313]}\n",
    "\n",
    "model = lgb.train(\n",
    "  params=params,\n",
    "  train_set=lgb_train,\n",
    "  valid_sets=[lgb_train, lgb_valid],\n",
    "#   valid_sets=[lgb_train, lgb_test],\n",
    "  valid_names=['Train', 'Test'],\n",
    "  num_boost_round=100,\n",
    ") \n",
    "\n",
    "# model = lgb_o.train(\n",
    "#     params,\n",
    "#     lgb_train,\n",
    "#     valid_sets=lgb_valid,\n",
    "#     verbose_eval=False,\n",
    "#     show_progress_bar=False,\n",
    "#     num_boost_round=100,\n",
    "#     early_stopping_rounds=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3010,
   "id": "e2141b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'regression',\n",
       " 'metric': 'rmse',\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 1.7428030658502613e-06,\n",
       " 'lambda_l2': 9.047629861481605,\n",
       " 'num_leaves': 217,\n",
       " 'feature_fraction': 0.41600000000000004,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 100,\n",
       " 'num_iterations': 100,\n",
       " 'early_stopping_round': 50,\n",
       " 'categorical_column': [2, 4, 5, 6, 7, 232, 236, 237, 309, 310, 312, 313]}"
      ]
     },
     "execution_count": 3010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0613699b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.60370589, 64.93039055, 92.37040751, ..., 96.96132748,\n",
       "       96.18697534, 87.58882222])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = X_valid.drop(['id'], axis=1)\n",
    "y_pred = model.predict(X)\n",
    "# r2_score(y_valid, y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec71279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trainer_id</td>\n",
       "      <td>3527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>stallion_id</td>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jockey_id</td>\n",
       "      <td>2950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distance</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1走前スピード指数</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>grade</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>間隔</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>上がり3F平均</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1走前上がり指数</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1走前上がり3F順</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>furlong_1</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1走前相対着順</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>コースタイプ勝率</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>furlong_2_1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>place_id</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>training_course</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>頭数</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2走前相対着順</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>body_weight</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>コースタイプ複勝率</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>コースタイプ連対率</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1走前上がり3F</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2走前スピード指数</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>父系統コース別勝率</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>馬場状態</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>body_weight_in_de</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>獲得賞金合計</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>種牡馬コース別複勝率</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>父系統コース別複勝率</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2走前上がり指数</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1走前スピードZI</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>種牡馬同コース同距離別複勝率</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>furlong_3</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1走前距離</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1走前平均速度</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1走前馬場指数</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>furlong_4</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>騎手同コース同距離別騎乗回数</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2走前上がり3F順</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  importance\n",
       "5           trainer_id        3527\n",
       "384        stallion_id        3283\n",
       "2            jockey_id        2950\n",
       "1             distance         418\n",
       "44           1走前スピード指数         251\n",
       "467              grade         241\n",
       "13                  間隔         207\n",
       "501            上がり3F平均         191\n",
       "43            1走前上がり指数         187\n",
       "34           1走前上がり3F順         180\n",
       "374          furlong_1         174\n",
       "15             1走前相対着順         170\n",
       "507           コースタイプ勝率         167\n",
       "379        furlong_2_1         165\n",
       "468           place_id         160\n",
       "380    training_course         136\n",
       "466                 頭数         136\n",
       "84             2走前相対着順         125\n",
       "9          body_weight         125\n",
       "523          コースタイプ複勝率         116\n",
       "515          コースタイプ連対率         110\n",
       "33            1走前上がり3F         104\n",
       "113          2走前スピード指数         100\n",
       "478          父系統コース別勝率          89\n",
       "465               馬場状態          84\n",
       "10   body_weight_in_de          83\n",
       "528             獲得賞金合計          83\n",
       "444         種牡馬コース別複勝率          83\n",
       "480         父系統コース別複勝率          81\n",
       "112           2走前上がり指数          80\n",
       "24           1走前スピードZI          77\n",
       "452     種牡馬同コース同距離別複勝率          74\n",
       "373          furlong_3          74\n",
       "48               1走前距離          73\n",
       "37             1走前平均速度          71\n",
       "0                  age          70\n",
       "46             1走前馬場指数          69\n",
       "372          furlong_4          68\n",
       "402     騎手同コース同距離別騎乗回数          68\n",
       "103          2走前上がり3F順          67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({'features': X.columns, 'importance':model.feature_importance()})\n",
    "importances.sort_values('importance', ascending=False)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e296943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allrace = pd.read_pickle('./pickle_new/base_race_20220813_6.pickle')\n",
    "time_odds_base = pd.read_csv('./csv_new2/time_odds.csv')\n",
    "allrace = allrace.merge(time_odds_base, how='left', on='id')\n",
    "\n",
    "df = allrace.query('course == 2')\n",
    "all_r = preprocessing(df)\n",
    "# all_r = preprocessing(allrace)\n",
    "all_r['result'] = all_r['result'].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "all_r.drop([\n",
    "  '気温', '風向', '風速', '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI', '4走前スピードZI', '5走前スピードZI',\n",
    "], axis=1, inplace=True)\n",
    "for i in range(1, 63):\n",
    "    all_r.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "\n",
    "categorical = process_categorical(all_r, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "train1, valid1 = split_data(categorical)\n",
    "valid1, test2 = train_valid_split_data(valid1)\n",
    "\n",
    "target = pd.read_pickle('./pickle_new/new_race_20220904.pickle')\n",
    "time_odds = pd.read_csv('./csv_new2/20220904/time_odds.csv')\n",
    "target = target.merge(time_odds, how='left', on='id')\n",
    "target = target[target['date'].notnull()]\n",
    "\n",
    "target = target.query('course == 2')\n",
    "target = preprocessing(target)\n",
    "target['result'] = target['result'].map(lambda x: 1 if x == 1 else 0)\n",
    "target.drop([\n",
    "  '気温', '風向', '風速', '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI', '4走前スピードZI', '5走前スピードZI',\n",
    "  '先行指数', 'ペース指数', '上がり指数', 'スピード指数'\n",
    "], axis=1, inplace=True)\n",
    "for i in range(1, 63):\n",
    "    target.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "test1 = process_categorical(target,  [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "X_train1 = train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "y_train1 = train1['result']\n",
    "X_valid1 = valid1.drop(['date', 'result', 'popular',  'time_popular', 'horse_id'], axis=1)\n",
    "y_valid1 = valid1['result']\n",
    "X_test1 = test1.drop(['date', 'result', 'popular',  'time_popular', 'horse_id'], axis=1)\n",
    "y_test1 = test1['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74580db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-03 20:45:31,708]\u001b[0m A new study created in memory with name: no-name-4e6f673b-ef60-41ad-b0a4-2a96d27bfaae\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                        | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248693:  14%|########4                                                  | 1/7 [00:10<01:04, 10.79s/it]\u001b[32m[I 2022-09-03 20:45:42,558]\u001b[0m Trial 0 finished with value: 0.24869310728734992 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.24869310728734992.\u001b[0m\n",
      "feature_fraction, val_score: 0.248693:  14%|########4                                                  | 1/7 [00:10<01:04, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.214613\tvalid_1's binary_logloss: 0.248693\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248693:  29%|################8                                          | 2/7 [00:22<00:56, 11.36s/it]\u001b[32m[I 2022-09-03 20:45:54,313]\u001b[0m Trial 1 finished with value: 0.250002943510029 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.24869310728734992.\u001b[0m\n",
      "feature_fraction, val_score: 0.248693:  29%|################8                                          | 2/7 [00:22<00:56, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.21594\tvalid_1's binary_logloss: 0.250003\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248693:  43%|#########################2                                 | 3/7 [00:31<00:41, 10.49s/it]\u001b[32m[I 2022-09-03 20:46:03,760]\u001b[0m Trial 2 finished with value: 0.2487850506486552 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.24869310728734992.\u001b[0m\n",
      "feature_fraction, val_score: 0.248693:  43%|#########################2                                 | 3/7 [00:32<00:41, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.221231\tvalid_1's binary_logloss: 0.248785\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248693:  57%|#################################7                         | 4/7 [00:42<00:31, 10.63s/it]\u001b[32m[I 2022-09-03 20:46:14,620]\u001b[0m Trial 3 finished with value: 0.25106810049504763 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.24869310728734992.\u001b[0m\n",
      "feature_fraction, val_score: 0.248693:  57%|#################################7                         | 4/7 [00:42<00:31, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.219765\tvalid_1's binary_logloss: 0.251068\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248153:  71%|##########################################1                | 5/7 [00:54<00:21, 10.85s/it]\u001b[32m[I 2022-09-03 20:46:25,858]\u001b[0m Trial 4 finished with value: 0.24815321261937678 and parameters: {'feature_fraction': 0.6}. Best is trial 4 with value: 0.24815321261937678.\u001b[0m\n",
      "feature_fraction, val_score: 0.248153:  71%|##########################################1                | 5/7 [00:54<00:21, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.215566\tvalid_1's binary_logloss: 0.248153\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248153:  86%|##################################################5        | 6/7 [01:07<00:11, 11.61s/it]\u001b[32m[I 2022-09-03 20:46:38,927]\u001b[0m Trial 5 finished with value: 0.24984475935938116 and parameters: {'feature_fraction': 1.0}. Best is trial 4 with value: 0.24815321261937678.\u001b[0m\n",
      "feature_fraction, val_score: 0.248153:  86%|##################################################5        | 6/7 [01:07<00:11, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.211666\tvalid_1's binary_logloss: 0.249845\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.248153: 100%|###########################################################| 7/7 [01:18<00:00, 11.38s/it]\u001b[32m[I 2022-09-03 20:46:49,843]\u001b[0m Trial 6 finished with value: 0.25028352737515797 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 4 with value: 0.24815321261937678.\u001b[0m\n",
      "feature_fraction, val_score: 0.248153: 100%|###########################################################| 7/7 [01:18<00:00, 11.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.218222\tvalid_1's binary_logloss: 0.250284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:   0%|                                                                        | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:   5%|###2                                                            | 1/20 [00:09<03:09, 10.00s/it]\u001b[32m[I 2022-09-03 20:46:59,891]\u001b[0m Trial 7 finished with value: 0.24938357841065079 and parameters: {'num_leaves': 76}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:   5%|###2                                                            | 1/20 [00:10<03:09, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.220654\tvalid_1's binary_logloss: 0.249384\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.505526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  10%|######4                                                         | 2/20 [00:20<03:03, 10.18s/it]\u001b[32m[I 2022-09-03 20:47:10,188]\u001b[0m Trial 8 finished with value: 0.24981032478845452 and parameters: {'num_leaves': 59}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  10%|######4                                                         | 2/20 [00:20<03:03, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.222375\tvalid_1's binary_logloss: 0.24981\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  15%|#########6                                                      | 3/20 [00:30<02:49,  9.99s/it]\u001b[32m[I 2022-09-03 20:47:19,952]\u001b[0m Trial 9 finished with value: 0.25190533990218533 and parameters: {'num_leaves': 191}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  15%|#########6                                                      | 3/20 [00:30<02:49,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.232146\tvalid_1's binary_logloss: 0.251905\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.461611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  20%|############8                                                   | 4/20 [00:40<02:40, 10.03s/it]\u001b[32m[I 2022-09-03 20:47:30,042]\u001b[0m Trial 10 finished with value: 0.24993117526749728 and parameters: {'num_leaves': 69}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  20%|############8                                                   | 4/20 [00:40<02:40, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.221354\tvalid_1's binary_logloss: 0.249931\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117486 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  25%|################                                                | 5/20 [00:50<02:29,  9.97s/it]\u001b[32m[I 2022-09-03 20:47:39,909]\u001b[0m Trial 11 finished with value: 0.25164488319772205 and parameters: {'num_leaves': 196}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  25%|################                                                | 5/20 [00:50<02:29,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.232023\tvalid_1's binary_logloss: 0.251645\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.487295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  30%|###################2                                            | 6/20 [00:59<02:17,  9.81s/it]\u001b[32m[I 2022-09-03 20:47:49,389]\u001b[0m Trial 12 finished with value: 0.2525948976449673 and parameters: {'num_leaves': 121}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  30%|###################2                                            | 6/20 [00:59<02:17,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.234758\tvalid_1's binary_logloss: 0.252595\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  35%|######################4                                         | 7/20 [01:09<02:08,  9.87s/it]\u001b[32m[I 2022-09-03 20:47:59,391]\u001b[0m Trial 13 finished with value: 0.2515672099783707 and parameters: {'num_leaves': 252}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  35%|######################4                                         | 7/20 [01:09<02:08,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.230016\tvalid_1's binary_logloss: 0.251567\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  40%|#########################6                                      | 8/20 [01:22<02:09, 10.80s/it]\u001b[32m[I 2022-09-03 20:48:12,200]\u001b[0m Trial 14 finished with value: 0.24949489624931892 and parameters: {'num_leaves': 199}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  40%|#########################6                                      | 8/20 [01:22<02:09, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.193878\tvalid_1's binary_logloss: 0.249495\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  45%|############################8                                   | 9/20 [01:31<01:54, 10.43s/it]\u001b[32m[I 2022-09-03 20:48:21,796]\u001b[0m Trial 15 finished with value: 0.25195090461775316 and parameters: {'num_leaves': 170}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  45%|############################8                                   | 9/20 [01:31<01:54, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.232918\tvalid_1's binary_logloss: 0.251951\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.475763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  50%|###############################5                               | 10/20 [01:42<01:43, 10.34s/it]\u001b[32m[I 2022-09-03 20:48:31,953]\u001b[0m Trial 16 finished with value: 0.25211905361076403 and parameters: {'num_leaves': 223}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  50%|###############################5                               | 10/20 [01:42<01:43, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.230991\tvalid_1's binary_logloss: 0.252119\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.453406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  55%|##################################6                            | 11/20 [01:51<01:31, 10.13s/it]\u001b[32m[I 2022-09-03 20:48:41,606]\u001b[0m Trial 17 finished with value: 0.2518217992112271 and parameters: {'num_leaves': 9}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  55%|##################################6                            | 11/20 [01:51<01:31, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.222238\tvalid_1's binary_logloss: 0.251822\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.474659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  60%|#####################################8                         | 12/20 [02:01<01:19,  9.99s/it]\u001b[32m[I 2022-09-03 20:48:51,293]\u001b[0m Trial 18 finished with value: 0.25278352005323473 and parameters: {'num_leaves': 129}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  60%|#####################################8                         | 12/20 [02:01<01:19,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.234447\tvalid_1's binary_logloss: 0.252784\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  65%|########################################9                      | 13/20 [02:10<01:08,  9.74s/it]\u001b[32m[I 2022-09-03 20:49:00,427]\u001b[0m Trial 19 finished with value: 0.25190519849832066 and parameters: {'num_leaves': 90}. Best is trial 7 with value: 0.24938357841065079.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  65%|########################################9                      | 13/20 [02:10<01:08,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.235862\tvalid_1's binary_logloss: 0.251905\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.484081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  70%|############################################                   | 14/20 [02:22<01:01, 10.27s/it]\u001b[32m[I 2022-09-03 20:49:11,911]\u001b[0m Trial 20 finished with value: 0.248684384096857 and parameters: {'num_leaves': 16}. Best is trial 20 with value: 0.248684384096857.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  70%|############################################                   | 14/20 [02:22<01:01, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.216038\tvalid_1's binary_logloss: 0.248684\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.510860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  75%|###############################################2               | 15/20 [02:32<00:51, 10.37s/it]\u001b[32m[I 2022-09-03 20:49:22,530]\u001b[0m Trial 21 finished with value: 0.24902324355757344 and parameters: {'num_leaves': 18}. Best is trial 20 with value: 0.248684384096857.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  75%|###############################################2               | 15/20 [02:32<00:51, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.218781\tvalid_1's binary_logloss: 0.249023\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  80%|##################################################4            | 16/20 [02:40<00:38,  9.66s/it]\u001b[32m[I 2022-09-03 20:49:30,528]\u001b[0m Trial 22 finished with value: 0.2548037249599716 and parameters: {'num_leaves': 4}. Best is trial 20 with value: 0.248684384096857.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  80%|##################################################4            | 16/20 [02:40<00:38,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.242819\tvalid_1's binary_logloss: 0.254804\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.248153:  85%|#####################################################5         | 17/20 [02:51<00:30, 10.02s/it]\u001b[32m[I 2022-09-03 20:49:41,411]\u001b[0m Trial 23 finished with value: 0.24859044278340703 and parameters: {'num_leaves': 40}. Best is trial 23 with value: 0.24859044278340703.\u001b[0m\n",
      "num_leaves, val_score: 0.248153:  85%|#####################################################5         | 17/20 [02:51<00:30, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217102\tvalid_1's binary_logloss: 0.24859\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.247777:  90%|########################################################7      | 18/20 [03:03<00:20, 10.47s/it]\u001b[32m[I 2022-09-03 20:49:52,938]\u001b[0m Trial 24 finished with value: 0.2477772808228006 and parameters: {'num_leaves': 39}. Best is trial 24 with value: 0.2477772808228006.\u001b[0m\n",
      "num_leaves, val_score: 0.247777:  90%|########################################################7      | 18/20 [03:03<00:20, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.214411\tvalid_1's binary_logloss: 0.247777\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.479376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.247777:  95%|###########################################################8   | 19/20 [03:13<00:10, 10.59s/it]\u001b[32m[I 2022-09-03 20:50:03,816]\u001b[0m Trial 25 finished with value: 0.24859044278340703 and parameters: {'num_leaves': 40}. Best is trial 24 with value: 0.2477772808228006.\u001b[0m\n",
      "num_leaves, val_score: 0.247777:  95%|###########################################################8   | 19/20 [03:13<00:10, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217102\tvalid_1's binary_logloss: 0.24859\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.247777: 100%|###############################################################| 20/20 [03:23<00:00, 10.30s/it]\u001b[32m[I 2022-09-03 20:50:13,412]\u001b[0m Trial 26 finished with value: 0.2519723778584138 and parameters: {'num_leaves': 101}. Best is trial 24 with value: 0.2477772808228006.\u001b[0m\n",
      "num_leaves, val_score: 0.247777: 100%|###############################################################| 20/20 [03:23<00:00, 10.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.235507\tvalid_1's binary_logloss: 0.251972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247777:   0%|                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247777:  10%|######7                                                            | 1/10 [00:12<01:50, 12.23s/it]\u001b[32m[I 2022-09-03 20:50:25,672]\u001b[0m Trial 27 finished with value: 0.24835460945104834 and parameters: {'bagging_fraction': 0.9600314984565207, 'bagging_freq': 4}. Best is trial 27 with value: 0.24835460945104834.\u001b[0m\n",
      "bagging, val_score: 0.247777:  10%|######7                                                            | 1/10 [00:12<01:50, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.211307\tvalid_1's binary_logloss: 0.248355\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247777:  20%|#############4                                                     | 2/10 [00:20<01:20, 10.12s/it]\u001b[32m[I 2022-09-03 20:50:34,322]\u001b[0m Trial 28 finished with value: 0.25286128742572955 and parameters: {'bagging_fraction': 0.42100893694864455, 'bagging_freq': 5}. Best is trial 27 with value: 0.24835460945104834.\u001b[0m\n",
      "bagging, val_score: 0.247777:  20%|#############4                                                     | 2/10 [00:20<01:20, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.232626\tvalid_1's binary_logloss: 0.252861\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247777:  30%|####################                                               | 3/10 [00:31<01:12, 10.36s/it]\u001b[32m[I 2022-09-03 20:50:44,976]\u001b[0m Trial 29 finished with value: 0.24788212795630432 and parameters: {'bagging_fraction': 0.8798342197427209, 'bagging_freq': 7}. Best is trial 29 with value: 0.24788212795630432.\u001b[0m\n",
      "bagging, val_score: 0.247777:  30%|####################                                               | 3/10 [00:31<01:12, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.215774\tvalid_1's binary_logloss: 0.247882\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  40%|##########################8                                        | 4/10 [00:42<01:03, 10.57s/it]\u001b[32m[I 2022-09-03 20:50:55,863]\u001b[0m Trial 30 finished with value: 0.24718820224564583 and parameters: {'bagging_fraction': 0.695505240014514, 'bagging_freq': 7}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  40%|##########################8                                        | 4/10 [00:42<01:03, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  50%|#################################5                                 | 5/10 [00:50<00:49,  9.81s/it]\u001b[32m[I 2022-09-03 20:51:04,320]\u001b[0m Trial 31 finished with value: 0.2528062873913272 and parameters: {'bagging_fraction': 0.4667410497241142, 'bagging_freq': 7}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  50%|#################################5                                 | 5/10 [00:50<00:49,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.228218\tvalid_1's binary_logloss: 0.252806\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  60%|########################################1                          | 6/10 [01:00<00:38,  9.71s/it]\u001b[32m[I 2022-09-03 20:51:13,836]\u001b[0m Trial 32 finished with value: 0.2511577650925067 and parameters: {'bagging_fraction': 0.9415175777559253, 'bagging_freq': 3}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  60%|########################################1                          | 6/10 [01:00<00:38,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.227786\tvalid_1's binary_logloss: 0.251158\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  70%|##############################################9                    | 7/10 [01:10<00:29,  9.69s/it]\u001b[32m[I 2022-09-03 20:51:23,491]\u001b[0m Trial 33 finished with value: 0.25237645919129426 and parameters: {'bagging_fraction': 0.8398772804519108, 'bagging_freq': 6}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  70%|##############################################9                    | 7/10 [01:10<00:29,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.223155\tvalid_1's binary_logloss: 0.252376\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.498782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  80%|#####################################################6             | 8/10 [01:18<00:18,  9.45s/it]\u001b[32m[I 2022-09-03 20:51:32,434]\u001b[0m Trial 34 finished with value: 0.2532091850700246 and parameters: {'bagging_fraction': 0.40172271700869794, 'bagging_freq': 6}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  80%|#####################################################6             | 8/10 [01:19<00:18,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.220375\tvalid_1's binary_logloss: 0.253209\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188:  90%|############################################################3      | 9/10 [01:29<00:09,  9.68s/it]\u001b[32m[I 2022-09-03 20:51:42,633]\u001b[0m Trial 35 finished with value: 0.2501083836327584 and parameters: {'bagging_fraction': 0.7203124887361333, 'bagging_freq': 1}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188:  90%|############################################################3      | 9/10 [01:29<00:09,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.216422\tvalid_1's binary_logloss: 0.250108\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.474836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.247188: 100%|##################################################################| 10/10 [01:38<00:00,  9.53s/it]\u001b[32m[I 2022-09-03 20:51:51,824]\u001b[0m Trial 36 finished with value: 0.253290578699552 and parameters: {'bagging_fraction': 0.6889580919647158, 'bagging_freq': 3}. Best is trial 30 with value: 0.24718820224564583.\u001b[0m\n",
      "bagging, val_score: 0.247188: 100%|##################################################################| 10/10 [01:38<00:00,  9.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.229826\tvalid_1's binary_logloss: 0.253291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:   0%|                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.479321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:  17%|########6                                           | 1/6 [00:09<00:49,  9.90s/it]\u001b[32m[I 2022-09-03 20:52:01,764]\u001b[0m Trial 37 finished with value: 0.25057153463069914 and parameters: {'feature_fraction': 0.6479999999999999}. Best is trial 37 with value: 0.25057153463069914.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188:  17%|########6                                           | 1/6 [00:09<00:49,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.220885\tvalid_1's binary_logloss: 0.250572\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:  33%|#################3                                  | 2/6 [00:19<00:40, 10.00s/it]\u001b[32m[I 2022-09-03 20:52:11,855]\u001b[0m Trial 38 finished with value: 0.25121234918641716 and parameters: {'feature_fraction': 0.6799999999999999}. Best is trial 37 with value: 0.25057153463069914.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188:  33%|#################3                                  | 2/6 [00:20<00:40, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.218657\tvalid_1's binary_logloss: 0.251212\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:  50%|##########################                          | 3/6 [00:29<00:28,  9.56s/it]\u001b[32m[I 2022-09-03 20:52:20,890]\u001b[0m Trial 39 finished with value: 0.2507518696345076 and parameters: {'feature_fraction': 0.52}. Best is trial 37 with value: 0.25057153463069914.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188:  50%|##########################                          | 3/6 [00:29<00:28,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.222285\tvalid_1's binary_logloss: 0.250752\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:  67%|##################################6                 | 4/6 [00:39<00:19,  9.94s/it]\u001b[32m[I 2022-09-03 20:52:31,390]\u001b[0m Trial 40 finished with value: 0.2487261351104912 and parameters: {'feature_fraction': 0.616}. Best is trial 40 with value: 0.2487261351104912.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188:  67%|##################################6                 | 4/6 [00:39<00:19,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.215934\tvalid_1's binary_logloss: 0.248726\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188:  83%|###########################################3        | 5/6 [00:48<00:09,  9.69s/it]\u001b[32m[I 2022-09-03 20:52:40,649]\u001b[0m Trial 41 finished with value: 0.24931880613570737 and parameters: {'feature_fraction': 0.552}. Best is trial 40 with value: 0.2487261351104912.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188:  83%|###########################################3        | 5/6 [00:48<00:09,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.219942\tvalid_1's binary_logloss: 0.249319\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.247188: 100%|####################################################| 6/6 [00:58<00:00,  9.76s/it]\u001b[32m[I 2022-09-03 20:52:50,545]\u001b[0m Trial 42 finished with value: 0.2493090520406665 and parameters: {'feature_fraction': 0.584}. Best is trial 40 with value: 0.2487261351104912.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.247188: 100%|####################################################| 6/6 [00:58<00:00,  9.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.216706\tvalid_1's binary_logloss: 0.249309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:   0%|                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.449539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:   5%|##6                                                 | 1/20 [00:10<03:27, 10.94s/it]\u001b[32m[I 2022-09-03 20:53:01,518]\u001b[0m Trial 43 finished with value: 0.24718821139360578 and parameters: {'lambda_l1': 2.6344748853740735e-08, 'lambda_l2': 8.616476583443115e-05}. Best is trial 43 with value: 0.24718821139360578.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:   5%|##6                                                 | 1/20 [00:10<03:27, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.523705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  10%|#####2                                              | 2/20 [00:24<03:44, 12.49s/it]\u001b[32m[I 2022-09-03 20:53:15,111]\u001b[0m Trial 44 finished with value: 0.2471882022535103 and parameters: {'lambda_l1': 5.391702414582258e-08, 'lambda_l2': 5.046358817804582e-08}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  10%|#####2                                              | 2/20 [00:24<03:44, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  15%|#######8                                            | 3/20 [00:35<03:21, 11.83s/it]\u001b[32m[I 2022-09-03 20:53:26,155]\u001b[0m Trial 45 finished with value: 0.24876260281767512 and parameters: {'lambda_l1': 5.412624433773939e-05, 'lambda_l2': 0.045713017584400405}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  15%|#######8                                            | 3/20 [00:35<03:21, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.215978\tvalid_1's binary_logloss: 0.248763\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  20%|##########4                                         | 4/20 [00:48<03:18, 12.39s/it]\u001b[32m[I 2022-09-03 20:53:39,403]\u001b[0m Trial 46 finished with value: 0.24718820287060603 and parameters: {'lambda_l1': 0.0001750227986511235, 'lambda_l2': 4.863780309781348e-08}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  20%|##########4                                         | 4/20 [00:48<03:18, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  25%|#############                                       | 5/20 [00:59<02:58, 11.90s/it]\u001b[32m[I 2022-09-03 20:53:50,430]\u001b[0m Trial 47 finished with value: 0.24914215247782415 and parameters: {'lambda_l1': 2.383544984897175e-07, 'lambda_l2': 0.16707099654328053}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  25%|#############                                       | 5/20 [00:59<02:58, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.221024\tvalid_1's binary_logloss: 0.249142\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.483889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  30%|###############6                                    | 6/20 [01:11<02:43, 11.66s/it]\u001b[32m[I 2022-09-03 20:54:01,614]\u001b[0m Trial 48 finished with value: 0.24718833446652716 and parameters: {'lambda_l1': 2.829979627717984e-05, 'lambda_l2': 0.00135211451636215}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  30%|###############6                                    | 6/20 [01:11<02:43, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211974\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  35%|##################2                                 | 7/20 [01:22<02:31, 11.64s/it]\u001b[32m[I 2022-09-03 20:54:13,213]\u001b[0m Trial 49 finished with value: 0.2474085225573573 and parameters: {'lambda_l1': 0.16852661883791112, 'lambda_l2': 6.811285958594995e-05}. Best is trial 44 with value: 0.2471882022535103.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  35%|##################2                                 | 7/20 [01:22<02:31, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.209788\tvalid_1's binary_logloss: 0.247409\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  40%|####################8                               | 8/20 [01:33<02:17, 11.44s/it]\u001b[32m[I 2022-09-03 20:54:24,222]\u001b[0m Trial 50 finished with value: 0.24718820225249064 and parameters: {'lambda_l1': 8.745232611759162e-08, 'lambda_l2': 4.152418124794684e-08}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  40%|####################8                               | 8/20 [01:33<02:17, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  45%|#######################4                            | 9/20 [01:48<02:19, 12.64s/it]\u001b[32m[I 2022-09-03 20:54:39,500]\u001b[0m Trial 51 finished with value: 0.24850640473059027 and parameters: {'lambda_l1': 1.6113435964881561e-07, 'lambda_l2': 4.534388767665213}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  45%|#######################4                            | 9/20 [01:48<02:19, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's binary_logloss: 0.203634\tvalid_1's binary_logloss: 0.248506\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  50%|#########################5                         | 10/20 [01:58<01:57, 11.71s/it]\u001b[32m[I 2022-09-03 20:54:49,125]\u001b[0m Trial 52 finished with value: 0.2500170363444127 and parameters: {'lambda_l1': 1.1134375059816044, 'lambda_l2': 6.929060793858594e-05}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  50%|#########################5                         | 10/20 [01:58<01:57, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.2234\tvalid_1's binary_logloss: 0.250017\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.471220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  55%|############################                       | 11/20 [02:09<01:43, 11.47s/it]\u001b[32m[I 2022-09-03 20:55:00,050]\u001b[0m Trial 53 finished with value: 0.24718825005840106 and parameters: {'lambda_l1': 0.011918323889227733, 'lambda_l2': 9.73122361091223e-07}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  55%|############################                       | 11/20 [02:09<01:43, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211975\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  60%|##############################6                    | 12/20 [02:20<01:30, 11.35s/it]\u001b[32m[I 2022-09-03 20:55:11,139]\u001b[0m Trial 54 finished with value: 0.24718820225934482 and parameters: {'lambda_l1': 1.2905185947597977e-06, 'lambda_l2': 1.2265938834581663e-08}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  60%|##############################6                    | 12/20 [02:20<01:30, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  65%|#################################1                 | 13/20 [02:31<01:18, 11.28s/it]\u001b[32m[I 2022-09-03 20:55:22,235]\u001b[0m Trial 55 finished with value: 0.24718820235703295 and parameters: {'lambda_l1': 1.1301578290909334e-08, 'lambda_l2': 9.920828693830563e-07}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  65%|#################################1                 | 13/20 [02:31<01:18, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  70%|###################################6               | 14/20 [02:42<01:07, 11.21s/it]\u001b[32m[I 2022-09-03 20:55:33,302]\u001b[0m Trial 56 finished with value: 0.24718820233469943 and parameters: {'lambda_l1': 2.812233205181919e-06, 'lambda_l2': 6.486150099210082e-07}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  70%|###################################6               | 14/20 [02:42<01:07, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.483456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  75%|######################################2            | 15/20 [02:53<00:55, 11.17s/it]\u001b[32m[I 2022-09-03 20:55:44,371]\u001b[0m Trial 57 finished with value: 0.2471882160282821 and parameters: {'lambda_l1': 0.0037418192212889086, 'lambda_l2': 1.9058195440332466e-07}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  75%|######################################2            | 15/20 [02:53<00:55, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211972\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.486655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  80%|########################################8          | 16/20 [03:04<00:44, 11.18s/it]\u001b[32m[I 2022-09-03 20:55:55,573]\u001b[0m Trial 58 finished with value: 0.24718820309569148 and parameters: {'lambda_l1': 6.2820601764772225e-06, 'lambda_l2': 7.746877643475893e-06}. Best is trial 50 with value: 0.24718820225249064.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  80%|########################################8          | 16/20 [03:05<00:44, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  85%|###########################################3       | 17/20 [03:16<00:33, 11.19s/it]\u001b[32m[I 2022-09-03 20:56:06,762]\u001b[0m Trial 59 finished with value: 0.2471882022480838 and parameters: {'lambda_l1': 1.2177856129380854e-07, 'lambda_l2': 1.097679939528439e-08}. Best is trial 59 with value: 0.2471882022480838.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  85%|###########################################3       | 17/20 [03:16<00:33, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  90%|#############################################9     | 18/20 [03:27<00:22, 11.15s/it]\u001b[32m[I 2022-09-03 20:56:17,830]\u001b[0m Trial 60 finished with value: 0.24718820225392032 and parameters: {'lambda_l1': 7.159758323246364e-07, 'lambda_l2': 1.0184399585145764e-08}. Best is trial 59 with value: 0.2471882022480838.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  90%|#############################################9     | 18/20 [03:27<00:22, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.490209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188:  95%|################################################4  | 19/20 [03:38<00:11, 11.14s/it]\u001b[32m[I 2022-09-03 20:56:28,962]\u001b[0m Trial 61 finished with value: 0.24718820643657494 and parameters: {'lambda_l1': 0.0009645572959548228, 'lambda_l2': 7.265743452862432e-06}. Best is trial 59 with value: 0.2471882022480838.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188:  95%|################################################4  | 19/20 [03:38<00:11, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211971\tvalid_1's binary_logloss: 0.247188\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.247188: 100%|###################################################| 20/20 [03:49<00:00, 11.12s/it]\u001b[32m[I 2022-09-03 20:56:40,028]\u001b[0m Trial 62 finished with value: 0.24718833484805164 and parameters: {'lambda_l1': 1.1730524405119643e-05, 'lambda_l2': 0.0012393629423176576}. Best is trial 59 with value: 0.2471882022480838.\u001b[0m\n",
      "regularization_factors, val_score: 0.247188: 100%|###################################################| 20/20 [03:49<00:00, 11.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.211973\tvalid_1's binary_logloss: 0.247188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.247188:   0%|                                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.247188:  20%|###########8                                               | 1/5 [00:09<00:38,  9.66s/it]\u001b[32m[I 2022-09-03 20:56:49,733]\u001b[0m Trial 63 finished with value: 0.2496379507661188 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.2496379507661188.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.247188:  20%|###########8                                               | 1/5 [00:09<00:38,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.220834\tvalid_1's binary_logloss: 0.249638\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.246540:  40%|#######################6                                   | 2/5 [00:21<00:33, 11.05s/it]\u001b[32m[I 2022-09-03 20:57:01,741]\u001b[0m Trial 64 finished with value: 0.24654005555563852 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 0.24654005555563852.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.246540:  40%|#######################6                                   | 2/5 [00:21<00:33, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's binary_logloss: 0.208945\tvalid_1's binary_logloss: 0.24654\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.246540:  60%|###################################4                       | 3/5 [00:32<00:22, 11.12s/it]\u001b[32m[I 2022-09-03 20:57:12,972]\u001b[0m Trial 65 finished with value: 0.24877084979630734 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.24654005555563852.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.246540:  60%|###################################4                       | 3/5 [00:32<00:22, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.211388\tvalid_1's binary_logloss: 0.248771\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.468214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.246540:  80%|###############################################2           | 4/5 [00:42<00:10, 10.71s/it]\u001b[32m[I 2022-09-03 20:57:23,036]\u001b[0m Trial 66 finished with value: 0.24954316802830698 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 0.24654005555563852.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.246540:  80%|###############################################2           | 4/5 [00:43<00:10, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.217224\tvalid_1's binary_logloss: 0.249543\n",
      "[LightGBM] [Info] Number of positive: 16700, number of negative: 222323\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57759\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 522\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069868 -> initscore=-2.588723\n",
      "[LightGBM] [Info] Start training from score -2.588723\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.246425: 100%|###########################################################| 5/5 [00:54<00:00, 10.84s/it]\u001b[32m[I 2022-09-03 20:57:34,136]\u001b[0m Trial 67 finished with value: 0.24642544895521692 and parameters: {'min_child_samples': 25}. Best is trial 67 with value: 0.24642544895521692.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.246425: 100%|###########################################################| 5/5 [00:54<00:00, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.212104\tvalid_1's binary_logloss: 0.246425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "lgb_train = lgb_o.Dataset(X_train1.values, y_train1.values)\n",
    "lgb_valid = lgb_o.Dataset(X_valid1.values, y_valid1.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04c350c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.0,\n",
       " 'lambda_l2': 0.0,\n",
       " 'num_leaves': 39,\n",
       " 'feature_fraction': 0.6,\n",
       " 'bagging_fraction': 0.695505240014514,\n",
       " 'bagging_freq': 7,\n",
       " 'min_child_samples': 25,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff605d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.695505240014514, subsample=1.0 will be ignored. Current value: bagging_fraction=0.695505240014514\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.695505240014514, bagging_freq=7,\n",
       "               feature_fraction=0.6, feature_pre_filter=False, lambda_l1=0.0,\n",
       "               lambda_l2=0.0, min_child_samples=25, num_iterations=1000,\n",
       "               num_leaves=39, objective='binary', random_state=100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 0.0,\n",
    " 'lambda_l2': 0.0,\n",
    " 'num_leaves': 39,\n",
    " 'feature_fraction': 0.6,\n",
    " 'bagging_fraction': 0.695505240014514,\n",
    " 'bagging_freq': 7,\n",
    " 'min_child_samples': 25,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "lgb_clf1 = lgb.LGBMClassifier(**params)\n",
    "lgb_clf1.fit(X_train1.values, y_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "206015c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM:\n",
      "\tAUC  : 0.773\n",
      "\tBrier: 0.062\n",
      "\n",
      "CalibratedLGBM:\n",
      "\tAUC  : 0.755\n",
      "\tBrier: 0.074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 20363 (\\N{CJK UNIFIED IDEOGRAPH-4F8B}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 20449 (\\N{CJK UNIFIED IDEOGRAPH-4FE1}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 38972 (\\N{CJK UNIFIED IDEOGRAPH-983C}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 26354 (\\N{CJK UNIFIED IDEOGRAPH-66F2}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 32218 (\\N{CJK UNIFIED IDEOGRAPH-7DDA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 23436 (\\N{CJK UNIFIED IDEOGRAPH-5B8C}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 20840 (\\N{CJK UNIFIED IDEOGRAPH-5168}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12394 (\\N{HIRAGANA LETTER NA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 35036 (\\N{CJK UNIFIED IDEOGRAPH-88DC}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 20104 (\\N{CJK UNIFIED IDEOGRAPH-4E88}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 28204 (\\N{CJK UNIFIED IDEOGRAPH-6E2C}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 20516 (\\N{CJK UNIFIED IDEOGRAPH-5024}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 22343 (\\N{CJK UNIFIED IDEOGRAPH-5747}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12469 (\\N{KATAKANA LETTER SA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12503 (\\N{KATAKANA LETTER PU}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/1644490106.py:58: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20363 (\\N{CJK UNIFIED IDEOGRAPH-4F8B}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20104 (\\N{CJK UNIFIED IDEOGRAPH-4E88}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 28204 (\\N{CJK UNIFIED IDEOGRAPH-6E2C}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20516 (\\N{CJK UNIFIED IDEOGRAPH-5024}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22343 (\\N{CJK UNIFIED IDEOGRAPH-5747}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12469 (\\N{KATAKANA LETTER SA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12503 (\\N{KATAKANA LETTER PU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 25968 (\\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20449 (\\N{CJK UNIFIED IDEOGRAPH-4FE1}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 38972 (\\N{CJK UNIFIED IDEOGRAPH-983C}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 26354 (\\N{CJK UNIFIED IDEOGRAPH-66F2}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 32218 (\\N{CJK UNIFIED IDEOGRAPH-7DDA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 23436 (\\N{CJK UNIFIED IDEOGRAPH-5B8C}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20840 (\\N{CJK UNIFIED IDEOGRAPH-5168}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 12394 (\\N{HIRAGANA LETTER NA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 35036 (\\N{CJK UNIFIED IDEOGRAPH-88DC}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "X_test2 = X_valid1.drop(['id', 'odds', 'time_odds'], axis=1)\n",
    "\n",
    "calib = CalibratedClassifierCV(lgb_clf1, cv=\"prefit\", method=\"isotonic\")\n",
    "calib.fit(X_train1.values, y_train1.values)\n",
    "plot_calibration_curve(dict(LGBM=lgb_clf1, CalibratedLGBM=calib), X_test2, y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48d3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "haitou = pd.read_csv('./csv_new2/race_detail.csv')\n",
    "haitou = haitou.set_index('race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8a8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f = gain(me.fukusho_return, X_test)\n",
    "# # tp = gain(me_valid.tansho_return_proper, X_valid1)\n",
    "# t = gain(me_valid.tansho_return, X_valid1)\n",
    "\n",
    "# # plt.plot(tp.index, tp['return_rate'], label='proper')\n",
    "# # plt.plot(t.index, t['return_rate'], label='tansho')\n",
    "# # plt.plot(f.index, f['return_rate'], label='fukusho')\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# t['return_rate'].rename('tansho').plot(legend=True)\n",
    "# # tp['return_rate'].rename('tansho_proper').plot(legend=True)\n",
    "# # f['return_rate'].rename('fukusho').plot(legend=True)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70b4aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_valid = TimeModel(model, valid)\n",
    "new_df = tm_valid.race_pred_time(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38261a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'haitou' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m me_valid \u001b[38;5;241m=\u001b[39m ModelEvaluator(lgb_clf1, \u001b[43mhaitou\u001b[49m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m me_valid\u001b[38;5;241m.\u001b[39mscore(y_valid1, X_valid1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'haitou' is not defined"
     ]
    }
   ],
   "source": [
    "me_valid = ModelEvaluator(lgb_clf1, haitou, std=True)\n",
    "me_valid.score(y_valid1, X_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ec168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "\n",
    "y_pred = me_valid.predict_proba(X_valid1).values\n",
    "\n",
    "jtplot.style(theme='monokai')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid1, y_pred)\n",
    "pit.plot(fpr, tpr, marker='o')\n",
    "pit.xlabel(\"False positive rate\")\n",
    "pit.ylabel(\"True positive rate\")\n",
    "pit.grid()\n",
    "pit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf9a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "me_valid.feature_importance(X_train1, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6775f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = me_valid.pred_table(X_valid1, 0.65)\n",
    "wr['expected'] = wr['win_ratio'] * wr['time_odds'] \n",
    "# wr['expected_2'] = wr['time_odds']  / (1.05 / wr['win_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "907c7aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>expected</th>\n",
       "      <th>grade</th>\n",
       "      <th>place_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>11</td>\n",
       "      <td>8.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018041509020807</td>\n",
       "      <td>9</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.133162</td>\n",
       "      <td>1.651205</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018041503010409</td>\n",
       "      <td>2</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.214619</td>\n",
       "      <td>1.459412</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018041503010403</td>\n",
       "      <td>9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.204336</td>\n",
       "      <td>1.532523</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018041503010408</td>\n",
       "      <td>15</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.140821</td>\n",
       "      <td>1.196977</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>2021041804010403</td>\n",
       "      <td>8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.200115</td>\n",
       "      <td>2.661534</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>2021042405020107</td>\n",
       "      <td>15</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.140671</td>\n",
       "      <td>1.477045</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>13</td>\n",
       "      <td>29.8</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>4.521706</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>2021042409020912</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.164153</td>\n",
       "      <td>1.214729</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>2021042405020112</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.180985</td>\n",
       "      <td>1.393587</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1422 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               race_id  h_num  odds  time_odds  win_ratio  expected  grade  \\\n",
       "3     2018041506030807     11   8.9       10.0   0.151353  1.513531      3   \n",
       "8     2018041509020807      9  12.8       12.4   0.133162  1.651205      3   \n",
       "13    2018041503010409      2   6.7        6.8   0.214619  1.459412      3   \n",
       "15    2018041503010403      9   6.3        7.5   0.204336  1.532523      2   \n",
       "16    2018041503010408     15   6.3        8.5   0.140821  1.196977      3   \n",
       "...                ...    ...   ...        ...        ...       ...    ...   \n",
       "5704  2021041804010403      8  11.8       13.3   0.200115  2.661534      2   \n",
       "5712  2021042405020107     15  10.9       10.5   0.140671  1.477045      3   \n",
       "5714  2021042405020111     13  29.8       27.8   0.162651  4.521706      7   \n",
       "5720  2021042409020912      4   6.1        7.4   0.164153  1.214729      4   \n",
       "5721  2021042405020112      9   6.0        7.7   0.180985  1.393587      4   \n",
       "\n",
       "      place_id  \n",
       "3            6  \n",
       "8            9  \n",
       "13           3  \n",
       "15           3  \n",
       "16           3  \n",
       "...        ...  \n",
       "5704         4  \n",
       "5712         5  \n",
       "5714         5  \n",
       "5720         9  \n",
       "5721         5  \n",
       "\n",
       "[1422 rows x 8 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_grade = pd.read_csv('./csv_new2/races.csv')\n",
    "race_grade = race_grade.set_index('race_id')\n",
    "race_grade[['grade']]\n",
    "\n",
    "bets = wr.merge(race_grade, on='race_id')\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/weathers.csv')\n",
    "bets = bets.merge(weather[['race_id', 'place_id']], on='race_id')\n",
    "\n",
    "# 賭ける馬\n",
    "bt = bets[\n",
    "#     ((bets['grade'] >= 2))\n",
    "#     & \n",
    "     (bets['expected'] >= 1)\n",
    "]\n",
    "# bt = bets\n",
    "\n",
    "bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "054d6d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27334127930075486"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 対象レース数\n",
    "race_count = len(\n",
    "    X_valid1\n",
    "    .groupby('race_id')\n",
    ")\n",
    "\n",
    "# 対象レースの出現頻度\n",
    "target_race_count = len(bets[(\n",
    "     (bets['expected'] >= 1)\n",
    ")].groupby('race_id'))\n",
    "\n",
    "target_race_count / race_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00999914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 賭ける馬の出現頻度\n",
    "len(bt) / len(bets[(\n",
    " (bets['expected'] >= 1)\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a7d35daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8070, 106.16033755274262, 0.11392405063291139, 162)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh = bt.merge(haitou, on='race_id')\n",
    "bh[bh['h_num'] == bh['1着馬番']]['単勝'].max(), (bh[bh['h_num'] == bh['1着馬番']]['単勝'].sum() / (len(bt) * 100)) * 100, len(bh[bh['h_num'] == bh['1着馬番']]) / (len(bt)), len(bh[bh['h_num'] == bh['1着馬番']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "29a4fc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(572900, 1161750.0, 84.50934749399869, 0.5279697388521132, 7258)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = fm.merge(haitou, on='race_id')\n",
    "\n",
    "money = 0\n",
    "f_c = 0\n",
    "for i in range(1, 5):\n",
    "    s = str(i)\n",
    "    f_c += len(h[h['h_num'] == h[s + '着馬番']]['複勝' + s])\n",
    "    money += h[h['h_num'] == h[s + '着馬番']]['複勝' + s].sum()\n",
    "\n",
    "(len(bets) * 100), money, (money / (len(fm) * 100)) * 100, f_c / len(fm), f_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "07d73688",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 複勝\n",
    "fukusho_allrace = pd.read_pickle('./pickle_new/base_race_20220813_6.pickle')\n",
    "fukusho_time_odds_base = pd.read_csv('./csv_new2/time_odds.csv')\n",
    "fukusho_allrace = fukusho_allrace.merge(fukusho_time_odds_base, how='left', on='id')\n",
    "\n",
    "fukusho_df = fukusho_allrace.query('course == 2')\n",
    "fukusho_all_r = preprocessing(fukusho_df)\n",
    "fukusho_all_r['result'] = fukusho_all_r['result'].map(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "fukusho_all_r.drop([\n",
    "  '気温', '風向', '風速', '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI', '4走前スピードZI', '5走前スピードZI',\n",
    "], axis=1, inplace=True)\n",
    "for i in range(1, 63):\n",
    "    fukusho_all_r.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "\n",
    "fukusho_categorical = process_categorical(fukusho_all_r, [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "fukusho_train1, fukusho_valid1 = split_data(fukusho_categorical)\n",
    "valid1, test2 = train_valid_split_data(valid1)\n",
    "\n",
    "f_target = pd.read_pickle('./pickle_new/new_race_20220904.pickle')\n",
    "time_odds = pd.read_csv('./csv_new2/20220904/time_odds.csv')\n",
    "f_target = f_target.merge(time_odds, how='left', on='id')\n",
    "\n",
    "f_target = f_target.query('course == 2')\n",
    "f_target = preprocessing(f_target)\n",
    "f_target['result'] = f_target['result'].map(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "f_target.drop([\n",
    "  '気温', '風向', '風速', '1走前着差', '2走前着差', '3走前着差', '4走前着差', '5走前着差',\n",
    "#   '1走前スピードZI','2走前スピードZI', '3走前スピードZI', '4走前スピードZI', '5走前スピードZI',\n",
    "  '先行指数', 'ペース指数', '上がり指数', 'スピード指数'\n",
    "], axis=1, inplace=True)\n",
    "for i in range(1, 63):\n",
    "    f_target.drop(['peds' + str(i)], axis=1, inplace=True)\n",
    "fukusho_test1 = process_categorical(f_target,  [\n",
    "    'producer', 'owner', 'training_course', \n",
    "    'jockey_id', 'gender', 'trainer_id', 'weight',\n",
    "    '天候', '馬場状態', 'grade', 'age', 'place_id',\n",
    "    'color_id', 'stallion_id', 'affiliation_id'\n",
    "])\n",
    "\n",
    "X_fukusho_train1 = fukusho_train1.drop(['id', 'date', 'result',  'time_popular', 'time_odds', 'odds', 'popular', 'horse_id'], axis=1)\n",
    "y_fukusho_train1 = fukusho_train1['result']\n",
    "X_fukusho_valid1 = fukusho_valid1.drop(['date', 'result', 'popular',  'time_popular', 'horse_id'], axis=1)\n",
    "y_fukusho_valid1 = fukusho_valid1['result']\n",
    "X_fukusho_test1 = fukusho_test1.drop(['date', 'result', 'popular',  'time_popular', 'horse_id'], axis=1)\n",
    "y_fukusho_test1 = fukusho_test1['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10dbba4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-01 20:14:02,059]\u001b[0m A new study created in memory with name: no-name-fe7d8cd4-8f8d-4c78-9e81-b4bf1875ec9f\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                                                                                             | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.513717:  14%|###########4                                                                    | 1/7 [00:11<01:08, 11.45s/it]\u001b[32m[I 2022-09-01 20:14:13,548]\u001b[0m Trial 0 finished with value: 0.513717336448457 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.513717336448457.\u001b[0m\n",
      "feature_fraction, val_score: 0.513717:  14%|###########4                                                                    | 1/7 [00:11<01:08, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.490745\tvalid_1's binary_logloss: 0.513717\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.513717:  29%|######################8                                                         | 2/7 [00:20<00:50, 10.16s/it]\u001b[32m[I 2022-09-01 20:14:22,805]\u001b[0m Trial 1 finished with value: 0.5186505365227324 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.513717336448457.\u001b[0m\n",
      "feature_fraction, val_score: 0.513717:  29%|######################8                                                         | 2/7 [00:20<00:50, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.502915\tvalid_1's binary_logloss: 0.518651\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.510842:  43%|##################################2                                             | 3/7 [00:33<00:45, 11.32s/it]\u001b[32m[I 2022-09-01 20:14:35,513]\u001b[0m Trial 2 finished with value: 0.5108422205199515 and parameters: {'feature_fraction': 1.0}. Best is trial 2 with value: 0.5108422205199515.\u001b[0m\n",
      "feature_fraction, val_score: 0.510842:  43%|##################################2                                             | 3/7 [00:33<00:45, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.441014\tvalid_1's binary_logloss: 0.510842\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.597395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.510842:  57%|#############################################7                                  | 4/7 [00:44<00:33, 11.31s/it]\u001b[32m[I 2022-09-01 20:14:46,798]\u001b[0m Trial 3 finished with value: 0.5137172737057416 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.5108422205199515.\u001b[0m\n",
      "feature_fraction, val_score: 0.510842:  57%|#############################################7                                  | 4/7 [00:44<00:33, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.490787\tvalid_1's binary_logloss: 0.513717\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.509792:  71%|#########################################################1                      | 5/7 [00:58<00:24, 12.14s/it]\u001b[32m[I 2022-09-01 20:15:00,402]\u001b[0m Trial 4 finished with value: 0.5097924394029199 and parameters: {'feature_fraction': 0.8}. Best is trial 4 with value: 0.5097924394029199.\u001b[0m\n",
      "feature_fraction, val_score: 0.509792:  71%|#########################################################1                      | 5/7 [00:58<00:24, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.447324\tvalid_1's binary_logloss: 0.509792\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.509326:  86%|####################################################################5           | 6/7 [01:09<00:11, 11.82s/it]\u001b[32m[I 2022-09-01 20:15:11,599]\u001b[0m Trial 5 finished with value: 0.5093255856393166 and parameters: {'feature_fraction': 0.5}. Best is trial 5 with value: 0.5093255856393166.\u001b[0m\n",
      "feature_fraction, val_score: 0.509326:  86%|####################################################################5           | 6/7 [01:09<00:11, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.44843\tvalid_1's binary_logloss: 0.509326\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.509281: 100%|################################################################################| 7/7 [01:23<00:00, 12.38s/it]\u001b[32m[I 2022-09-01 20:15:25,137]\u001b[0m Trial 6 finished with value: 0.5092814727495577 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 6 with value: 0.5092814727495577.\u001b[0m\n",
      "feature_fraction, val_score: 0.509281: 100%|################################################################################| 7/7 [01:23<00:00, 11.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.439401\tvalid_1's binary_logloss: 0.509281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.509281:   0%|                                                                                             | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.509281:   5%|####2                                                                                | 1/20 [00:14<04:38, 14.66s/it]\u001b[32m[I 2022-09-01 20:15:39,852]\u001b[0m Trial 7 finished with value: 0.5098812621285677 and parameters: {'num_leaves': 28}. Best is trial 7 with value: 0.5098812621285677.\u001b[0m\n",
      "num_leaves, val_score: 0.509281:   5%|####2                                                                                | 1/20 [00:14<04:38, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.433923\tvalid_1's binary_logloss: 0.509881\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.509281:  10%|########5                                                                            | 2/20 [00:30<04:40, 15.59s/it]\u001b[32m[I 2022-09-01 20:15:56,079]\u001b[0m Trial 8 finished with value: 0.509610607628214 and parameters: {'num_leaves': 191}. Best is trial 8 with value: 0.509610607628214.\u001b[0m\n",
      "num_leaves, val_score: 0.509281:  10%|########5                                                                            | 2/20 [00:30<04:40, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.41808\tvalid_1's binary_logloss: 0.509611\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.509281:  15%|############7                                                                        | 3/20 [00:43<04:01, 14.19s/it]\u001b[32m[I 2022-09-01 20:16:08,583]\u001b[0m Trial 9 finished with value: 0.5133942115839252 and parameters: {'num_leaves': 110}. Best is trial 8 with value: 0.509610607628214.\u001b[0m\n",
      "num_leaves, val_score: 0.509281:  15%|############7                                                                        | 3/20 [00:43<04:01, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.448762\tvalid_1's binary_logloss: 0.513394\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.509281:  20%|#################                                                                    | 4/20 [00:58<03:52, 14.51s/it]\u001b[32m[I 2022-09-01 20:16:23,619]\u001b[0m Trial 10 finished with value: 0.511625377123042 and parameters: {'num_leaves': 142}. Best is trial 8 with value: 0.509610607628214.\u001b[0m\n",
      "num_leaves, val_score: 0.509281:  20%|#################                                                                    | 4/20 [00:58<03:52, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.4254\tvalid_1's binary_logloss: 0.511625\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  25%|#####################2                                                               | 5/20 [01:20<04:18, 17.21s/it]\u001b[32m[I 2022-09-01 20:16:45,601]\u001b[0m Trial 11 finished with value: 0.5083506968653129 and parameters: {'num_leaves': 147}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  25%|#####################2                                                               | 5/20 [01:20<04:18, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.414063\tvalid_1's binary_logloss: 0.508351\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  30%|#########################5                                                           | 6/20 [01:39<04:09, 17.83s/it]\u001b[32m[I 2022-09-01 20:17:04,614]\u001b[0m Trial 12 finished with value: 0.512415153118933 and parameters: {'num_leaves': 248}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  30%|#########################5                                                           | 6/20 [01:39<04:09, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.412239\tvalid_1's binary_logloss: 0.512415\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  35%|#############################7                                                       | 7/20 [01:54<03:38, 16.82s/it]\u001b[32m[I 2022-09-01 20:17:19,376]\u001b[0m Trial 13 finished with value: 0.5125285043945405 and parameters: {'num_leaves': 81}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  35%|#############################7                                                       | 7/20 [01:54<03:38, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.441259\tvalid_1's binary_logloss: 0.512529\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  40%|##################################                                                   | 8/20 [02:11<03:21, 16.83s/it]\u001b[32m[I 2022-09-01 20:17:36,219]\u001b[0m Trial 14 finished with value: 0.5134600654107992 and parameters: {'num_leaves': 229}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  40%|##################################                                                   | 8/20 [02:11<03:21, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.419328\tvalid_1's binary_logloss: 0.51346\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  45%|######################################2                                              | 9/20 [02:24<02:52, 15.69s/it]\u001b[32m[I 2022-09-01 20:17:49,397]\u001b[0m Trial 15 finished with value: 0.5102883667844553 and parameters: {'num_leaves': 5}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  45%|######################################2                                              | 9/20 [02:24<02:52, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.447925\tvalid_1's binary_logloss: 0.510288\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  50%|##########################################                                          | 10/20 [02:47<03:00, 18.03s/it]\u001b[32m[I 2022-09-01 20:18:12,681]\u001b[0m Trial 16 finished with value: 0.5111698353327816 and parameters: {'num_leaves': 240}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  50%|##########################################                                          | 10/20 [02:47<03:00, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's binary_logloss: 0.394085\tvalid_1's binary_logloss: 0.51117\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508351:  55%|##############################################2                                     | 11/20 [03:03<02:36, 17.42s/it]\u001b[32m[I 2022-09-01 20:18:28,715]\u001b[0m Trial 17 finished with value: 0.5124394794722523 and parameters: {'num_leaves': 162}. Best is trial 11 with value: 0.5083506968653129.\u001b[0m\n",
      "num_leaves, val_score: 0.508351:  55%|##############################################2                                     | 11/20 [03:03<02:36, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.441127\tvalid_1's binary_logloss: 0.512439\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.508228:  60%|##################################################4                                 | 12/20 [03:25<02:29, 18.69s/it]\u001b[32m[I 2022-09-01 20:18:50,292]\u001b[0m Trial 18 finished with value: 0.508228029508821 and parameters: {'num_leaves': 177}. Best is trial 18 with value: 0.508228029508821.\u001b[0m\n",
      "num_leaves, val_score: 0.508228:  60%|##################################################4                                 | 12/20 [03:25<02:29, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's binary_logloss: 0.396277\tvalid_1's binary_logloss: 0.508228\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  65%|######################################################6                             | 13/20 [03:46<02:17, 19.62s/it]\u001b[32m[I 2022-09-01 20:19:12,079]\u001b[0m Trial 19 finished with value: 0.5072709960385094 and parameters: {'num_leaves': 195}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  65%|######################################################6                             | 13/20 [03:46<02:17, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.394409\tvalid_1's binary_logloss: 0.507271\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  70%|##########################################################8                         | 14/20 [04:04<01:54, 19.12s/it]\u001b[32m[I 2022-09-01 20:19:30,021]\u001b[0m Trial 20 finished with value: 0.5103204458240879 and parameters: {'num_leaves': 202}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  70%|##########################################################8                         | 14/20 [04:04<01:54, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.416925\tvalid_1's binary_logloss: 0.51032\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  75%|###############################################################                     | 15/20 [04:20<01:30, 18.20s/it]\u001b[32m[I 2022-09-01 20:19:46,085]\u001b[0m Trial 21 finished with value: 0.5111318840808952 and parameters: {'num_leaves': 188}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  75%|###############################################################                     | 15/20 [04:20<01:30, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.439346\tvalid_1's binary_logloss: 0.511132\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  80%|###################################################################2                | 16/20 [04:38<01:11, 17.96s/it]\u001b[32m[I 2022-09-01 20:20:03,499]\u001b[0m Trial 22 finished with value: 0.5076805466233185 and parameters: {'num_leaves': 102}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  80%|###################################################################2                | 16/20 [04:38<01:11, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's binary_logloss: 0.41908\tvalid_1's binary_logloss: 0.507681\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  85%|#######################################################################3            | 17/20 [04:53<00:51, 17.02s/it]\u001b[32m[I 2022-09-01 20:20:18,369]\u001b[0m Trial 23 finished with value: 0.5080693996806439 and parameters: {'num_leaves': 88}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  85%|#######################################################################3            | 17/20 [04:53<00:51, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.433462\tvalid_1's binary_logloss: 0.508069\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  90%|###########################################################################6        | 18/20 [05:07<00:32, 16.11s/it]\u001b[32m[I 2022-09-01 20:20:32,314]\u001b[0m Trial 24 finished with value: 0.5101989233227411 and parameters: {'num_leaves': 67}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  90%|###########################################################################6        | 18/20 [05:07<00:32, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's binary_logloss: 0.445249\tvalid_1's binary_logloss: 0.510199\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271:  95%|###############################################################################8    | 19/20 [05:20<00:15, 15.43s/it]\u001b[32m[I 2022-09-01 20:20:46,158]\u001b[0m Trial 25 finished with value: 0.513819612745266 and parameters: {'num_leaves': 112}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271:  95%|###############################################################################8    | 19/20 [05:21<00:15, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.448601\tvalid_1's binary_logloss: 0.51382\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.507271: 100%|####################################################################################| 20/20 [05:34<00:00, 14.91s/it]\u001b[32m[I 2022-09-01 20:20:59,885]\u001b[0m Trial 26 finished with value: 0.5083413109839541 and parameters: {'num_leaves': 42}. Best is trial 19 with value: 0.5072709960385094.\u001b[0m\n",
      "num_leaves, val_score: 0.507271: 100%|####################################################################################| 20/20 [05:34<00:00, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.439073\tvalid_1's binary_logloss: 0.508341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:   0%|                                                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  10%|########8                                                                               | 1/10 [00:13<01:58, 13.16s/it]\u001b[32m[I 2022-09-01 20:21:13,091]\u001b[0m Trial 27 finished with value: 0.5115778878994818 and parameters: {'bagging_fraction': 0.6919179835320033, 'bagging_freq': 1}. Best is trial 27 with value: 0.5115778878994818.\u001b[0m\n",
      "bagging, val_score: 0.507271:  10%|########8                                                                               | 1/10 [00:13<01:58, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.467308\tvalid_1's binary_logloss: 0.511578\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  20%|#################6                                                                      | 2/10 [00:26<01:45, 13.13s/it]\u001b[32m[I 2022-09-01 20:21:26,177]\u001b[0m Trial 28 finished with value: 0.5085657576834509 and parameters: {'bagging_fraction': 0.4213621400509774, 'bagging_freq': 7}. Best is trial 28 with value: 0.5085657576834509.\u001b[0m\n",
      "bagging, val_score: 0.507271:  20%|#################6                                                                      | 2/10 [00:26<01:45, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.444189\tvalid_1's binary_logloss: 0.508566\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  30%|##########################4                                                             | 3/10 [00:41<01:38, 14.11s/it]\u001b[32m[I 2022-09-01 20:21:41,453]\u001b[0m Trial 29 finished with value: 0.5103340881937688 and parameters: {'bagging_fraction': 0.49117810473185625, 'bagging_freq': 1}. Best is trial 28 with value: 0.5085657576834509.\u001b[0m\n",
      "bagging, val_score: 0.507271:  30%|##########################4                                                             | 3/10 [00:41<01:38, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.416046\tvalid_1's binary_logloss: 0.510334\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  40%|###################################2                                                    | 4/10 [00:57<01:29, 14.84s/it]\u001b[32m[I 2022-09-01 20:21:57,421]\u001b[0m Trial 30 finished with value: 0.5080751807103463 and parameters: {'bagging_fraction': 0.7039345594666155, 'bagging_freq': 6}. Best is trial 30 with value: 0.5080751807103463.\u001b[0m\n",
      "bagging, val_score: 0.507271:  40%|###################################2                                                    | 4/10 [00:57<01:29, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_logloss: 0.413435\tvalid_1's binary_logloss: 0.508075\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  50%|############################################                                            | 5/10 [01:15<01:20, 16.02s/it]\u001b[32m[I 2022-09-01 20:22:15,515]\u001b[0m Trial 31 finished with value: 0.5104948372707663 and parameters: {'bagging_fraction': 0.7920036584552059, 'bagging_freq': 4}. Best is trial 30 with value: 0.5080751807103463.\u001b[0m\n",
      "bagging, val_score: 0.507271:  50%|############################################                                            | 5/10 [01:15<01:20, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.415517\tvalid_1's binary_logloss: 0.510495\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  60%|####################################################8                                   | 6/10 [01:32<01:04, 16.21s/it]\u001b[32m[I 2022-09-01 20:22:32,098]\u001b[0m Trial 32 finished with value: 0.5129103274169774 and parameters: {'bagging_fraction': 0.8775206478073929, 'bagging_freq': 2}. Best is trial 30 with value: 0.5080751807103463.\u001b[0m\n",
      "bagging, val_score: 0.507271:  60%|####################################################8                                   | 6/10 [01:32<01:04, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.428123\tvalid_1's binary_logloss: 0.51291\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  70%|#############################################################6                          | 7/10 [01:46<00:46, 15.62s/it]\u001b[32m[I 2022-09-01 20:22:46,512]\u001b[0m Trial 33 finished with value: 0.5115802699253647 and parameters: {'bagging_fraction': 0.6535284404272015, 'bagging_freq': 4}. Best is trial 30 with value: 0.5080751807103463.\u001b[0m\n",
      "bagging, val_score: 0.507271:  70%|#############################################################6                          | 7/10 [01:46<00:46, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.431677\tvalid_1's binary_logloss: 0.51158\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.507271:  80%|######################################################################4                 | 8/10 [01:58<00:28, 14.49s/it]\u001b[32m[I 2022-09-01 20:22:58,578]\u001b[0m Trial 34 finished with value: 0.5144231041401122 and parameters: {'bagging_fraction': 0.4149595854970872, 'bagging_freq': 2}. Best is trial 30 with value: 0.5080751807103463.\u001b[0m\n",
      "bagging, val_score: 0.507271:  80%|######################################################################4                 | 8/10 [01:58<00:28, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.448641\tvalid_1's binary_logloss: 0.514423\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.504098:  90%|###############################################################################2        | 9/10 [02:15<00:15, 15.18s/it]\u001b[32m[I 2022-09-01 20:23:15,288]\u001b[0m Trial 35 finished with value: 0.5040983284137168 and parameters: {'bagging_fraction': 0.7957269767931685, 'bagging_freq': 5}. Best is trial 35 with value: 0.5040983284137168.\u001b[0m\n",
      "bagging, val_score: 0.504098:  90%|###############################################################################2        | 9/10 [02:15<00:15, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.422495\tvalid_1's binary_logloss: 0.504098\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.504098: 100%|#######################################################################################| 10/10 [02:30<00:00, 15.17s/it]\u001b[32m[I 2022-09-01 20:23:30,419]\u001b[0m Trial 36 finished with value: 0.5127588538728233 and parameters: {'bagging_fraction': 0.8724058695421857, 'bagging_freq': 5}. Best is trial 35 with value: 0.5040983284137168.\u001b[0m\n",
      "bagging, val_score: 0.504098: 100%|#######################################################################################| 10/10 [02:30<00:00, 15.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.448006\tvalid_1's binary_logloss: 0.512759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.504098:   0%|                                                                                 | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.504098:  17%|############1                                                            | 1/6 [00:16<01:21, 16.22s/it]\u001b[32m[I 2022-09-01 20:23:46,674]\u001b[0m Trial 37 finished with value: 0.510522766834955 and parameters: {'feature_fraction': 0.948}. Best is trial 37 with value: 0.510522766834955.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.504098:  17%|############1                                                            | 1/6 [00:16<01:21, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.42808\tvalid_1's binary_logloss: 0.510523\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.504098:  33%|########################3                                                | 2/6 [00:35<01:12, 18.13s/it]\u001b[32m[I 2022-09-01 20:24:06,138]\u001b[0m Trial 38 finished with value: 0.505884626368506 and parameters: {'feature_fraction': 0.852}. Best is trial 38 with value: 0.505884626368506.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.504098:  33%|########################3                                                | 2/6 [00:35<01:12, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.409568\tvalid_1's binary_logloss: 0.505885\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.504098:  50%|####################################5                                    | 3/6 [00:56<00:58, 19.33s/it]\u001b[32m[I 2022-09-01 20:24:26,915]\u001b[0m Trial 39 finished with value: 0.5046115166160988 and parameters: {'feature_fraction': 0.9159999999999999}. Best is trial 39 with value: 0.5046115166160988.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.504098:  50%|####################################5                                    | 3/6 [00:56<00:58, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.402508\tvalid_1's binary_logloss: 0.504612\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.502747:  67%|################################################6                        | 4/6 [01:13<00:37, 18.52s/it]\u001b[32m[I 2022-09-01 20:24:44,174]\u001b[0m Trial 40 finished with value: 0.5027470298531421 and parameters: {'feature_fraction': 0.82}. Best is trial 40 with value: 0.5027470298531421.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.502747:  67%|################################################6                        | 4/6 [01:13<00:37, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.502747\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.502747:  83%|############################################################8            | 5/6 [01:31<00:18, 18.28s/it]\u001b[32m[I 2022-09-01 20:25:02,031]\u001b[0m Trial 41 finished with value: 0.5050628799987809 and parameters: {'feature_fraction': 0.8839999999999999}. Best is trial 40 with value: 0.5027470298531421.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.502747:  83%|############################################################8            | 5/6 [01:31<00:18, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.417653\tvalid_1's binary_logloss: 0.505063\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.502747: 100%|#########################################################################| 6/6 [01:45<00:00, 16.94s/it]\u001b[32m[I 2022-09-01 20:25:16,366]\u001b[0m Trial 42 finished with value: 0.5144353347612268 and parameters: {'feature_fraction': 0.9799999999999999}. Best is trial 40 with value: 0.5027470298531421.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.502747: 100%|#########################################################################| 6/6 [01:45<00:00, 17.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.485609\tvalid_1's binary_logloss: 0.514435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:   0%|                                                                                 | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:   5%|###6                                                                     | 1/20 [00:17<05:38, 17.79s/it]\u001b[32m[I 2022-09-01 20:25:34,190]\u001b[0m Trial 43 finished with value: 0.5033460133945445 and parameters: {'lambda_l1': 2.5292710162333683e-07, 'lambda_l2': 5.950382282610973e-05}. Best is trial 43 with value: 0.5033460133945445.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:   5%|###6                                                                     | 1/20 [00:17<05:38, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.503346\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  10%|#######3                                                                 | 2/20 [00:35<05:19, 17.75s/it]\u001b[32m[I 2022-09-01 20:25:51,925]\u001b[0m Trial 44 finished with value: 0.5037512793541132 and parameters: {'lambda_l1': 1.595278290298975e-05, 'lambda_l2': 0.01206127012241186}. Best is trial 43 with value: 0.5033460133945445.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  10%|#######3                                                                 | 2/20 [00:35<05:19, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.420472\tvalid_1's binary_logloss: 0.503751\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  15%|##########9                                                              | 3/20 [00:53<05:00, 17.69s/it]\u001b[32m[I 2022-09-01 20:26:09,544]\u001b[0m Trial 45 finished with value: 0.5127925219681941 and parameters: {'lambda_l1': 1.7108464938032767e-06, 'lambda_l2': 9.126042529502053}. Best is trial 43 with value: 0.5033460133945445.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  15%|##########9                                                              | 3/20 [00:53<05:00, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.423583\tvalid_1's binary_logloss: 0.512793\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  20%|##############6                                                          | 4/20 [01:11<04:47, 17.94s/it]\u001b[32m[I 2022-09-01 20:26:27,868]\u001b[0m Trial 46 finished with value: 0.5033375567738972 and parameters: {'lambda_l1': 0.0001229356060860523, 'lambda_l2': 2.298485502737746e-05}. Best is trial 46 with value: 0.5033375567738972.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  20%|##############6                                                          | 4/20 [01:11<04:47, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.503338\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  25%|##################2                                                      | 5/20 [01:29<04:27, 17.84s/it]\u001b[32m[I 2022-09-01 20:26:45,528]\u001b[0m Trial 47 finished with value: 0.5033375178750396 and parameters: {'lambda_l1': 2.0546389792123013e-08, 'lambda_l2': 6.111783749727872e-07}. Best is trial 47 with value: 0.5033375178750396.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  25%|##################2                                                      | 5/20 [01:29<04:27, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.503338\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  30%|#####################9                                                   | 6/20 [01:52<04:37, 19.84s/it]\u001b[32m[I 2022-09-01 20:27:09,232]\u001b[0m Trial 48 finished with value: 0.5048365859720667 and parameters: {'lambda_l1': 0.026617150273791815, 'lambda_l2': 2.3228206967588774}. Best is trial 47 with value: 0.5033375178750396.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  30%|#####################9                                                   | 6/20 [01:52<04:37, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.393155\tvalid_1's binary_logloss: 0.504837\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.588687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  35%|#########################5                                               | 7/20 [02:10<04:08, 19.09s/it]\u001b[32m[I 2022-09-01 20:27:26,807]\u001b[0m Trial 49 finished with value: 0.5035016186179856 and parameters: {'lambda_l1': 9.474078218457904e-06, 'lambda_l2': 8.133912937057684e-06}. Best is trial 47 with value: 0.5033375178750396.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  35%|#########################5                                               | 7/20 [02:10<04:08, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.503502\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  40%|#############################2                                           | 8/20 [02:27<03:40, 18.36s/it]\u001b[32m[I 2022-09-01 20:27:43,591]\u001b[0m Trial 50 finished with value: 0.5037514014450059 and parameters: {'lambda_l1': 0.0004924179447283676, 'lambda_l2': 0.011142943446306773}. Best is trial 47 with value: 0.5033375178750396.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  40%|#############################2                                           | 8/20 [02:27<03:40, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.420474\tvalid_1's binary_logloss: 0.503751\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  45%|################################8                                        | 9/20 [02:44<03:19, 18.15s/it]\u001b[32m[I 2022-09-01 20:28:01,276]\u001b[0m Trial 51 finished with value: 0.5031660004103177 and parameters: {'lambda_l1': 0.026189714822992073, 'lambda_l2': 1.2422213220878395e-07}. Best is trial 51 with value: 0.5031660004103177.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  45%|################################8                                        | 9/20 [02:44<03:19, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425674\tvalid_1's binary_logloss: 0.503166\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  50%|####################################                                    | 10/20 [03:02<03:00, 18.01s/it]\u001b[32m[I 2022-09-01 20:28:18,977]\u001b[0m Trial 52 finished with value: 0.5033378455633104 and parameters: {'lambda_l1': 2.3303815701157197e-05, 'lambda_l2': 0.0002478270552074194}. Best is trial 51 with value: 0.5031660004103177.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  50%|####################################                                    | 10/20 [03:02<03:00, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425472\tvalid_1's binary_logloss: 0.503338\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  55%|#######################################6                                | 11/20 [03:15<02:28, 16.47s/it]\u001b[32m[I 2022-09-01 20:28:31,970]\u001b[0m Trial 53 finished with value: 0.516278127983753 and parameters: {'lambda_l1': 8.664666868058164, 'lambda_l2': 1.9911435202658593e-08}. Best is trial 51 with value: 0.5031660004103177.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  55%|#######################################6                                | 11/20 [03:15<02:28, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.486851\tvalid_1's binary_logloss: 0.516278\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  60%|###########################################1                            | 12/20 [03:31<02:11, 16.43s/it]\u001b[32m[I 2022-09-01 20:28:48,306]\u001b[0m Trial 54 finished with value: 0.5033375174382718 and parameters: {'lambda_l1': 1.012824001509964e-08, 'lambda_l2': 5.73942625444724e-08}. Best is trial 51 with value: 0.5031660004103177.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  60%|###########################################1                            | 12/20 [03:31<02:11, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425471\tvalid_1's binary_logloss: 0.503338\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  65%|##############################################8                         | 13/20 [03:48<01:56, 16.60s/it]\u001b[32m[I 2022-09-01 20:29:05,295]\u001b[0m Trial 55 finished with value: 0.5027848820346558 and parameters: {'lambda_l1': 0.02204047077752543, 'lambda_l2': 1.0553513317479858e-08}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  65%|##############################################8                         | 13/20 [03:48<01:56, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425671\tvalid_1's binary_logloss: 0.502785\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  70%|##################################################4                     | 14/20 [04:05<01:40, 16.73s/it]\u001b[32m[I 2022-09-01 20:29:22,315]\u001b[0m Trial 56 finished with value: 0.5059320946611882 and parameters: {'lambda_l1': 0.062209154142578243, 'lambda_l2': 5.499624038253295e-07}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  70%|##################################################4                     | 14/20 [04:05<01:40, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425628\tvalid_1's binary_logloss: 0.505932\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  75%|######################################################                  | 15/20 [04:23<01:24, 16.90s/it]\u001b[32m[I 2022-09-01 20:29:39,615]\u001b[0m Trial 57 finished with value: 0.5052474183770281 and parameters: {'lambda_l1': 0.07732282066716559, 'lambda_l2': 1.0670042108147927e-08}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  75%|######################################################                  | 15/20 [04:23<01:24, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.420363\tvalid_1's binary_logloss: 0.505247\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  80%|#########################################################6              | 16/20 [04:41<01:09, 17.40s/it]\u001b[32m[I 2022-09-01 20:29:58,186]\u001b[0m Trial 58 finished with value: 0.5035953015976662 and parameters: {'lambda_l1': 0.0032405810819700416, 'lambda_l2': 5.79130338251426e-07}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  80%|#########################################################6              | 16/20 [04:41<01:09, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425444\tvalid_1's binary_logloss: 0.503595\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  85%|#############################################################2          | 17/20 [05:00<00:53, 17.88s/it]\u001b[32m[I 2022-09-01 20:30:17,168]\u001b[0m Trial 59 finished with value: 0.5092539975734806 and parameters: {'lambda_l1': 1.3602360829921745, 'lambda_l2': 2.6747348258547736e-06}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  85%|#############################################################2          | 17/20 [05:00<00:53, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.421354\tvalid_1's binary_logloss: 0.509254\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  90%|################################################################8       | 18/20 [05:18<00:35, 17.83s/it]\u001b[32m[I 2022-09-01 20:30:34,878]\u001b[0m Trial 60 finished with value: 0.5058201382393588 and parameters: {'lambda_l1': 0.004656736058633862, 'lambda_l2': 8.853716061147077e-08}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  90%|################################################################8       | 18/20 [05:18<00:35, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.42031\tvalid_1's binary_logloss: 0.50582\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747:  95%|####################################################################3   | 19/20 [05:42<00:19, 19.63s/it]\u001b[32m[I 2022-09-01 20:30:58,715]\u001b[0m Trial 61 finished with value: 0.5030762126952671 and parameters: {'lambda_l1': 0.4167490008393923, 'lambda_l2': 0.00766697144932622}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747:  95%|####################################################################3   | 19/20 [05:42<00:19, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.39864\tvalid_1's binary_logloss: 0.503076\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.502747: 100%|########################################################################| 20/20 [06:01<00:00, 19.36s/it]\u001b[32m[I 2022-09-01 20:31:17,445]\u001b[0m Trial 62 finished with value: 0.506614053015785 and parameters: {'lambda_l1': 0.7686714187552722, 'lambda_l2': 0.009439044250746353}. Best is trial 55 with value: 0.5027848820346558.\u001b[0m\n",
      "regularization_factors, val_score: 0.502747: 100%|########################################################################| 20/20 [06:01<00:00, 18.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.420973\tvalid_1's binary_logloss: 0.506614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747:   0%|                                                                                        | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747:  20%|################                                                                | 1/5 [00:16<01:05, 16.29s/it]\u001b[32m[I 2022-09-01 20:31:33,769]\u001b[0m Trial 63 finished with value: 0.5100051033038155 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.5100051033038155.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502747:  20%|################                                                                | 1/5 [00:16<01:05, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.425971\tvalid_1's binary_logloss: 0.510005\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747:  40%|################################                                                | 2/5 [00:35<00:54, 18.01s/it]\u001b[32m[I 2022-09-01 20:31:52,988]\u001b[0m Trial 64 finished with value: 0.5079981745810838 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.5079981745810838.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502747:  40%|################################                                                | 2/5 [00:35<00:54, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's binary_logloss: 0.402976\tvalid_1's binary_logloss: 0.507998\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747:  60%|################################################                                | 3/5 [00:50<00:33, 16.80s/it]\u001b[32m[I 2022-09-01 20:32:08,349]\u001b[0m Trial 65 finished with value: 0.5035479989142189 and parameters: {'min_child_samples': 10}. Best is trial 65 with value: 0.5035479989142189.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502747:  60%|################################################                                | 3/5 [00:50<00:33, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_logloss: 0.431727\tvalid_1's binary_logloss: 0.503548\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747:  80%|################################################################                | 4/5 [01:08<00:16, 16.99s/it]\u001b[32m[I 2022-09-01 20:32:25,626]\u001b[0m Trial 66 finished with value: 0.5122369495927748 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.5035479989142189.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502747:  80%|################################################################                | 4/5 [01:08<00:16, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's binary_logloss: 0.41892\tvalid_1's binary_logloss: 0.512237\n",
      "[LightGBM] [Info] Number of positive: 50122, number of negative: 188901\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 59848\n",
      "[LightGBM] [Info] Number of data points in the train set: 239023, number of used features: 537\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.209695 -> initscore=-1.326763\n",
      "[LightGBM] [Info] Start training from score -1.326763\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.502747: 100%|################################################################################| 5/5 [01:22<00:00, 16.15s/it]\u001b[32m[I 2022-09-01 20:32:40,284]\u001b[0m Trial 67 finished with value: 0.5067713698179988 and parameters: {'min_child_samples': 5}. Best is trial 65 with value: 0.5035479989142189.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.502747: 100%|################################################################################| 5/5 [01:22<00:00, 16.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.439064\tvalid_1's binary_logloss: 0.506771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "f_lgb_train = lgb_o.Dataset(X_fukusho_train1.values, y_fukusho_train1.values)\n",
    "f_lgb_valid = lgb_o.Dataset(X_fukusho_valid1.values, y_fukusho_valid1.values)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "f_lgb_clf_o = lgb_o.train(params, f_lgb_train, valid_sets=(f_lgb_train, f_lgb_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c07608b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'random_state': 100,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.0,\n",
       " 'lambda_l2': 0.0,\n",
       " 'num_leaves': 195,\n",
       " 'feature_fraction': 0.82,\n",
       " 'bagging_fraction': 0.7957269767931685,\n",
       " 'bagging_freq': 5,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 1000,\n",
       " 'early_stopping_round': 10}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae79cfdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimizukeita/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8030314072629412, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8030314072629412\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0008794517970418271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0008794517970418271\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.761229882619368e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.761229882619368e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.8030314072629412, bagging_freq=5,\n",
       "               feature_fraction=0.5, feature_pre_filter=False,\n",
       "               lambda_l1=5.761229882619368e-06, lambda_l2=0.0008794517970418271,\n",
       "               num_iterations=1000, num_leaves=73, objective='binary',\n",
       "               random_state=100)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 5.761229882619368e-06,\n",
    " 'lambda_l2': 0.0008794517970418271,\n",
    " 'num_leaves': 73,\n",
    " 'feature_fraction': 0.5,\n",
    " 'bagging_fraction': 0.8030314072629412,\n",
    " 'bagging_freq': 5,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 1000,\n",
    "}\n",
    "\n",
    "f_lgb_clf = lgb.LGBMClassifier(**params)\n",
    "f_lgb_clf.fit(X_fukusho_train1.values, y_fukusho_train1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32d6cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7646403402875598"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_me_valid = ModelEvaluator(f_lgb_clf, haitou, std=True)\n",
    "f_me_valid.score(y_fukusho_valid1, X_fukusho_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a729c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_num</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>expected_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018041409020710</th>\n",
       "      <td>12</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.137220</td>\n",
       "      <td>0.384215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018041503010401</th>\n",
       "      <td>14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.134265</td>\n",
       "      <td>0.335663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018041503010401</th>\n",
       "      <td>2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.125475</td>\n",
       "      <td>0.966155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018041503010403</th>\n",
       "      <td>4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.149814</td>\n",
       "      <td>0.389515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018041503010403</th>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>0.128550</td>\n",
       "      <td>1.632588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022080704020406</th>\n",
       "      <td>6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.103477</td>\n",
       "      <td>0.248344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022080704020406</th>\n",
       "      <td>2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.124520</td>\n",
       "      <td>0.473177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022080704020408</th>\n",
       "      <td>11</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.124080</td>\n",
       "      <td>0.719664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022080704020411</th>\n",
       "      <td>8</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.104550</td>\n",
       "      <td>1.589165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022080704020411</th>\n",
       "      <td>9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.127177</td>\n",
       "      <td>0.483271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13747 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  h_num  odds  time_odds  win_ratio  expected_f\n",
       "race_id                                                        \n",
       "2018041409020710     12   2.6        2.8   0.137220    0.384215\n",
       "2018041503010401     14   2.2        2.5   0.134265    0.335663\n",
       "2018041503010401      2   9.4        7.7   0.125475    0.966155\n",
       "2018041503010403      4   2.4        2.6   0.149814    0.389515\n",
       "2018041503010403      8  10.0       12.7   0.128550    1.632588\n",
       "...                 ...   ...        ...        ...         ...\n",
       "2022080704020406      6   2.1        2.4   0.103477    0.248344\n",
       "2022080704020406      2   4.1        3.8   0.124520    0.473177\n",
       "2022080704020408     11   5.1        5.8   0.124080    0.719664\n",
       "2022080704020411      8  16.5       15.2   0.104550    1.589165\n",
       "2022080704020411      9   3.7        3.8   0.127177    0.483271\n",
       "\n",
       "[13747 rows x 5 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = f_me_valid.pred_table(X_fukusho_valid1, 0.6)\n",
    "fm['expected_f'] = fm['time_odds'] * fm['win_ratio']\n",
    "fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "26ae3c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>h_num_x</th>\n",
       "      <th>expected</th>\n",
       "      <th>h_num_y</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>expected_f</th>\n",
       "      <th>1着馬番</th>\n",
       "      <th>2着馬番</th>\n",
       "      <th>3着馬番</th>\n",
       "      <th>馬連</th>\n",
       "      <th>馬単</th>\n",
       "      <th>3連複</th>\n",
       "      <th>3連単</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.127886</td>\n",
       "      <td>1.176554</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.391390</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018041509020807</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.651205</td>\n",
       "      <td>5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.110525</td>\n",
       "      <td>0.817884</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>730</td>\n",
       "      <td>1550</td>\n",
       "      <td>1500</td>\n",
       "      <td>6750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018041503010409</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.459412</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.146889</td>\n",
       "      <td>0.837267</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8220</td>\n",
       "      <td>13040</td>\n",
       "      <td>26540</td>\n",
       "      <td>132470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018041503010403</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.532523</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.149814</td>\n",
       "      <td>0.389515</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>3890</td>\n",
       "      <td>5300</td>\n",
       "      <td>3470</td>\n",
       "      <td>21440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>2021041804010405</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3.436915</td>\n",
       "      <td>1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.134093</td>\n",
       "      <td>1.434790</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13500</td>\n",
       "      <td>20480</td>\n",
       "      <td>30560</td>\n",
       "      <td>150330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>2021041804010403</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.661534</td>\n",
       "      <td>11</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.145722</td>\n",
       "      <td>1.253205</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1880</td>\n",
       "      <td>4490</td>\n",
       "      <td>6300</td>\n",
       "      <td>40570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>2021042405020107</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1.477045</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.151523</td>\n",
       "      <td>0.515178</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>940</td>\n",
       "      <td>2040</td>\n",
       "      <td>6660</td>\n",
       "      <td>25030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>4.521706</td>\n",
       "      <td>2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.146676</td>\n",
       "      <td>0.425359</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1200</td>\n",
       "      <td>1780</td>\n",
       "      <td>3070</td>\n",
       "      <td>11510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2021042405020112</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1.393587</td>\n",
       "      <td>7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.119398</td>\n",
       "      <td>0.226856</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>45840</td>\n",
       "      <td>83610</td>\n",
       "      <td>573390</td>\n",
       "      <td>3896880.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2220 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               race_id  place_id  h_num_x  expected  h_num_y  odds  time_odds  \\\n",
       "0     2018041506030807         6       11  1.513531        8  11.6        9.2   \n",
       "1     2018041506030807         6       11  1.513531        2   4.0        3.6   \n",
       "2     2018041509020807         9        9  1.651205        5   7.3        7.4   \n",
       "3     2018041503010409         3        2  1.459412        3   3.1        5.7   \n",
       "5     2018041503010403         3        9  1.532523        4   2.4        2.6   \n",
       "...                ...       ...      ...       ...      ...   ...        ...   \n",
       "2762  2021041804010405         4        3  3.436915        1   4.3       10.7   \n",
       "2763  2021041804010403         4        8  2.661534       11   7.7        8.6   \n",
       "2764  2021042405020107         5       15  1.477045        1   3.8        3.4   \n",
       "2765  2021042405020111         5       13  4.521706        2   3.1        2.9   \n",
       "2767  2021042405020112         5        9  1.393587        7   2.4        1.9   \n",
       "\n",
       "      win_ratio  expected_f  1着馬番  2着馬番  3着馬番     馬連     馬単     3連複        3連単  \n",
       "0      0.127886    1.176554    13    15     2   5000  12260   10910    90850.0  \n",
       "1      0.108720    0.391390    13    15     2   5000  12260   10910    90850.0  \n",
       "2      0.110525    0.817884    16    12     5    730   1550    1500     6750.0  \n",
       "3      0.146889    0.837267     2    10     5   8220  13040   26540   132470.0  \n",
       "5      0.149814    0.389515     4    14     9   3890   5300    3470    21440.0  \n",
       "...         ...         ...   ...   ...   ...    ...    ...     ...        ...  \n",
       "2762   0.134093    1.434790     2     5     6  13500  20480   30560   150330.0  \n",
       "2763   0.145722    1.253205     8    10     3   1880   4490    6300    40570.0  \n",
       "2764   0.151523    0.515178     1    13     2    940   2040    6660    25030.0  \n",
       "2765   0.146676    0.425359     2     9     5   1200   1780    3070    11510.0  \n",
       "2767   0.119398    0.226856    12    15     8  45840  83610  573390  3896880.0  \n",
       "\n",
       "[2220 rows x 16 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umaren = bt[['race_id', 'place_id', 'h_num', 'expected']].merge(fm, on='race_id')\n",
    "\n",
    "uma_haito = umaren.merge(haitou[['1着馬番', '2着馬番', '3着馬番', '馬連', '馬単', '3連複', '3連単']], on='race_id')\n",
    "uma_haito = uma_haito[uma_haito['h_num_x'] != uma_haito['h_num_y']]\n",
    "uma_haito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5da14b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245030,\n",
       " 34100,\n",
       " 110.37387387387388,\n",
       " 0.0518018018018018,\n",
       " 115,\n",
       " 0.08914728682170543)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "f_bt['馬連'].sum(), f_bt['馬連'].max(), (f_bt['馬連'].sum() / (len(uma_haito) * 100)) * 100, len(f_bt) / len(uma_haito), len(f_bt), len(f_bt.groupby('race_id')) / len(uma_haito.groupby('race_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e40c32e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499820, 87030, 112.57207207207207, 0.0518018018018018)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_bt = uma_haito[\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['1着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma_haito['h_num_x'] == uma_haito['2着馬番'])\n",
    "        &\n",
    "        (uma_haito['h_num_y'] == uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "ut_bt['馬単'].sum(), ut_bt['馬単'].max(), (ut_bt['馬単'].sum() / (len(uma_haito) * 200)) * 100, len(ut_bt) / len(uma_haito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "466ee010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>h_num_x</th>\n",
       "      <th>expected</th>\n",
       "      <th>h_num_y</th>\n",
       "      <th>odds</th>\n",
       "      <th>time_odds</th>\n",
       "      <th>win_ratio</th>\n",
       "      <th>expected_f</th>\n",
       "      <th>1着馬番</th>\n",
       "      <th>2着馬番</th>\n",
       "      <th>3着馬番</th>\n",
       "      <th>馬連</th>\n",
       "      <th>馬単</th>\n",
       "      <th>3連複</th>\n",
       "      <th>3連単</th>\n",
       "      <th>h_num</th>\n",
       "      <th>expected_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.127886</td>\n",
       "      <td>1.176554</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.011575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.127886</td>\n",
       "      <td>1.176554</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.391390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.391390</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.176554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018041506030807</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1.513531</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.108720</td>\n",
       "      <td>0.391390</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>12260</td>\n",
       "      <td>10910</td>\n",
       "      <td>90850.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.011575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018041509020807</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.651205</td>\n",
       "      <td>5</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.110525</td>\n",
       "      <td>0.817884</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>730</td>\n",
       "      <td>1550</td>\n",
       "      <td>1500</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.472413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8026</th>\n",
       "      <td>2021041804010403</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.661534</td>\n",
       "      <td>11</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.145722</td>\n",
       "      <td>1.253205</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1880</td>\n",
       "      <td>4490</td>\n",
       "      <td>6300</td>\n",
       "      <td>40570.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.590212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8029</th>\n",
       "      <td>2021042405020107</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1.477045</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.151523</td>\n",
       "      <td>0.515178</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>940</td>\n",
       "      <td>2040</td>\n",
       "      <td>6660</td>\n",
       "      <td>25030.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.444448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8030</th>\n",
       "      <td>2021042405020107</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1.477045</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.151523</td>\n",
       "      <td>0.515178</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>940</td>\n",
       "      <td>2040</td>\n",
       "      <td>6660</td>\n",
       "      <td>25030.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.593379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>2021042405020111</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>4.521706</td>\n",
       "      <td>2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.146676</td>\n",
       "      <td>0.425359</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1200</td>\n",
       "      <td>1780</td>\n",
       "      <td>3070</td>\n",
       "      <td>11510.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.940314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8033</th>\n",
       "      <td>2021042405020112</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1.393587</td>\n",
       "      <td>7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.119398</td>\n",
       "      <td>0.226856</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>45840</td>\n",
       "      <td>83610</td>\n",
       "      <td>573390</td>\n",
       "      <td>3896880.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2.060991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4550 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               race_id  place_id  h_num_x  expected  h_num_y  odds  time_odds  \\\n",
       "1     2018041506030807         6       11  1.513531        8  11.6        9.2   \n",
       "2     2018041506030807         6       11  1.513531        8  11.6        9.2   \n",
       "3     2018041506030807         6       11  1.513531        2   4.0        3.6   \n",
       "4     2018041506030807         6       11  1.513531        2   4.0        3.6   \n",
       "6     2018041509020807         9        9  1.651205        5   7.3        7.4   \n",
       "...                ...       ...      ...       ...      ...   ...        ...   \n",
       "8026  2021041804010403         4        8  2.661534       11   7.7        8.6   \n",
       "8029  2021042405020107         5       15  1.477045        1   3.8        3.4   \n",
       "8030  2021042405020107         5       15  1.477045        1   3.8        3.4   \n",
       "8032  2021042405020111         5       13  4.521706        2   3.1        2.9   \n",
       "8033  2021042405020112         5        9  1.393587        7   2.4        1.9   \n",
       "\n",
       "      win_ratio  expected_f  1着馬番  2着馬番  3着馬番     馬連     馬単     3連複  \\\n",
       "1      0.127886    1.176554    13    15     2   5000  12260   10910   \n",
       "2      0.127886    1.176554    13    15     2   5000  12260   10910   \n",
       "3      0.108720    0.391390    13    15     2   5000  12260   10910   \n",
       "4      0.108720    0.391390    13    15     2   5000  12260   10910   \n",
       "6      0.110525    0.817884    16    12     5    730   1550    1500   \n",
       "...         ...         ...   ...   ...   ...    ...    ...     ...   \n",
       "8026   0.145722    1.253205     8    10     3   1880   4490    6300   \n",
       "8029   0.151523    0.515178     1    13     2    940   2040    6660   \n",
       "8030   0.151523    0.515178     1    13     2    940   2040    6660   \n",
       "8032   0.146676    0.425359     2     9     5   1200   1780    3070   \n",
       "8033   0.119398    0.226856    12    15     8  45840  83610  573390   \n",
       "\n",
       "            3連単  h_num  expected_3  \n",
       "1       90850.0      6    1.011575  \n",
       "2       90850.0      2    0.391390  \n",
       "3       90850.0      8    1.176554  \n",
       "4       90850.0      6    1.011575  \n",
       "6        6750.0     14    1.472413  \n",
       "...         ...    ...         ...  \n",
       "8026    40570.0     10    0.590212  \n",
       "8029    25030.0     11    0.444448  \n",
       "8030    25030.0     16    0.593379  \n",
       "8032    11510.0     11    0.940314  \n",
       "8033  3896880.0     11    2.060991  \n",
       "\n",
       "[4550 rows x 18 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm3 = f_me_valid.pred_table(X_fukusho_valid1, 0.5)\n",
    "fm3['expected_3'] = fm3['time_odds'] * fm3['win_ratio']\n",
    "uma3 = uma_haito.merge(fm3[['h_num', 'expected_3']], on='race_id')\n",
    "uma3 = uma3[\n",
    "    (\n",
    "        (uma3['h_num'] != uma3['h_num_y'])\n",
    "        &\n",
    "        (uma3['h_num'] != uma3['h_num_x'])\n",
    "    )\n",
    "#     &\n",
    "#     uma3['expected_3'] > 0.7\n",
    "]\n",
    "uma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5979bd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419230, 37010, 92.13846153846154, 0.02879120879120879, 0.08375209380234507)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_b3 = uma3[\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['1着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['2着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['3着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['1着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['3着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['2着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['2着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['1着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['3着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['2着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['3着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['1着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['3着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['1着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['2着馬番'])\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (uma3['h_num_x'] == uma3['3着馬番'])\n",
    "        &\n",
    "        (uma3['h_num_y'] == uma3['2着馬番'])\n",
    "        &\n",
    "        (uma3['h_num'] == uma3['1着馬番'])\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "f_b3['3連複'].sum(), f_b3['3連複'].max(), (f_b3['3連複'].sum() / (len(uma3) * 100)) * 100, len(f_b3) / len(uma3),  len(f_b3.groupby('race_id')) / len(uma3.groupby('race_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eddccfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2449250.0, 203670.0, 89.71611721611723, 0.02879120879120879, 131)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_b3['3連単'].sum(), f_b3['3連単'].max(), (f_b3['3連単'].sum() / (len(uma3) * 600)) * 100, len(f_b3) / len(uma3), len(f_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "480d873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/99p8d8fn6g75hkllgmntdmxc0000gn/T/ipykernel_12898/3819530198.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a['h_num_y'] = a['h_num_y_x']\n"
     ]
    }
   ],
   "source": [
    "u = uma_haito[['race_id', 'h_num_x', 'h_num_y']]\n",
    "s = u.merge(uma3[['race_id', 'h_num_x', 'h_num_y', 'h_num']], how='left', on=['race_id', 'h_num_x'])\n",
    "s = s[\n",
    "    (s['h_num_y_x'] != s['h_num_y_y'])\n",
    "    &\n",
    "    (s['h_num_y_x'] != s['h_num'])\n",
    "]\n",
    "\n",
    "a = s[['race_id', 'h_num_x', 'h_num_y_x']]\n",
    "a['h_num_y'] = a['h_num_y_x']\n",
    "\n",
    "b = s[['race_id', 'h_num_y_x', 'h_num_y_y']].dropna()\n",
    "b['h_num_x'] = b['h_num_y_x']\n",
    "b['h_num_y'] = b['h_num_y_y']\n",
    "\n",
    "c = s[['race_id', 'h_num_x', 'h_num_y_y']].dropna()\n",
    "c['h_num_y'] = c['h_num_y_y']\n",
    "\n",
    "d = s[['race_id', 'h_num_x', 'h_num']].dropna()\n",
    "d['h_num_y'] = d['h_num']\n",
    "\n",
    "e = s[['race_id', 'h_num_y_x', 'h_num']].dropna()\n",
    "e['h_num_x'] = e['h_num_y_x']\n",
    "e['h_num_y'] = e['h_num']\n",
    "\n",
    "f = s[['race_id', 'h_num_y_y', 'h_num']].dropna()\n",
    "f['h_num_x'] = f['h_num_y_y']\n",
    "f['h_num_y'] = f['h_num']\n",
    "\n",
    "w = pd.concat([\n",
    "    a[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    b[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    c[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    d[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    e[['race_id', 'h_num_x', 'h_num_y']],\n",
    "    f[['race_id', 'h_num_x', 'h_num_y']]\n",
    "]).drop_duplicates()\n",
    "wide = pd.read_csv('./csv_new2/base/wide.csv')\n",
    "w = w.merge(wide, on='race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a08b4285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5346, 534600, 481410.0, 90.05050505050505, 0.132996632996633, 711)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide = 0\n",
    "wide_tekichu = 0\n",
    "\n",
    "for i in range(1, 8):\n",
    "    s = str(i)\n",
    "    w_b3 = w[\n",
    "        (\n",
    "            ((w['h_num_x'] == w['wide' + s + '_uma1']) | (w['h_num_x'] == w['wide' + s + '_uma2']))\n",
    "            &\n",
    "            ((w['h_num_y'] == w['wide' + s + '_uma1']) | (w['h_num_y'] == w['wide' + s + '_uma2']))\n",
    "        )\n",
    "    ]\n",
    "    wide = wide + w_b3['wide_' + s].sum()\n",
    "    wide_tekichu = wide_tekichu + len(w_b3)\n",
    "\n",
    "len(w), len(w) * 100, wide, (wide / (len(w) * 100)) * 100, wide_tekichu / len(w), wide_tekichu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a3ffc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tm = TimeModel(model, test)\n",
    "# test_df = tm.race_pred_time(X_test)\n",
    "\n",
    "me_test = ModelEvaluator(lgb_clf1, haitou, True)\n",
    "# me_test.score(y_test1, X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ebd2a70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>h_num</th>\n",
       "      <th>win_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.070676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.098533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.093939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.070492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.103490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.075535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.085469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.085766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.089687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.089628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.077737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.087711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.098835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.105365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.092653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.079763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.094608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.103504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.082320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.106385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.074670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.086806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.101682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.111860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.086867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.107579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.086657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.086695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.108446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.087454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.145780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.140630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.081186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.074552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.092066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.075613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.080225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.103863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.083613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.096530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.098535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.097636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.098410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.097430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.097421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.107978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.088659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.103548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.102957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.100323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.094466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.085909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.092130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.089747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.089965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.105035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.112157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.100710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.078434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.106360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.093233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.085255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.088222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.074392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             race_id  place_id  h_num  win_ratio\n",
       "0   2022090410040803        10      3   0.070676\n",
       "1   2022090410040803        10      1   0.079361\n",
       "2   2022090410040803        10      2   0.098533\n",
       "3   2022090410040803        10      5   0.093939\n",
       "4   2022090410040803        10      6   0.070492\n",
       "5   2022090410040803        10     12   0.103490\n",
       "6   2022090410040803        10     14   0.075535\n",
       "7   2022090404030810         4     12   0.085469\n",
       "8   2022090404030810         4      9   0.085766\n",
       "9   2022090404030810         4      7   0.089687\n",
       "10  2022090404030810         4      6   0.089628\n",
       "11  2022090404030810         4      1   0.096100\n",
       "12  2022090404030810         4      3   0.077737\n",
       "13  2022090404030810         4      4   0.087711\n",
       "14  2022090404030810         4      5   0.083756\n",
       "15  2022090401020803         1      2   0.098835\n",
       "16  2022090401020803         1      4   0.092507\n",
       "17  2022090401020803         1      7   0.105365\n",
       "18  2022090401020803         1      9   0.092653\n",
       "19  2022090401020803         1     10   0.079763\n",
       "20  2022090401020803         1     11   0.094608\n",
       "21  2022090401020803         1     12   0.103504\n",
       "22  2022090404030808         4     11   0.082320\n",
       "23  2022090404030808         4      4   0.106385\n",
       "24  2022090404030808         4      5   0.074670\n",
       "25  2022090404030808         4      9   0.086806\n",
       "26  2022090404030808         4     10   0.101682\n",
       "27  2022090404030808         4     13   0.111860\n",
       "28  2022090410040810        10      4   0.096956\n",
       "29  2022090410040810        10      3   0.086867\n",
       "30  2022090410040810        10      6   0.107579\n",
       "31  2022090410040810        10      7   0.086657\n",
       "32  2022090410040810        10     10   0.086695\n",
       "33  2022090410040810        10     11   0.108446\n",
       "34  2022090410040810        10     14   0.087454\n",
       "35  2022090410040808        10      4   0.145780\n",
       "36  2022090410040808        10      3   0.135335\n",
       "37  2022090410040808        10      2   0.140630\n",
       "38  2022090410040806        10      1   0.078442\n",
       "39  2022090410040806        10      2   0.081186\n",
       "40  2022090410040806        10      3   0.075668\n",
       "41  2022090410040806        10      7   0.074552\n",
       "42  2022090410040806        10      9   0.092066\n",
       "43  2022090410040806        10     10   0.075613\n",
       "44  2022090410040806        10     14   0.080225\n",
       "45  2022090410040806        10     15   0.103863\n",
       "46  2022090401020808         1      4   0.083613\n",
       "47  2022090401020808         1      3   0.096530\n",
       "48  2022090401020808         1      6   0.098535\n",
       "49  2022090401020808         1     12   0.097636\n",
       "50  2022090401020808         1     13   0.098410\n",
       "51  2022090401020808         1     14   0.097430\n",
       "52  2022090401020805         1     10   0.097421\n",
       "53  2022090401020805         1     12   0.107978\n",
       "54  2022090401020805         1     14   0.088659\n",
       "55  2022090401020805         1      9   0.103548\n",
       "56  2022090401020805         1      8   0.102957\n",
       "57  2022090401020805         1      6   0.100323\n",
       "58  2022090401020812         1      1   0.086843\n",
       "59  2022090401020812         1      2   0.080270\n",
       "60  2022090401020812         1      3   0.094466\n",
       "61  2022090401020812         1      5   0.085909\n",
       "62  2022090401020812         1      7   0.092130\n",
       "63  2022090401020812         1      9   0.089747\n",
       "64  2022090401020812         1     12   0.089965\n",
       "65  2022090404030807         4     14   0.105035\n",
       "66  2022090404030807         4      2   0.091954\n",
       "67  2022090404030807         4      5   0.112157\n",
       "68  2022090404030807         4     10   0.100710\n",
       "69  2022090404030807         4     11   0.078434\n",
       "70  2022090404030803         4     11   0.106360\n",
       "71  2022090404030803         4     14   0.093233\n",
       "72  2022090404030803         4     10   0.085255\n",
       "73  2022090404030803         4      9   0.078000\n",
       "74  2022090404030803         4      8   0.088222\n",
       "75  2022090404030803         4      7   0.074392\n",
       "76  2022090404030803         4      2   0.085810"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_grade = pd.read_csv('./csv_new2/20220904/races.csv')\n",
    "race_grade = race_grade.set_index('race_id')\n",
    "# race_grade[['grade']]\n",
    "\n",
    "wrm = me_test.pred_table(X_test1, 0.65)\n",
    "wrm['expected'] = wrm['win_ratio'] * wrm['time_odds'] \n",
    "\n",
    "lbets = wrm.copy()\n",
    "\n",
    "weather = pd.read_csv('./csv_new2/20220904/weathers.csv')\n",
    "lbets = lbets.merge(weather, on='race_id')\n",
    "\n",
    "lbets = lbets.merge(race_grade, on='race_id')\n",
    "# lbt = lbets[\n",
    "#      (lbets['expected'] >= 1)\n",
    "# ]\n",
    "\n",
    "lbt = lbets\n",
    "lbt[['race_id', 'place_id', 'h_num', 'win_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "f0fe3b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.66666666666666"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bt[bt['h_num'] == bt['1着馬番']]['単勝'].sum() / (len(bt) * 100)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e4a8148a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>place_id</th>\n",
       "      <th>h_num_x</th>\n",
       "      <th>h_num_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2022090410040803</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2022090404030810</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2022090401020803</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2022090404030808</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2022090410040810</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2022090410040808</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2022090410040806</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2022090401020808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2022090401020805</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2022090401020812</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>2022090404030807</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>2022090404030803</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              race_id  place_id  h_num_x  h_num_y\n",
       "1    2022090410040803        10        3        1\n",
       "2    2022090410040803        10        3        2\n",
       "3    2022090410040803        10        3        5\n",
       "4    2022090410040803        10        3        6\n",
       "5    2022090410040803        10        3       10\n",
       "6    2022090410040803        10        3       12\n",
       "7    2022090410040803        10        3       14\n",
       "8    2022090410040803        10        1        3\n",
       "10   2022090410040803        10        1        2\n",
       "11   2022090410040803        10        1        5\n",
       "12   2022090410040803        10        1        6\n",
       "13   2022090410040803        10        1       10\n",
       "14   2022090410040803        10        1       12\n",
       "15   2022090410040803        10        1       14\n",
       "16   2022090410040803        10        2        3\n",
       "17   2022090410040803        10        2        1\n",
       "19   2022090410040803        10        2        5\n",
       "20   2022090410040803        10        2        6\n",
       "21   2022090410040803        10        2       10\n",
       "22   2022090410040803        10        2       12\n",
       "23   2022090410040803        10        2       14\n",
       "24   2022090410040803        10        5        3\n",
       "25   2022090410040803        10        5        1\n",
       "26   2022090410040803        10        5        2\n",
       "28   2022090410040803        10        5        6\n",
       "29   2022090410040803        10        5       10\n",
       "30   2022090410040803        10        5       12\n",
       "31   2022090410040803        10        5       14\n",
       "32   2022090410040803        10        6        3\n",
       "33   2022090410040803        10        6        1\n",
       "34   2022090410040803        10        6        2\n",
       "35   2022090410040803        10        6        5\n",
       "37   2022090410040803        10        6       10\n",
       "38   2022090410040803        10        6       12\n",
       "39   2022090410040803        10        6       14\n",
       "40   2022090410040803        10       12        3\n",
       "41   2022090410040803        10       12        1\n",
       "42   2022090410040803        10       12        2\n",
       "43   2022090410040803        10       12        5\n",
       "44   2022090410040803        10       12        6\n",
       "45   2022090410040803        10       12       10\n",
       "47   2022090410040803        10       12       14\n",
       "48   2022090410040803        10       14        3\n",
       "49   2022090410040803        10       14        1\n",
       "50   2022090410040803        10       14        2\n",
       "51   2022090410040803        10       14        5\n",
       "52   2022090410040803        10       14        6\n",
       "53   2022090410040803        10       14       10\n",
       "54   2022090410040803        10       14       12\n",
       "57   2022090404030810         4       12        9\n",
       "58   2022090404030810         4       12        7\n",
       "59   2022090404030810         4       12        6\n",
       "60   2022090404030810         4       12        1\n",
       "61   2022090404030810         4       12        3\n",
       "62   2022090404030810         4       12        4\n",
       "63   2022090404030810         4       12        5\n",
       "64   2022090404030810         4        9       12\n",
       "66   2022090404030810         4        9        7\n",
       "67   2022090404030810         4        9        6\n",
       "68   2022090404030810         4        9        1\n",
       "69   2022090404030810         4        9        3\n",
       "70   2022090404030810         4        9        4\n",
       "71   2022090404030810         4        9        5\n",
       "72   2022090404030810         4        7       12\n",
       "73   2022090404030810         4        7        9\n",
       "75   2022090404030810         4        7        6\n",
       "76   2022090404030810         4        7        1\n",
       "77   2022090404030810         4        7        3\n",
       "78   2022090404030810         4        7        4\n",
       "79   2022090404030810         4        7        5\n",
       "80   2022090404030810         4        6       12\n",
       "81   2022090404030810         4        6        9\n",
       "82   2022090404030810         4        6        7\n",
       "84   2022090404030810         4        6        1\n",
       "85   2022090404030810         4        6        3\n",
       "86   2022090404030810         4        6        4\n",
       "87   2022090404030810         4        6        5\n",
       "88   2022090404030810         4        1       12\n",
       "89   2022090404030810         4        1        9\n",
       "90   2022090404030810         4        1        7\n",
       "91   2022090404030810         4        1        6\n",
       "93   2022090404030810         4        1        3\n",
       "94   2022090404030810         4        1        4\n",
       "95   2022090404030810         4        1        5\n",
       "96   2022090404030810         4        3       12\n",
       "97   2022090404030810         4        3        9\n",
       "98   2022090404030810         4        3        7\n",
       "99   2022090404030810         4        3        6\n",
       "100  2022090404030810         4        3        1\n",
       "102  2022090404030810         4        3        4\n",
       "103  2022090404030810         4        3        5\n",
       "104  2022090404030810         4        4       12\n",
       "105  2022090404030810         4        4        9\n",
       "106  2022090404030810         4        4        7\n",
       "107  2022090404030810         4        4        6\n",
       "108  2022090404030810         4        4        1\n",
       "109  2022090404030810         4        4        3\n",
       "111  2022090404030810         4        4        5\n",
       "112  2022090404030810         4        5       12\n",
       "113  2022090404030810         4        5        9\n",
       "114  2022090404030810         4        5        7\n",
       "115  2022090404030810         4        5        6\n",
       "116  2022090404030810         4        5        1\n",
       "117  2022090404030810         4        5        3\n",
       "118  2022090404030810         4        5        4\n",
       "121  2022090401020803         1        2        4\n",
       "122  2022090401020803         1        2        7\n",
       "123  2022090401020803         1        2        9\n",
       "124  2022090401020803         1        2       10\n",
       "125  2022090401020803         1        2       11\n",
       "126  2022090401020803         1        2       12\n",
       "127  2022090401020803         1        4        2\n",
       "129  2022090401020803         1        4        7\n",
       "130  2022090401020803         1        4        9\n",
       "131  2022090401020803         1        4       10\n",
       "132  2022090401020803         1        4       11\n",
       "133  2022090401020803         1        4       12\n",
       "134  2022090401020803         1        7        2\n",
       "135  2022090401020803         1        7        4\n",
       "137  2022090401020803         1        7        9\n",
       "138  2022090401020803         1        7       10\n",
       "139  2022090401020803         1        7       11\n",
       "140  2022090401020803         1        7       12\n",
       "141  2022090401020803         1        9        2\n",
       "142  2022090401020803         1        9        4\n",
       "143  2022090401020803         1        9        7\n",
       "145  2022090401020803         1        9       10\n",
       "146  2022090401020803         1        9       11\n",
       "147  2022090401020803         1        9       12\n",
       "148  2022090401020803         1       10        2\n",
       "149  2022090401020803         1       10        4\n",
       "150  2022090401020803         1       10        7\n",
       "151  2022090401020803         1       10        9\n",
       "153  2022090401020803         1       10       11\n",
       "154  2022090401020803         1       10       12\n",
       "155  2022090401020803         1       11        2\n",
       "156  2022090401020803         1       11        4\n",
       "157  2022090401020803         1       11        7\n",
       "158  2022090401020803         1       11        9\n",
       "159  2022090401020803         1       11       10\n",
       "161  2022090401020803         1       11       12\n",
       "162  2022090401020803         1       12        2\n",
       "163  2022090401020803         1       12        4\n",
       "164  2022090401020803         1       12        7\n",
       "165  2022090401020803         1       12        9\n",
       "166  2022090401020803         1       12       10\n",
       "167  2022090401020803         1       12       11\n",
       "170  2022090404030808         4       11        4\n",
       "171  2022090404030808         4       11        5\n",
       "172  2022090404030808         4       11        9\n",
       "173  2022090404030808         4       11       10\n",
       "174  2022090404030808         4       11       13\n",
       "175  2022090404030808         4       11       14\n",
       "176  2022090404030808         4        4       11\n",
       "178  2022090404030808         4        4        5\n",
       "179  2022090404030808         4        4        9\n",
       "180  2022090404030808         4        4       10\n",
       "181  2022090404030808         4        4       13\n",
       "182  2022090404030808         4        4       14\n",
       "183  2022090404030808         4        5       11\n",
       "184  2022090404030808         4        5        4\n",
       "186  2022090404030808         4        5        9\n",
       "187  2022090404030808         4        5       10\n",
       "188  2022090404030808         4        5       13\n",
       "189  2022090404030808         4        5       14\n",
       "190  2022090404030808         4        9       11\n",
       "191  2022090404030808         4        9        4\n",
       "192  2022090404030808         4        9        5\n",
       "194  2022090404030808         4        9       10\n",
       "195  2022090404030808         4        9       13\n",
       "196  2022090404030808         4        9       14\n",
       "197  2022090404030808         4       10       11\n",
       "198  2022090404030808         4       10        4\n",
       "199  2022090404030808         4       10        5\n",
       "200  2022090404030808         4       10        9\n",
       "202  2022090404030808         4       10       13\n",
       "203  2022090404030808         4       10       14\n",
       "204  2022090404030808         4       13       11\n",
       "205  2022090404030808         4       13        4\n",
       "206  2022090404030808         4       13        5\n",
       "207  2022090404030808         4       13        9\n",
       "208  2022090404030808         4       13       10\n",
       "210  2022090404030808         4       13       14\n",
       "212  2022090410040810        10        4        2\n",
       "213  2022090410040810        10        4        3\n",
       "214  2022090410040810        10        4        6\n",
       "215  2022090410040810        10        4        7\n",
       "216  2022090410040810        10        4       10\n",
       "217  2022090410040810        10        4       11\n",
       "218  2022090410040810        10        4       14\n",
       "219  2022090410040810        10        3        4\n",
       "220  2022090410040810        10        3        2\n",
       "222  2022090410040810        10        3        6\n",
       "223  2022090410040810        10        3        7\n",
       "224  2022090410040810        10        3       10\n",
       "225  2022090410040810        10        3       11\n",
       "226  2022090410040810        10        3       14\n",
       "227  2022090410040810        10        6        4\n",
       "228  2022090410040810        10        6        2\n",
       "229  2022090410040810        10        6        3\n",
       "231  2022090410040810        10        6        7\n",
       "232  2022090410040810        10        6       10\n",
       "233  2022090410040810        10        6       11\n",
       "234  2022090410040810        10        6       14\n",
       "235  2022090410040810        10        7        4\n",
       "236  2022090410040810        10        7        2\n",
       "237  2022090410040810        10        7        3\n",
       "238  2022090410040810        10        7        6\n",
       "240  2022090410040810        10        7       10\n",
       "241  2022090410040810        10        7       11\n",
       "242  2022090410040810        10        7       14\n",
       "243  2022090410040810        10       10        4\n",
       "244  2022090410040810        10       10        2\n",
       "245  2022090410040810        10       10        3\n",
       "246  2022090410040810        10       10        6\n",
       "247  2022090410040810        10       10        7\n",
       "249  2022090410040810        10       10       11\n",
       "250  2022090410040810        10       10       14\n",
       "251  2022090410040810        10       11        4\n",
       "252  2022090410040810        10       11        2\n",
       "253  2022090410040810        10       11        3\n",
       "254  2022090410040810        10       11        6\n",
       "255  2022090410040810        10       11        7\n",
       "256  2022090410040810        10       11       10\n",
       "258  2022090410040810        10       11       14\n",
       "259  2022090410040810        10       14        4\n",
       "260  2022090410040810        10       14        2\n",
       "261  2022090410040810        10       14        3\n",
       "262  2022090410040810        10       14        6\n",
       "263  2022090410040810        10       14        7\n",
       "264  2022090410040810        10       14       10\n",
       "265  2022090410040810        10       14       11\n",
       "267  2022090410040808        10        4        6\n",
       "269  2022090410040808        10        4        3\n",
       "270  2022090410040808        10        4        2\n",
       "271  2022090410040808        10        3        6\n",
       "272  2022090410040808        10        3        4\n",
       "274  2022090410040808        10        3        2\n",
       "275  2022090410040808        10        2        6\n",
       "276  2022090410040808        10        2        4\n",
       "277  2022090410040808        10        2        3\n",
       "280  2022090410040806        10        1        2\n",
       "281  2022090410040806        10        1        3\n",
       "282  2022090410040806        10        1        7\n",
       "283  2022090410040806        10        1        9\n",
       "284  2022090410040806        10        1       10\n",
       "285  2022090410040806        10        1       14\n",
       "286  2022090410040806        10        1       15\n",
       "287  2022090410040806        10        2        1\n",
       "289  2022090410040806        10        2        3\n",
       "290  2022090410040806        10        2        7\n",
       "291  2022090410040806        10        2        9\n",
       "292  2022090410040806        10        2       10\n",
       "293  2022090410040806        10        2       14\n",
       "294  2022090410040806        10        2       15\n",
       "295  2022090410040806        10        3        1\n",
       "296  2022090410040806        10        3        2\n",
       "298  2022090410040806        10        3        7\n",
       "299  2022090410040806        10        3        9\n",
       "300  2022090410040806        10        3       10\n",
       "301  2022090410040806        10        3       14\n",
       "302  2022090410040806        10        3       15\n",
       "303  2022090410040806        10        7        1\n",
       "304  2022090410040806        10        7        2\n",
       "305  2022090410040806        10        7        3\n",
       "307  2022090410040806        10        7        9\n",
       "308  2022090410040806        10        7       10\n",
       "309  2022090410040806        10        7       14\n",
       "310  2022090410040806        10        7       15\n",
       "311  2022090410040806        10        9        1\n",
       "312  2022090410040806        10        9        2\n",
       "313  2022090410040806        10        9        3\n",
       "314  2022090410040806        10        9        7\n",
       "316  2022090410040806        10        9       10\n",
       "317  2022090410040806        10        9       14\n",
       "318  2022090410040806        10        9       15\n",
       "319  2022090410040806        10       10        1\n",
       "320  2022090410040806        10       10        2\n",
       "321  2022090410040806        10       10        3\n",
       "322  2022090410040806        10       10        7\n",
       "323  2022090410040806        10       10        9\n",
       "325  2022090410040806        10       10       14\n",
       "326  2022090410040806        10       10       15\n",
       "327  2022090410040806        10       14        1\n",
       "328  2022090410040806        10       14        2\n",
       "329  2022090410040806        10       14        3\n",
       "330  2022090410040806        10       14        7\n",
       "331  2022090410040806        10       14        9\n",
       "332  2022090410040806        10       14       10\n",
       "334  2022090410040806        10       14       15\n",
       "335  2022090410040806        10       15        1\n",
       "336  2022090410040806        10       15        2\n",
       "337  2022090410040806        10       15        3\n",
       "338  2022090410040806        10       15        7\n",
       "339  2022090410040806        10       15        9\n",
       "340  2022090410040806        10       15       10\n",
       "341  2022090410040806        10       15       14\n",
       "344  2022090401020808         1        4        3\n",
       "345  2022090401020808         1        4        6\n",
       "346  2022090401020808         1        4        7\n",
       "347  2022090401020808         1        4        9\n",
       "348  2022090401020808         1        4       12\n",
       "349  2022090401020808         1        4       13\n",
       "350  2022090401020808         1        4       14\n",
       "351  2022090401020808         1        3        4\n",
       "353  2022090401020808         1        3        6\n",
       "354  2022090401020808         1        3        7\n",
       "355  2022090401020808         1        3        9\n",
       "356  2022090401020808         1        3       12\n",
       "357  2022090401020808         1        3       13\n",
       "358  2022090401020808         1        3       14\n",
       "359  2022090401020808         1        6        4\n",
       "360  2022090401020808         1        6        3\n",
       "362  2022090401020808         1        6        7\n",
       "363  2022090401020808         1        6        9\n",
       "364  2022090401020808         1        6       12\n",
       "365  2022090401020808         1        6       13\n",
       "366  2022090401020808         1        6       14\n",
       "367  2022090401020808         1       12        4\n",
       "368  2022090401020808         1       12        3\n",
       "369  2022090401020808         1       12        6\n",
       "370  2022090401020808         1       12        7\n",
       "371  2022090401020808         1       12        9\n",
       "373  2022090401020808         1       12       13\n",
       "374  2022090401020808         1       12       14\n",
       "375  2022090401020808         1       13        4\n",
       "376  2022090401020808         1       13        3\n",
       "377  2022090401020808         1       13        6\n",
       "378  2022090401020808         1       13        7\n",
       "379  2022090401020808         1       13        9\n",
       "380  2022090401020808         1       13       12\n",
       "382  2022090401020808         1       13       14\n",
       "383  2022090401020808         1       14        4\n",
       "384  2022090401020808         1       14        3\n",
       "385  2022090401020808         1       14        6\n",
       "386  2022090401020808         1       14        7\n",
       "387  2022090401020808         1       14        9\n",
       "388  2022090401020808         1       14       12\n",
       "389  2022090401020808         1       14       13\n",
       "392  2022090401020805         1       10       12\n",
       "393  2022090401020805         1       10       14\n",
       "394  2022090401020805         1       10        9\n",
       "395  2022090401020805         1       10        8\n",
       "396  2022090401020805         1       10        6\n",
       "397  2022090401020805         1       12       10\n",
       "399  2022090401020805         1       12       14\n",
       "400  2022090401020805         1       12        9\n",
       "401  2022090401020805         1       12        8\n",
       "402  2022090401020805         1       12        6\n",
       "403  2022090401020805         1       14       10\n",
       "404  2022090401020805         1       14       12\n",
       "406  2022090401020805         1       14        9\n",
       "407  2022090401020805         1       14        8\n",
       "408  2022090401020805         1       14        6\n",
       "409  2022090401020805         1        9       10\n",
       "410  2022090401020805         1        9       12\n",
       "411  2022090401020805         1        9       14\n",
       "413  2022090401020805         1        9        8\n",
       "414  2022090401020805         1        9        6\n",
       "415  2022090401020805         1        8       10\n",
       "416  2022090401020805         1        8       12\n",
       "417  2022090401020805         1        8       14\n",
       "418  2022090401020805         1        8        9\n",
       "420  2022090401020805         1        8        6\n",
       "421  2022090401020805         1        6       10\n",
       "422  2022090401020805         1        6       12\n",
       "423  2022090401020805         1        6       14\n",
       "424  2022090401020805         1        6        9\n",
       "425  2022090401020805         1        6        8\n",
       "428  2022090401020812         1        1        2\n",
       "429  2022090401020812         1        1        3\n",
       "430  2022090401020812         1        1        5\n",
       "431  2022090401020812         1        1        6\n",
       "432  2022090401020812         1        1        7\n",
       "433  2022090401020812         1        1        9\n",
       "434  2022090401020812         1        1       10\n",
       "435  2022090401020812         1        1       12\n",
       "436  2022090401020812         1        1       14\n",
       "437  2022090401020812         1        2        1\n",
       "439  2022090401020812         1        2        3\n",
       "440  2022090401020812         1        2        5\n",
       "441  2022090401020812         1        2        6\n",
       "442  2022090401020812         1        2        7\n",
       "443  2022090401020812         1        2        9\n",
       "444  2022090401020812         1        2       10\n",
       "445  2022090401020812         1        2       12\n",
       "446  2022090401020812         1        2       14\n",
       "447  2022090401020812         1        3        1\n",
       "448  2022090401020812         1        3        2\n",
       "450  2022090401020812         1        3        5\n",
       "451  2022090401020812         1        3        6\n",
       "452  2022090401020812         1        3        7\n",
       "453  2022090401020812         1        3        9\n",
       "454  2022090401020812         1        3       10\n",
       "455  2022090401020812         1        3       12\n",
       "456  2022090401020812         1        3       14\n",
       "457  2022090401020812         1        5        1\n",
       "458  2022090401020812         1        5        2\n",
       "459  2022090401020812         1        5        3\n",
       "461  2022090401020812         1        5        6\n",
       "462  2022090401020812         1        5        7\n",
       "463  2022090401020812         1        5        9\n",
       "464  2022090401020812         1        5       10\n",
       "465  2022090401020812         1        5       12\n",
       "466  2022090401020812         1        5       14\n",
       "467  2022090401020812         1        7        1\n",
       "468  2022090401020812         1        7        2\n",
       "469  2022090401020812         1        7        3\n",
       "470  2022090401020812         1        7        5\n",
       "471  2022090401020812         1        7        6\n",
       "473  2022090401020812         1        7        9\n",
       "474  2022090401020812         1        7       10\n",
       "475  2022090401020812         1        7       12\n",
       "476  2022090401020812         1        7       14\n",
       "477  2022090401020812         1        9        1\n",
       "478  2022090401020812         1        9        2\n",
       "479  2022090401020812         1        9        3\n",
       "480  2022090401020812         1        9        5\n",
       "481  2022090401020812         1        9        6\n",
       "482  2022090401020812         1        9        7\n",
       "484  2022090401020812         1        9       10\n",
       "485  2022090401020812         1        9       12\n",
       "486  2022090401020812         1        9       14\n",
       "487  2022090401020812         1       12        1\n",
       "488  2022090401020812         1       12        2\n",
       "489  2022090401020812         1       12        3\n",
       "490  2022090401020812         1       12        5\n",
       "491  2022090401020812         1       12        6\n",
       "492  2022090401020812         1       12        7\n",
       "493  2022090401020812         1       12        9\n",
       "494  2022090401020812         1       12       10\n",
       "496  2022090401020812         1       12       14\n",
       "498  2022090404030807         4       14        2\n",
       "499  2022090404030807         4       14        5\n",
       "500  2022090404030807         4       14        9\n",
       "501  2022090404030807         4       14       10\n",
       "502  2022090404030807         4       14       11\n",
       "503  2022090404030807         4        2       14\n",
       "505  2022090404030807         4        2        5\n",
       "506  2022090404030807         4        2        9\n",
       "507  2022090404030807         4        2       10\n",
       "508  2022090404030807         4        2       11\n",
       "509  2022090404030807         4        5       14\n",
       "510  2022090404030807         4        5        2\n",
       "512  2022090404030807         4        5        9\n",
       "513  2022090404030807         4        5       10\n",
       "514  2022090404030807         4        5       11\n",
       "515  2022090404030807         4       10       14\n",
       "516  2022090404030807         4       10        2\n",
       "517  2022090404030807         4       10        5\n",
       "518  2022090404030807         4       10        9\n",
       "520  2022090404030807         4       10       11\n",
       "521  2022090404030807         4       11       14\n",
       "522  2022090404030807         4       11        2\n",
       "523  2022090404030807         4       11        5\n",
       "524  2022090404030807         4       11        9\n",
       "525  2022090404030807         4       11       10\n",
       "528  2022090404030803         4       11       14\n",
       "529  2022090404030803         4       11       15\n",
       "530  2022090404030803         4       11       10\n",
       "531  2022090404030803         4       11        9\n",
       "532  2022090404030803         4       11        8\n",
       "533  2022090404030803         4       11        7\n",
       "534  2022090404030803         4       11        2\n",
       "535  2022090404030803         4       14       11\n",
       "537  2022090404030803         4       14       15\n",
       "538  2022090404030803         4       14       10\n",
       "539  2022090404030803         4       14        9\n",
       "540  2022090404030803         4       14        8\n",
       "541  2022090404030803         4       14        7\n",
       "542  2022090404030803         4       14        2\n",
       "543  2022090404030803         4       10       11\n",
       "544  2022090404030803         4       10       14\n",
       "545  2022090404030803         4       10       15\n",
       "547  2022090404030803         4       10        9\n",
       "548  2022090404030803         4       10        8\n",
       "549  2022090404030803         4       10        7\n",
       "550  2022090404030803         4       10        2\n",
       "551  2022090404030803         4        9       11\n",
       "552  2022090404030803         4        9       14\n",
       "553  2022090404030803         4        9       15\n",
       "554  2022090404030803         4        9       10\n",
       "556  2022090404030803         4        9        8\n",
       "557  2022090404030803         4        9        7\n",
       "558  2022090404030803         4        9        2\n",
       "559  2022090404030803         4        8       11\n",
       "560  2022090404030803         4        8       14\n",
       "561  2022090404030803         4        8       15\n",
       "562  2022090404030803         4        8       10\n",
       "563  2022090404030803         4        8        9\n",
       "565  2022090404030803         4        8        7\n",
       "566  2022090404030803         4        8        2\n",
       "567  2022090404030803         4        7       11\n",
       "568  2022090404030803         4        7       14\n",
       "569  2022090404030803         4        7       15\n",
       "570  2022090404030803         4        7       10\n",
       "571  2022090404030803         4        7        9\n",
       "572  2022090404030803         4        7        8\n",
       "574  2022090404030803         4        7        2\n",
       "575  2022090404030803         4        2       11\n",
       "576  2022090404030803         4        2       14\n",
       "577  2022090404030803         4        2       15\n",
       "578  2022090404030803         4        2       10\n",
       "579  2022090404030803         4        2        9\n",
       "580  2022090404030803         4        2        8\n",
       "581  2022090404030803         4        2        7"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_test = ModelEvaluator(f_lgb_clf, haitou, True)\n",
    "\n",
    "fm_test = me_test.pred_table(X_fukusho_test1, 0.6)\n",
    "fm_test['expected_f'] = fm_test['time_odds'] * fm_test['win_ratio']\n",
    "\n",
    "t_local_umaren = lbt[['race_id', 'place_id', 'h_num', 'expected']].merge(fm_test, on='race_id')\n",
    "# t_local_uma_haito = t_local_umaren.merge(haitou[['1着馬番', '2着馬番', '3着馬番', '馬連', '馬単', '3連複', '3連単']], on='race_id')\n",
    "t_local_umaren = t_local_umaren[t_local_umaren['h_num_x'] != t_local_umaren['h_num_y']]\n",
    "\n",
    "# l_umaren = bt[['race_id', 'place_id', 'h_num', 'expected']].merge(fm_test, on='race_id')\n",
    "# l_umaren = l_umaren[\n",
    "#     (l_umaren['expected_f'] >= 1)\n",
    "# ]\n",
    "pd.set_option('display.max_rows', None)\n",
    "t_local_umaren[['race_id', 'place_id', 'h_num_x', 'h_num_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373254ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_uma_haito = l_umaren.merge(haitou[['1着馬番', '2着馬番', '3着馬番', '馬連', '馬単', '3連複', '3連単']], on='race_id')\n",
    "l_uma_haito = l_uma_haito[l_uma_haito['h_num_x'] != l_uma_haito['h_num_y']]\n",
    "l_uma_haito\n",
    "\n",
    "f_bt = l_uma_haito[\n",
    "    (\n",
    "        (l_uma_haito['h_num_x'] == l_uma_haito['1着馬番'])\n",
    "        &\n",
    "        (l_uma_haito['h_num_y'] == l_uma_haito['2着馬番'])      \n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (l_uma_haito['h_num_x'] == l_uma_haito['2着馬番'])\n",
    "        &\n",
    "        (l_uma_haito['h_num_y'] == l_uma_haito['1着馬番'])      \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "f_bt['馬連'].sum(), f_bt['馬連'].max(), (f_bt['馬連'].sum() / (len(l_uma_haito) * 100)) * 100, len(f_bt) / len(l_uma_haito), len(f_bt), len(f_bt.groupby('race_id')) / len(l_uma_haito.groupby('race_id'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "vir_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
